Deformable ConvNets v2: More Deformable, Better Results
Xizhou Zhu1,2∗ Han Hu2 Stephen Lin2 Jifeng Dai2 1University of Science and Technology of China 2Microsoft Research Asia
ezra0408@mail.ustc.edu.cn {hanhu,stevelin,jifdai}@microsoft.com

arXiv:1811.11168v2 [cs.CV] 28 Nov 2018

Abstract
The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be inﬂuenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reﬂect the object focus and classiﬁcation power of RCNN features. With the proposed contributions, this new version of Deformable ConvNets yields signiﬁcant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.
1. Introduction
Geometric variations due to scale, pose, viewpoint and part deformation present a major challenge in object recognition and detection. The current state-of-the-art method for addressing this issue is Deformable Convolutional Networks (DCNv1) [8], which introduces two modules that aid CNNs in modeling such variations. One of these modules is deformable convolution, in which the grid sampling
∗This work is done when Xizhou Zhu is an intern at Microsoft Research Asia.

locations of standard convolution are each offset by displacements learned with respect to the preceding feature maps. The other is deformable RoIpooling, where offsets are learned for the bin positions in RoIpooling [16]. The incorporation of these modules into a neural network gives it the ability to adapt its feature representation to the conﬁguration of an object, speciﬁcally by deforming its sampling and pooling patterns to ﬁt the object’s structure. With this approach, large improvements in object detection accuracy are obtained.
Towards understanding Deformable ConvNets, the authors visualized the induced changes in receptive ﬁeld, via the arrangement of offset sampling positions in PASCAL VOC images [11]. It is found that samples for an activation unit tend to cluster around the object on which it lies. However, the coverage over an object is inexact, exhibiting a spread of samples beyond the area of interest. In a deeper analysis of spatial support using images from the more challenging COCO dataset [29], we observe that such behavior becomes more pronounced. These ﬁndings suggest that greater potential exists for learning deformable convolutions.
In this paper, we present a new version of Deformable ConvNets, called Deformable ConvNets v2 (DCNv2), with enhanced modeling power for learning deformable convolutions. This increase in modeling capability comes in two complementary forms. The ﬁrst is the expanded use of deformable convolution layers within the network. Equipping more convolutional layers with offset learning capacity allows DCNv2 to control sampling over a broader range of feature levels. The second is a modulation mechanism in the deformable convolution modules, where each sample not only undergoes a learned offset, but is also modulated by a learned feature amplitude. The network module is thus given the ability to vary both the spatial distribution and the relative inﬂuence of its samples.
To fully exploit the increased modeling capacity of DCNv2, effective training is needed. Inspired by work on

1

knowledge distillation in neural networks [2, 22], we make use of a teacher network for this purpose, where the teacher provides guidance during training. We speciﬁcally utilize R-CNN [17] as the teacher. Since it is a network trained for classiﬁcation on cropped image content, R-CNN learns features unaffected by irrelevant information outside the region of interest. To emulate this property, DCNv2 incorporates a feature mimicking loss into its training, which favors learning of features consistent to those of R-CNN. In this way, DCNv2 is given a strong training signal for its enhanced deformable sampling.
With the proposed changes, the deformable modules remain lightweight and can easily be incorporated into existing network architectures. Speciﬁcally, we incorporate DCNv2 into the Faster R-CNN [33] and Mask R-CNN [20] systems, with a variety of backbone networks. Extensive experiments on the COCO benchmark demonstrate the signiﬁcant improvement of DCNv2 over DCNv1 for object detection and instance segmentation. The code for DCNv2 will be released.
2. Analysis of Deformable ConvNet Behavior
2.1. Spatial Support Visualization
To better understand the behavior of Deformable ConvNets, we visualize the spatial support of network nodes by their effective receptive ﬁelds [31], effective sampling locations, and error-bounded saliency regions. These three modalities provide different and complementary perspectives on the underlying image regions that contribute to a node’s response.
Effective receptive ﬁelds Not all pixels within the receptive ﬁeld of a network node contribute equally to its response. The differences in these contributions are represented by an effective receptive ﬁeld, whose values are calculated as the gradient of the node response with respect to intensity perturbations of each image pixel [31]. We utilize the effective receptive ﬁeld to examine the relative inﬂuence of individual pixels on a network node, but note that this measure does not reﬂect the structured inﬂuence of full image regions.
Effective sampling / bin locations In [8], the sampling locations of (stacked) convolutional layers and the sampling bins in RoIpooling layers are visualized for understanding the behavior of Deformable ConvNets. However, the relative contributions of these sampling locations to the network node are not revealed. We instead visualize effective sampling locations that incorporate this information, computed as the gradient of the network node with respect to the sampling / bin locations, so as to understand their contribution strength.
Error-bounded saliency regions The response of a network node will not change if we remove image regions

that do not inﬂuence it, as demonstrated in recent research on image saliency [41, 44, 13, 7]. Based on this property, we can determine a node’s support region as the smallest image region giving the same response as the full image, within a small error bound. We refer to this as the errorbounded saliency region, which can be found by progressively masking parts of the image and computing the resulting node response, as described in more detail in the Appendix. The error-bounded saliency region facilitates comparison of support regions from different networks.
2.2. Spatial Support of Deformable ConvNets
We analyze the visual support regions of Deformable ConvNets in object detection. The regular ConvNet we employ as a baseline consists of a Faster R-CNN + ResNet50 [21] object detector with aligned RoIpooling1 [20]. All the convolutional layers in ResNet-50 are applied on the whole input image. The effective stride in the conv5 stage is reduced from 32 to 16 pixels to increase feature map resolution. The RPN [33] head is added on top of the conv4 features of ResNet-101. On top of the conv5 features we add the Fast R-CNN head [16], which is composed of aligned RoIpooling and two fully-connected (fc) layers, followed by the classiﬁcation and bounding box regression branches. We follow the procedure in [8] to turn the object detector into its deformable counterpart. The three layers of 3 × 3 convolutions in the conv5 stage are replaced by deformable convolution layers. Also, the aligned RoIpooling layer is replaced by deformable RoIPooling. Both networks are trained and visualized on the COCO benchmark. It is worth mentioning that when the offset learning rate is set to zero, the Deformable Faster R-CNN detector degenerates to regular Faster R-CNN with aligned RoIpooling.
Using the three visualization modalities, we examine the spatial support of nodes in the last layer of the conv5 stage in Figure 1 (a)∼(b). The sampling locations analyzed in [8] are also shown. From these visualizations, we make the following observations:
1. Regular ConvNets can model geometric variations to some extent, as evidenced by the changes in spatial support with respect to image content. Thanks to the strong representation power of deep ConvNets, the network weights are learned to accommodate some degree of geometric transformation.
2. By introducing deformable convolution, the network’s ability to model geometric transformation is considerably enhanced, even on the challenging COCO benchmark. The spatial support adapts much more to image content, with nodes on the foreground having support that covers the whole object, while nodes on the background have ex-
1Aligned RoIpooling is called RoIAlign in [20]. We use the term “aligned RoIpooling” in this paper to more clearly describe it in the context of other related terms.

2

(a) regular conv (b) deformable conv@conv5 stage (DCNv1)

high high

foreground node including background areas irrelevant for

high detection.

3. The three presented types of spatial support visual-

izations are more informative than the sampling locations

low low

used in [8]. This can be seen, for example, with regu-

low lar ConvNets, which have ﬁxed sampling locations along a

grid, but actually adapt its effective spatial support via net-

work weights. The same is true for Deformable ConvNets,

whose predictions are jointly affected by learned offsets and

network weights. Examining sampling locations alone, as

done in [8], can result in misleading conclusions about De-

formable ConvNets.

Figure 2 (a)∼(b) display the spatial support of the 2fc

node in the per-RoI detection head, which is directly fol-

high lowed by the classiﬁcation and the bounding box regreshigh sion branches. The visualization of effective bin locations high suggests that bins on the object foreground generally re-

ceive larger gradients from the classiﬁcation branch, and

thus exert greater inﬂuence on prediction. This observa-
low
low tion holds for both aligned RoIpooling and Deformable

low RoIpooling. In Deformable RoIpooling, a much larger pro-

portion of bins cover the object foreground than in aligned

RoIpooling, thanks to the introduction of learnable bin off-

sets. Thus, more information from relevant bins is avail-

able for the downstream Fast R-CNN head. Meanwhile, the

error-bounded saliency regions in both aligned RoIpooling

and Deformable RoIpooling are not fully focused on the ob-

ject foreground, which suggests that image content outside

of the RoI affects the prediction result. According to a re-

cent study [6], such feature interference could be harmful

for detection.

While it is evident that Deformable ConvNets have

markedly improved ability to adapt to geometric variation

in comparison to regular ConvNets, it can also be seen that

their spatial support may extend beyond the region of inter-

est. We thus seek to upgrade Deformable ConvNets so that

they can better focus on pertinent image content and deliver

greater detection accuracy.

(c) modulated deformable conv@conv3∼5 stages (DCNv2)
Figure 1. Spatial support of nodes in the last layer of the conv5 stage in a regular ConvNet, DCNv1 and DCNv2. The regular ConvNet baseline is Faster R-CNN + ResNet-50. In each subﬁgure, the effective sampling locations, effective receptive ﬁeld, and error-bounded saliency regions are shown from the top to the bottom rows. Effective sampling locations are omitted in (c) as they are similar to those in (b), providing limited additional information. The visualized nodes (green points) are on a small object (left), a large object (middle), and the background (right).
panded support that encompasses greater context. However, the range of spatial support may be inexact, with the effective receptive ﬁeld and error-bounded saliency region of a

3. More Deformable ConvNets
To improve the network’s ability to adapt to geometric variations, we present changes to boost its modeling power and to help it take advantage of this increased capability.
3.1. Stacking More Deformable Conv Layers
Encouraged by the observation that Deformable ConvNets can effectively model geometric transformation on challenging benchmarks, we boldly replace more regular conv layers by their deformable counterparts. We expect that by stacking more deformable conv layers, the geometric transformation modeling capability of the entire network can be further strengthened.

3

low low

high

high

high

low

low

low

(a) aligned RoIpooling, with deformable conv@conv5 stage high (b) deformable RoIpooling, with deformable conv@conv5 stage (DCNv1)
low

(c) modulated deformable RoIpooling, with modulated deformable conv@conv3∼5 stages

(d) with R-CNN feature mimicking on setting (c) (DCNv2)

(e) with R-CNN feature mimicking in regular ConvNet

Figure 2. Spatial support of the 2fc node in the per-RoI detection head, directly followed by the classiﬁcation and the bounding box regression branches. Visualization is conducted on a regular ConvNet, DCNv1 and DCNv2. The regular ConvNet baseline is Faster R-CNN + ResNet-50. In each subﬁgure, the effective bin locations, effective receptive ﬁelds, and error-bounded saliency regions are shown from the top to the bottom rows, except for (c)∼(e) where the effective bin locations are omitted as they provide little additional understanding over those in (a)∼(b). The input RoIs (green boxes) are on a small object (left), a large object (middle), and the background (right).

In this paper, deformable convolutions are applied in all the 3 × 3 conv layers in stages conv3, conv4, and conv5 in ResNet-50. Thus, there are 12 layers of deformable convolution in the network. In contrast, just three layers of deformable convolution are used in [8], all in the conv5 stage. It is observed in [8] that performance saturates when stacking more than three layers for the relatively simple and small-scale PASCAL VOC benchmark. Also, misleading offset visualizations on COCO may have hindered further exploration on more challenging benchmarks. In experiments, we observe that utilizing deformable layers in the conv3-conv5 stages achieves the best tradeoff between accuracy and efﬁciency for object detection on COCO. See Section 5.2 for details.

3.2. Modulated Deformable Modules
To further strengthen the capability of Deformable ConvNets in manipulating spatial support regions, a modulation mechanism is introduced. With it, the Deformable ConvNets modules can not only adjust offsets in perceiving input features, but also modulate the input feature amplitudes from different spatial locations / bins. In the extreme case, a module can decide not to perceive signals from a particular location / bin by setting its feature amplitude to zero. Consequently, image content from the corresponding spatial location will have considerably reduced or no impact on the module output. Thus, the modulation mechanism provides the network module another dimension of freedom to adjust its spatial support regions.
Given a convolutional kernel of K sampling locations, let wk and pk denote the weight and pre-speciﬁed offset for

4

the k-th location, respectively. For example, K = 9 and pk ∈ {(−1, −1), (−1, 0), . . . , (1, 1)} deﬁnes a 3 × 3 convolutional kernel of dilation 1. Let x(p) and y(p) denote the features at location p from the input feature maps x and output feature maps y, respectively. The modulated deformable convolution can then be expressed as

K
y(p) = wk · x(p + pk + ∆pk) · ∆mk, (1)
k=1

where ∆pk and ∆mk are the learnable offset and modulation scalar for the k-th location, respectively. The modulation scalar ∆mk lies in the range [0, 1], while ∆pk is a real number with unconstrained range. As p + pk + ∆pk is fractional, bilinear interpolation is applied as in [8] in computing x(p + pk + ∆pk). Both ∆pk and ∆mk are obtained via a separate convolution layer applied over the same input feature maps x. This convolutional layer is of the same spatial resolution and dilation as the current convolutional layer. The output is of 3K channels, where the ﬁrst 2K channels correspond to the learned offsets {∆pk}Kk=1, and the remaining K channels are further fed to a sigmoid layer to obtain the modulation scalars {∆mk}Kk=1. The kernel weights in this separate convolution layer are initialized to zero. Thus, the initial values of ∆pk and ∆mk are 0 and 0.5, respectively. The learning rates of the added conv layers for offset and modulation learning are set to 0.1 times those of the existing layers.
The design of modulated deformable RoIpooling is similar. Given an input RoI, RoIpooling divides it into K spatial bins (e.g. 7 × 7). Within each bin, sampling grids of even spatial intervals are applied (e.g. 2 × 2). The sampled values on the grids are averaged to compute the bin output. Let ∆pk and ∆mk be the learnable offset and modulation scalar for the k-th bin. The output binning feature y(k) is computed as

nk

y(k) = x(pkj + ∆pk) · ∆mk/nk,

(2)

j=1

where pkj is the sampling location for the j-th grid cell in the k-th bin, and nk denotes the number of sampled grid cells. Bilinear interpolation is applied to obtain features x(pkj + ∆pk). The values of ∆pk and ∆mk are produced by a sibling branch on the input feature maps. In this branch, RoIpooling generates features on the RoI, followed by two fc layers of 1024-D (initialized with Gaussian distribution of standard derivation of 0.01). On top of that, an additional fc layer produces output of 3K channels (weights initialized to be zero). The ﬁrst 2K channels are the normalized learnable offsets, where element-wise multiplications with the RoI’s width and height are computed to obtain {∆pk}Kk=1. The remaining K channels are normalized by a sigmoid layer to produce {∆mk}Kk=1. The

learning rates of the added fc layers for offset learning are the same as those of the existing layers.
3.3. R-CNN Feature Mimicking
As observed in Figure 2, the error-bounded saliency region of a per-RoI classiﬁcation node can stretch beyond the RoI for both regular ConvNets and Deformable ConvNets. Image content outside of the RoI may thus affect the extracted features and consequently degrade the ﬁnal results of object detection.
In [6], the authors ﬁnd redundant context to be a plausible source of detection error for Faster R-CNN. Together with other motivations (e.g., to share fewer features between the classiﬁcation and bounding box regression branches), the authors propose to combine the classiﬁcation scores of Faster R-CNN and R-CNN to obtain the ﬁnal detection score. Since R-CNN classiﬁcation scores are focused on cropped image content from the input RoI, incorporating them would help to alleviate the redundant context problem and improve detection accuracy. However, the combined system is slow because both the Faster-RCNN and R-CNN branches need to be applied in both training and inference.
Meanwhile, Deformable ConvNets are powerful in adjusting spatial support regions. For Deformable ConvNets v2 in particular, the modulated deformable RoIpooling module could simply set the modulation scalars of bins in a way that excludes redundant context. However, our experiments in Section 5.3 show that even with modulated deformable modules, such representations cannot be learned well through the standard Faster R-CNN training procedure. We suspect that this is because the conventional Faster RCNN training loss cannot effectively drive the learning of such representations. Additional guidance is needed to steer the training.
Motivated by recent work on feature mimicking [2, 22, 28], we incorporate a feature mimic loss on the per-RoI features of Deformable Faster R-CNN to force them to be similar to R-CNN features extracted from cropped images. This auxiliary training objective is intended to drive Deformable Faster R-CNN to learn more “focused” feature representations like R-CNN. We note that, based on the visualized spatial support regions in Figure 2, a focused feature representation may well not be optimal for negative RoIs on the image background. For background areas, more context information may need to be considered so as not to produce false positive detections. Thus, the feature mimic loss is enforced only on positive RoIs that sufﬁciently overlap with ground-truth objects.
The network architecture for training Deformable Faster R-CNN is presented in Figure 3. In addition to the Faster R-CNN network, an additional R-CNN branch is added for feature mimicking. Given an RoI b for feature mimicking, the image patch corresponding to it is cropped and resized

5

to 224 × 224 pixels. In the R-CNN branch, the backbone network operates on the resized image patch and produces feature maps of 14 × 14 spatial resolution. A (modulated) deformable RoIpooling layer is applied on top of the feature maps, where the input RoI covers the whole resized image patch (top-left corner at (0, 0), and height and width are 224 pixels). After that, 2 fc layers of 1024-D are applied, producing an R-CNN feature representation for the input image patch, denoted by fRCNN(b). A (C +1)-way Softmax classiﬁer follows for classiﬁcation, where C denotes the number of foreground categories, plus one for background. The feature mimic loss is enforced between the R-CNN feature representation fRCNN(b) and the counterpart in Faster R-CNN, fFRCNN(b), which is also 1024-D and is produced by the 2 fc layers in the Fast R-CNN head. The feature mimic loss is deﬁned on the cosine similarity between fRCNN(b) and fFRCNN(b), computed as
Lmimic = [1 − cos(fRCNN(b), fFRCNN(b))], (3)
b∈Ω
where Ω denotes the set of RoIs sampled for feature mimic training. In the SGD training, given an input image, 32 positive region proposals generated by RPN are randomly sampled into Ω. A cross-entropy classiﬁcation loss is enforced on the R-CNN classiﬁcation head, also computed on the RoIs in Ω. Network training is driven by the feature mimic loss and the R-CNN classiﬁcation loss, together with the original loss terms in Faster R-CNN. The loss weights of the two newly introduced loss terms are 0.1 times those of the original Faster R-CNN loss terms. The network parameters between the corresponding modules in the R-CNN and the Faster R-CNN branches are shared, including the backbone network, (modulated) deformable RoIpooling, and the 2 fc heads (the classiﬁcation heads in the two branches are unshared). In inference, only the Faster R-CNN network is applied on the test images, without the auxiliary R-CNN branch. Thus, no additional computation is introduced by R-CNN feature mimicking in inference.
4. Related Work
Deformation Modeling is a long-standing problem in computer vision, and there has been tremendous effort in designing translation-invariant features. Prior to the deep learning era, notable works include scale-invariant feature transform (SIFT) [30], oriented FAST and rotated BRIEF (ORB) [34], and deformable part-based models (DPM) [12]. Such works are limited by the inferior representation power of handcrafted features and the constrained family of geometric transformations they address (e.g., afﬁne transformations). Spatial transformer networks (STN) [25] is the ﬁrst work on learning translation-invariant features for deep CNNs. It learns to apply global afﬁne

Classification Bounding-Box Regression

1024-D fully-connected

Feature Mimicking

1024-D fully-connected

(Modulated) Deformable RoIpooling

Classification
1024-D fully-connected
1024-D fully-connected
(Modulated) Deformable RoIpooling

RPN
(Modulated) Deformable Convolutions

Region Proposals

Whole Image Regions

(Modulated) Deformable Convolutions

Crop & Resize

224x224

Figure 3. Network training with R-CNN feature mimicking.

transformations to warp feature maps, but such transformations inadequately model the more complex geometric variations encountered in many vision tasks. Instead of performing global parametric transformations and feature warping, Deformable ConvNets sample feature maps in a local and dense manner, via learnable offsets in the proposed deformable convolution and deformable RoIpooling modules. Deformable ConvNets is the ﬁrst work to effectively model geometric transformations in complex vision tasks (e.g., object detection and semantic segmentation) on challenging benchmarks.
Our work extends Deformable ConvNets by enhancing its modeling power and facilitating network training. This new version of Deformable ConvNets yields signiﬁcant performance gains over the original model.
Relation Networks and Attention Modules are ﬁrst proposed in natural language processing [14, 15, 4, 36] and physical system modeling [3, 38, 23, 35, 10, 32]. An attention / relation module effects an individual element (e.g., a word in a sentence) by aggregating features from a set of elements (e.g., all the words in the sentence), where the aggregation weights are usually deﬁned on feature similarities among the elements. They are powerful in capturing longrange dependencies and contextual information in these tasks. Recently, the concurrent works of [24] and [37] successfully extend relation networks and attention modules to the image domain, for modeling long-range object-object and pixel-pixel relations, respectively. In [19], a learnable region feature extractor is proposed, unifying the previous region feature extraction modules from the pixel-object relation perspective. A common issue with such approaches is that the aggregation weights and the aggregation operation

6

need to be computed on the elements in a pairwise fashion, incurring heavy computation that is quadratic to the number of elements (e.g., all the pixels in an image). Our developed approach can be perceived as a special attention mechanism where only a sparse set of elements have non-zero aggregation weights (e.g., 3 × 3 pixels from among all the image pixels). The attended elements are speciﬁed by the learnable offsets, and the aggregation weights are controlled by the modulation mechanism. The computational overhead is just linear to the number of elements, which is negligible compared to that of the entire network (See Table 1).
Spatial Support Manipulation. For atrous convolution, the spatial support of convolutional layers has been enlarged by padding zeros in the convolutional kernels [5]. The padding parameters are handpicked and predetermined. In active convolution [26], which is contemporary with Deformable ConvNets, convolutional kernel offsets are learned via back-propagation. But the offsets are static model parameters ﬁxed after training and shared over different spatial locations. In a multi-path network for object detection [40], multiple RoIpooling layers are employed for each input RoI to better exploit multi-scale and context information. The multiple RoIpooling layers are centered at the input RoI, and are of different spatial scales. A common issue with these approaches is that the spatial support is controlled by static parameters and does not adapt to image content.
Effective Receptive Field and Salient Region. Towards better interpreting how a deep network functions, signiﬁcant progress has been made in understanding which image regions contribute most to network prediction. Recent works on effective receptive ﬁelds [31] and salient regions [41, 44, 13, 7] reveal that only a small proportion of pixels in the theoretical receptive ﬁeld contribute signiﬁcantly to the ﬁnal network prediction. The effective support region is controlled by the joint effect of network weights and sampling locations. Here we exploit the developed techniques to better understand the network behavior of Deformable ConvNets. The resulting observations guide and motivate us to improve over the original model.
Network Mimicking and Distillation are recently introduced techniques for model acceleration and compression. Given a large teacher model, a compact student model is trained by mimicking the teacher model output or feature responses on training images [2, 22, 28]. The hope is that the compact model can be better trained by distilling knowledge from the large model.
Here we employ a feature mimic loss to help the network learn features that reﬂect the object focus and classiﬁcation power of R-CNN features. Improved accuracy is obtained and the visualized spatial supports corroborate this approach.

5. Experiments
5.1. Experiment Settings
Our models are trained on the 118k images of the COCO 2017 train set. In ablation, evaluation is done on the 5k images of the COCO 2017 validation set. We also evaluate performance on the 20k images of the COCO 2017 test-dev set. The standard mean average-precision scores at different box and mask IoUs are used for measuring object detection and instance segmentation accuracy, respectively.
Faster R-CNN and Mask R-CNN are chosen as the baseline systems. ImageNet [9] pre-trained ResNet-50 is utilized as the backbone. The implementation of Faster RCNN is the same as in Section 3.3. For Mask R-CNN, we follow the implementation in [20]. To turn the networks into their deformable counterparts, the last set of 3 × 3 regular conv layers (close to the output in the bottom-up computation) are replaced by (modulated) deformable conv layers. Aligned RoIpooling is replaced by (modulated) deformable RoIpooling. Specially for Mask R-CNN, the two aligned RoIpooling layers with 7 × 7 and 14 × 14 bins are replaced by two (modulated) deformable RoIpooling layers with the same bin numbers. In R-CNN feature mimicking, the feature mimic loss is enforced on the RoI head for classiﬁcation only (excluding that for mask estimation). For both systems, the choice of hyper-parameters follows the latest Detectron [18] code base except for the image resolution, which is brieﬂy presented here. In both training and inference, images are resized so that the shorter side is 1,000 pixels2. Anchors of 5 scales and 3 aspect ratios are utilized. 2k and 1k region proposals are generated at a nonmaximum suppression threshold of 0.7 at training and inference respectively. In SGD training, 256 anchor boxes (of positive-negative ratio 1:1) and 512 region proposals (of positive-negative ratio 1:3) are sampled for backpropagating their gradients. In our experiments, the networks are trained on 8 GPUs with 2 images per GPU for 16 epochs. The learning rate is initialized to 0.02 and is divided by 10 at the 10-th and the 14-th epochs. The weight decay and the momentum parameters are set to 10−4 and 0.9, respectively.
5.2. Enriched Deformation Modeling
The effects of enriched deformation modeling are examined from ablations shown in Table 1. The baseline with regular CNN modules obtains an APbbox score of 34.7% for Faster R-CNN, and APbbox and APmask scores of 36.6% and 32.2% respectively for Mask R-CNN. To obtain a DCNv1 baseline, we follow the original Deformable ConvNets paper by replacing the last three layers of 3 × 3 convolution in the conv5 stage and the aligned RoIpooling layer by their deformable counterparts. This DCNv1 baseline achieves an
2The previous default setting in Detectron is 800 pixels. Ablation on input image resolution is present in Appendix.

7

method setting (shorter side 1000) regular (RoIpooling)

Faster R-CNN APbbox APbSbox APbMbox APbLbox param FLOP 32.1 14.9 37.5 44.4 51.3M 326.7G

baseline regular (aligned RoIpooling) 34.7 19.3 39.5 45.3 51.3M 326.7G

dconv@c5 + dpool (DCNv1) 38.0 20.7 41.8 52.2 52.7M 328.2G

dconv@c5

37.4 20.0 40.9 51.0 51.5M 327.1G

enriched deformation

dconv@c4∼c5 dconv@c3∼c5 dconv@c3∼c5 + dpool

40.0 21.4 43.8 55.3 51.7M 328.6G 40.4 21.6 44.2 56.2 51.8M 330.6G 41.0 22.0 45.1 56.6 53.0M 331.8G

mdconv@c3∼c5 + mdpool 41.7 22.2 45.8 58.7 65.5M 346.2G

Mask R-CNN APbbox APmask param FLOP

-

-

-

-

36.6 32.2 39.5M 447.5G

40.4 35.3 40.9M 449.0G

40.2 35.1 39.8M 447.8G

41.8 36.8 40.0M 449.4G

42.2 37.0 40.1M 451.4G

42.4 37.0 41.3M 452.5G

43.1 37.3 53.8M 461.1G

Table 1. Ablation study on enriched deformation modeling. The input images are of shorter side 1,000 pixels (default in paper). In the setting column, “(m)dconv” and “(m)dpool” stand for (modulated) deformable convolution and (modulated) deformable RoIpooling, respectively. Also, “dconv@c3∼c5” stands for applying deformable conv layers at stages conv3∼conv5, for example. Results are reported on the COCO 2017 validation set.

method setting (shorter side 800) regular (RoIpooling)

Faster R-CNN APbbox APbSbox APbMbox APbLbox param FLOP 32.8 13.6 37.2 48.7 51.3M 196.8G

baseline regular (aligned RoIpooling) 35.6 18.2 40.3 48.7 51.3M 196.8G

dconv@c5 + dpool (DCNv1) 38.2 19.1 42.2 54.0 52.7M 198.9G

dconv@c5

37.6 19.3 41.4 52.6 51.5M 197.7G

enriched deformation

dconv@c4∼c5 dconv@c3∼c5 dconv@c3∼c5 + dpool

39.2 19.9 43.4 55.5 51.7M 198.7G 39.5 21.0 43.5 55.6 51.8M 200.0G 40.0 21.1 44.6 56.3 53.0M 201.2G

mdconv@c3∼c5 + mdpool 40.8 21.3 45.0 58.5 65.5M 214.7G

Mask R-CNN APbbox APmask param FLOP

-

-

-

-

37.8 33.4 39.5M 303.5G

40.3 35.0 40.9M 304.9G

39.9 34.9 39.8M 303.7G

41.2 36.1 40.0M 304.7G

41.5 36.4 40.1M 306.0G

41.8 36.4 41.3M 307.2G

42.7 37.0 53.8M 320.3G

Table 2. Ablation study on enriched deformation modeling. The input images are of shorter side 800 pixels. Results are reported on the COCO 2017 validation set.

APbbox score of 38.0% for Faster R-CNN, and APbbox and APmask scores of 40.4% and 35.3% respectively for Mask R-CNN. The deformable modules considerably improve accuracy as observed in [8].
By replacing more 3 × 3 regular conv layers by their deformable counterparts, the accuracy of both Faster RCNN and Mask R-CNN steadily improve, with gains between 2.0% and 3.0% for APbbox and APmask scores when the conv layers in conv3-conv5 are replaced. No additional improvement is observed on the COCO benchmark by further replacing the regular conv layers in the conv2 stage. By upgrading the deformable modules to modulated deformable modules, we obtain further gains between 0.3% and 0.7% in APbbox and APmask scores. In total, enriching the deformation modeling capability yields a 41.7% APbbox score on Faster R-CNN, which is 3.7% higher than that of the DCNv1 baseline. On Mask R-CNN, 43.1% APbbox and 37.3% APmask scores are obtained with the enriched deformation modeling, which are respectively 2.7% and 2.0% higher than those of the DCNv1 baseline. Note that the added parameters and FLOPs for enriching the deformation modeling are minor compared to those of the overall networks.
Shown in Figure 1 (b)∼(c), the spatial support of the enriched deformable modeling exhibits better adaptation to image content compared to that of DCNv1.

Table 2 presents the results at input image resolution of 800 pixels, which follows the default setting in the Detectron code base. The same conclusion holds.
5.3. R-CNN Feature Mimicking
Ablations of the design choices in R-CNN feature mimicking are shown in Table 3. With the enriched deformation modeling, R-CNN feature mimicking further improves the APbbox and APmask scores by about 1% to 1.4% in both the Faster R-CNN and Mask R-CNN systems. Mimicking features of positive boxes on the object foreground is found to be particularly effective, and the results when mimicking all the boxes or just negative boxes are much lower. As shown in Figure 2 (c)∼(d), feature mimicking can help the network features better focus on the object foreground, which is beneﬁcial for positive boxes. For the negative boxes, the network tends to exploit more context information (see Figure 2), where feature mimicking would not be helpful.
We also apply R-CNN feature mimicking to regular ConvNets without any deformable layers. Almost no accuracy gains are observed. The visualized spatial support regions are shown in Figure 2 (e), which are not focused on the object foreground even with the auxiliary mimic loss. This is likely because it is beyond the representation capability of regular ConvNets to focus features on the object foreground, and thus this cannot be learned.

8

setting

Faster

Mask

regions to mimic

R-CNN APbbox

R-CNN APbbox APmask

None 41.7 43.1 37.3

mdconv3∼5 + FG & BG 42.1 43.4 37.6

mdpool BG Only 41.7 43.3 37.5

FG Only 43.1 44.3 38.3

regular

None 34.7 36.6 32.2 FG Only 35.0 36.8 32.3

Table 3. Ablation study on R-CNN feature mimicking. Results are reported on the COCO 2017 validation set.

backbone ResNet-50 ResNet-101 ResNext-101

method
regular DCNv1 DCNv2 regular DCNv1 DCNv2 regular DCNv1 DCNv2

Faster R-CNN APbbox
35.1 38.4 43.3 39.2 41.4 44.8 40.1 41.7 45.3

Mask R-CNN APbbox APmask 37.0 32.4 40.7 35.5 44.5 38.4 40.9 35.3 42.9 37.1 45.8 39.7 41.7 36.2 43.4 37.7 46.7 40.5

Table 4. Results of DCNv2, DCNv1 and regular ConvNets on various backbones on the COCO 2017 test-dev set.

5.4. Application on Stronger Backbones
Results on stronger backbones, by replacing ResNet-50 with ResNet-101 and ResNext-101 [39], are presented in Table 4. For the entries of DCNv1, the regular 3 × 3 conv layers in the conv5 stage are replaced by the deformable counterpart, and aligned RoIpooling is replaced by deformable RoIpooling. For the DCNv2 entries, all the 3 × 3 conv layers in the conv3-conv5 stages are of modulated deformable convolution, and modulated deformable RoIpooling is used instead, with supervision from the RCNN feature mimic loss. DCNv2 is found to outperform regular ConvNet and DCNv1 considerably on all the network backbones.

6. Conclusion
Despite the superior performance of Deformable ConvNets in modeling geometric variations, its spatial support extends well beyond the region of interest, causing features to be inﬂuenced by irrelevant image content. In this paper, we present a reformulation of Deformable ConvNets which improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. Signiﬁcant performance gains are obtained on the COCO

benchmark for object detection and instance segmentation.

A1. Error-bounded Image Saliency

In existing research on image saliency [41, 44, 13, 7],

a widely utilized formulation is as follows. Given an in-

put image I and a trained network N , let N (I) denote the

network response on the original image. A binary mask

M , which is of the same spatial dimension as I, can be ap-

plied on the image as I M . For the image pixel p where

M (p) = 1, its content is kept in the masked image. Mean-

while, if M (p) = 0, the content is set as 0 in the masked

image. The saliency map is obtained by optimizing loss

function L(M ) = ||N (I) − N (I M )||2 + λ||M ||1 as a function of M , where λ is the hyper-parameter balanc-

ing the output reconstruction error ||N (I) − N (I M )||2 and the salient area loss ||M ||1. The optimized mask M is

called the saliency map. The problem is it is hard to obtain

the salient region at a speciﬁed reconstruction error. Thus it

is hard to compare the salient regions from two networks at

the same reconstruction error.

We seek to strictly constrain the reconstruction loss in

the image saliency formulation, so as to facilitate compar-

ison among the salient regions derived from different net-

works. Thus, the optimization problem is slightly modiﬁed

to be

min||M ||1

(4)

s.t. Lrec(N (I), N (I M )) < ,

where Lrec(N (I), N (I M )) denotes an arbitrary form of reconstruction loss, which is strictly bounded by . We term the collection of image pixels where {p|M (p) = 1} in the optimized mask as visual support region.
The formulation in Eq. (4) is hard to be optimized, due to the hard reconstruction error constraint introduced. Here we develop a heuristic two-step procedure to reduce the search space in deriving the visual support region. At the ﬁrst step, the visual support region is constrained to be rectangular of arbitrary shape. The rectangular is centered on the node to be interpreted. The rectangular is initialized of area size 0, and is enlarged gradually (at even area increment). The enlargement stops upon the reconstruction error constraint is satisﬁed. At the second step, pixel-level visual support region is derived within the rectangular area. The image is segmented into super-pixels by the algorithm in [1], so as to restrict the solution space. At initial, all the super-pixels within the rectangular are counted in the visual support region (taking mask value 1). Then the super-pixels are gradually removed in a greedy manner. At each iteration, the super-pixel causing the smallest rise in reconstruction error is removed. The iteration stops till the constraint would be violated by removing anymore super-pixels.
We apply the two-step procedure to visualize network nodes in Faster R-CNN object detector [33]. We visualize

9

APbbox

46

44

42

40

38

36

34

32

30
Regular (ResNet−50)

28

Regular (ResNet−101)

DCN v2 (ResNet−50)

26

DCN v2 (ResNet−101)

24

400

600

800

1,000

1,200

1,400

Shorter side of input images

APSbbox

26

24

22

20

18

16

14

12

10

8

Regular (ResNet−50)

6

Regular (ResNet−101)

DCN v2 (ResNet−50)

4

DCN v2 (ResNet−101)

2

400

600

800

1,000

1,200

1,400

Shorter side of input images

(a) APbbox for all objects

(b) APbSbox for small objects

APbMbox

49

47

45

43

41

39

37

35

33

31

Regular (ResNet−50)

29

Regular (ResNet−101)

DCN v2 (ResNet−50)

27

DCN v2 (ResNet−101)

25

400

600

800

1,000

1,200

1,400

Shorter side of input images

APbLbox

62

60

58

56

54

52

50

48

46

44

Regular (ResNet−50)

42

Regular (ResNet−101)

DCN v2 (ResNet−50)

40

DCN v2 (ResNet−101)

38

400

600

800

1,000

1,200

1,400

Shorter side of input images

(c) APbMbox for medium objects

(d) APbLbox for large objects

Figure 4. APbbox scores of DCNv2 and regular ConvNets (Faster R-CNN + ResNet-50 / ResNet-101) on input images of varies resolution

on the COCO 2017 test-dev set.

(a) regular conv

(b) modulated deformable conv@conv3∼5 stages (DCNv2)

Figure 5. Spatial support of nodes in the last layer of the conv5 stage in DCNv2 and regular ConvNets. Input images are of shorter side 400 pixels (left), 800 pixels (middle), and 1400 pixels (right), respectively. The effective receptive ﬁeld and error-bounded saliency regions are shown in the top and bottom rows, respectively.

both feature map nodes shared on the whole image, and the 2fc node in the per-RoI detection head, which is directly followed by the classiﬁcation and the bounding box regression branches. For image-wise feature map nodes (at a certain location), square rectangular is applied in the two-step procedure. For RoI-wise feature nodes, the rectangular is of the same aspect ratio as the input RoI. For both image-wise and RoI-wise nodes, the reconstruction loss is one minus the cosine similarity between the feature vectors derived from masked and original images. The error upper bound is set as 0.1.

A2. DCNv2 with Various Image Resolution
Figure 4 presents the results of applying regular ConvNets and DCNv2 on images of various resolutions. The baseline model is Faster R-CNN with ResNet-50 [21] and ResNet-101. Models are trained and applied on images of shorter side {400, 600, 800, 1000, 1200, 1400} pixels, respectively. DCNv2 is found to outperform regular ConvNet on all input resolutions. For DCNv2, the highest APbbox scores are obtained at input images of shorter side 1,000 pixels. With the shorter side larger than 1,000 pixels, APbbox scores of regular ConvNet decrease noticeably, while those

10

method setting single-scale, shorter side 800

Faster R-CNN + ResNet-101 APbbox APb5b0ox APb7b5ox APbSbox APbMbox APbLbox 39.1 60.6 42.2 20.5 42.9 51.3

regular single-scale, shorter side 1000 (best) 39.2 60.6 42.4 21.6 42.2 51.3

multi-scale test

41.2 62.4 45.2 24.6 44.3 52.7

single-scale, shorter side 800

44.0 65.9 48.1 23.2 47.7 59.6

DCNv2 single-scale, shorter side 1000 (best) 44.8 66.3 48.8 24.4 48.1 59.6

multi-scale test

46.0 67.9 50.8 27.8 49.1 59.5

Table 5. Ablation study on input image resolution. Results are reported on the COCO 2017 test-dev set.

backbone

method

top-1 acc (%)

top-5 acc (%)

param

FLOP

regular 76.5 93.1 26.6M 4.1G

ResNet-50 DCNv1 76.6 93.2 26.8M 4.1G

DCNv2 78.2 94.0 27.4M 4.3G

regular 78.4 94.2 45.5M 7.8G

ResNet-101 DCNv1 78.4 94.2 45.8M 7.8G

DCNv2 79.2 94.6 47.4M 8.2G

regular 78.8 94.4 45.1M 8.0G

ResNeXt-101 DCNv1 78.9 94.4 45.6M 8.0G

DCNv2 79.8 94.8 49.0M 8.7G

trained on the ImageNet-1K training set. In experiments, we follow [39] for the training and inference settings. In DCNv1, the 3 × 3 conv layers in the conv5 stage are replaced by deformable conv layers. In DCNv2, all the 3 × 3 conv layers in the conv3∼conv5 stages are replaced by modulated deformable conv layers.
Table 6 presents the top-1 and top-5 classiﬁcation accuracies on the validation set. DCNv2 achieves noticeable improvements over both the regular and DCNv1 baselines, with minor additional computation overhead. The enriched deformation modeling capability of DCNv2 is beneﬁcial for the ImageNet classiﬁcation task itself.

Table 6. ImageNet classiﬁcation accuracies of DCNv2, DCNv1 and regular ConvNets.
of DCNv2 are almost unchanged. This phenomenon is more obvious for objects of large and medium sizes. As shown in Figure 5, the spatial support of regular ConvNets can just cover a small portion of the large objects at such high resolution, and the accuracy suffers. Meanwhile, the spatial support of DCNv2 can effectively adapt to objects at various resolutions.
Table 5 presents the results of multi-scale testing using ResNet-101. We ﬁrst apply the DCNv2 model trained on the best single-scale setting (shorter side of 1000 pixels) on multi-scale testing images. Following the latest Detectron [18] code base, test images range from shorter side of 400 to 1400 pixels with step size of 200 pixels. Multi-scale testing of DCNv2 improves the APbbox score by 1.2% compared with the best single-scale setting.
A3. ImageNet Pre-Trained DCNv2
It is well known that many vision tasks beneﬁt from ImageNet pre-training. This section investigates pre-training the learnable offsets and modulation scalars of DCNv2 on ImageNet [9], and ﬁnetuning on several tasks.
ImageNet Pretraining DCNv2 together with its DCNv1 [8] and regular ConvNet counterparts are pre-

Fine-tuning for Speciﬁc Tasks We investigate the effect of ImageNet pretrained DCNv2 models on several tasks, including object detection on Pascal VOC, ImageNet VID and COCO, and semantic segmentation on Pascal VOC3. In experiments, Faster R-CNN and DeepLab-v1 [5] are adopted as the baseline systems for object detection and semantic segmentation, respectively. For object detection on COCO, we follow the same settings as in Section 5.1. For experiments on Pascal VOC, we mainly follow [8] for the training and inference settings. Note that the baseline accuracy is higher than that reported in [8] mainly because of a better ImageNet pretrained model and the introduction of RoIAlign in object detection. For object detection on ImageNet VID, we mainly follow the protocol in [27, 43, 42] for the training and inference settings. The details are presented at the end of this section.
Table 7 compares the performance of DCNv2 on various tasks using different pre-trained models. By pre-training the learnable offsets and modulation scalars on ImageNet, rather than initializing them as zeros prior to ﬁne-tuning, noticeably accuracy improvements are observed on PASCAL VOC object detection and semantic segmentation. Meanwhile, the effect of pre-training on COCO detection is minor. This is probably because the larger and more challenging benchmark of COCO is sufﬁcient for learning the offsets and the modulation scalars from scratch.
3Note the mimicking module is not involved in semantic segmentation experiments.

11

method
regular DCNv2 DCNv2

offset&modulation pretraining
none none ImageNet

VOC det APb5b0ox APb7b0ox 81.9 68.2 83.7 72.4 84.9 73.5

VOC seg mIoU 72.0 76.1 78.3

ImageNet VID det APbbox 74.9 79.2 80.7

COCO det APbbox 39.2 44.8 44.9

Table 7. Finetuning the ImageNet-pretrained DCNv2 model for various tasks and benchmarks. ResNet-101 is utilized as the backbone.

ImageNet VID settings. The models are trained on the union of the ImageNet VID training set and the ImageNet DET training set (only the same 30 category labels are used), and are evaluated on the ImageNet VID validation set. In both training and inference, the input images are resized to a shorter side of 600 pixels. In RPN, the anchors are of 3 aspect ratios {1:2, 1:1, 2:1} and 4 scales {642, 1282, 2562, 5122}. 300 region proposals are generated for each frame at an NMS threshold of 0.7 IoU. SGD training is performed, with one image at each mini-batch. 120k iterations are performed on 4 GPUs, with each GPU holding one mini-batch. The learning rates are 10−3 and 10−4 in the rst 80k and last 40k iterations,respectively. In each mini-batch,images are sampled from ImageNet DET and ImageNet VID at a 1:1 ratio. The weight decay and the momentum parameters are set to 0.0001 and 0.9, respectively. In inference, detection boxes are generated at an NMS threshold of 0.3 IoU.
References
[1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, S. Su¨sstrunk, et al. Slic superpixels compared to state-ofthe-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):2274–2282, 2012. 9
[2] J. Ba and R. Caruana. Do deep nets really need to be deep? In NIPS, 2014. 2, 5, 7
[3] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In NIPS, 2016. 6
[4] D. Britz, A. Goldie, M.-T. Luong, and Q. Le. Massive exploration of neural machine translation architectures. In EMNLP, 2017. 6
[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv preprint arXiv:1606.00915, 2016. 7, 11
[6] B. Cheng, Y. Wei, H. Shi, R. Feris, J. Xiong, and T. Huang. Revisiting rcnn: On awakening the classiﬁcation power of faster rcnn. In ECCV, 2018. 3, 5
[7] P. Dabkowski and Y. Gal. Real time image saliency for black box classiﬁers. In NIPS, 2017. 2, 7, 9
[8] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei. Deformable convolutional networks. In ICCV, 2017. 1, 2, 3, 4, 5, 8, 11

[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 7, 11
[10] M. Denil, S. G. Colmenarejo, S. Cabi, D. Saxton, and N. de Freitas. Programmable agents. arXiv preprint arXiv:1706.06383, 2017. 6
[11] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010. 1
[12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. TPAMI, 2010. 6
[13] R. C. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In ICCV, 2017. 2, 7, 9
[14] J. Gehring, M. Auli, D. Grangier, and Y. N. Dauphin. A convolutional encoder model for neural machine translation. In ACL, 2017. 6
[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017. 6
[16] R. Girshick. Fast R-CNN. In ICCV, 2015. 1, 2 [17] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 2 [18] R. Girshick, I. Radosavovic, G. Gkioxari, P. Dolla´r, and K. He. Detectron. https://github.com/ facebookresearch/detectron, 2018. 7, 11 [19] J. Gu, H. Hu, L. Wang, Y. Wei, and J. Dai. Learning region features for object detection. In ECCV, 2018. 6 [20] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick. Mask r-cnn. In ICCV, 2017. 2, 7 [21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 2, 10 [22] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. STAT, 2015. 2, 5, 7 [23] Y. Hoshen. Vain: Attentional multi-agent predictive modeling. In NIPS, 2017. 6 [24] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks for object detection. In CVPR, 2018. 6 [25] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In NIPS, 2015. 6 [26] Y. Jeon and J. Kim. Active convolution: Learning the shape of convolution for image classiﬁcation. In CVPR, 2017. 7 [27] B. Lee, E. Erdenee, S. Jin, M. Y. Nam, Y. G. Jung, and P. K. Rhee. Multi-class multi-object tracking using changing point detection. In ECCV, 2016. 11

12

[28] Q. Li, S. Jin, and J. Yan. Mimicking very efﬁcient network for object detection. In CVPR, 2017. 5, 7
[29] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla´r, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. 1
[30] D. G. Lowe. Object recognition from local scale-invariant features. In ICCV, 1999. 6
[31] W. Luo, Y. Li, R. Urtasun, and R. Zemel. Understanding the effective receptive ﬁeld in deep convolutional neural networks. arXiv preprint arXiv:1701.04128, 2017. 2, 7
[32] D. Raposo, A. Santoro, D. Barrett, R. Pascanu, T. Lillicrap, and P. Battaglia. Discovering objects and their relations from entangled scene representations. In ICLR, 2017. 6
[33] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. 2, 10
[34] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. Orb: an efﬁcient alternative to sift or surf. In ICCV, 2011. 6
[35] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural network module for relational reasoning. In NIPS, 2017. 6
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, 2017. 6
[37] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In CVPR, 2018. 6
[38] N. Watters, D. Zoran, T. Weber, P. Battaglia, R. Pascanu, and A. Tacchetti. Visual interaction networks. In NIPS, 2017. 6
[39] S. Xie, R. Girshick, P. Dolla´r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In CVPR, 2017. 9, 11
[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. H. Pinheiro, S. Gross, S. Chintala, and P. Dollar. A multipath network for object detection. In BMVC, 2016. 7
[41] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In CVPR, 2016. 2, 7, 9
[42] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei. Flow-guided feature aggregation for video object detection. In ICCV, 2017. 11
[43] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei. Deep feature ﬂow for video recognition. In CVPR, 2017. 11
[44] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling. Visualizing deep neural network decisions: Prediction difference analysis. In ICLR, 2017. 2, 7, 9
13

