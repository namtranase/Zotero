ElasticFace: Elastic Margin Loss for Deep Face Recognition
Fadi Boutros1,2, Naser Damer1,2, Florian Kirchbuchner1, Arjan Kuijper1,2 1Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany 2Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany
Email: fadi.boutros@igd.fraunhofer.de

arXiv:2109.09416v3 [cs.CV] 13 Dec 2021

Abstract— Learning discriminative face features plays a major role in building high-performing face recognition models. The recent state-of-the-art face recognition solutions proposed to incorporate a ﬁxed penalty margin on commonly used classiﬁcation loss function, softmax loss, in the normalized hypersphere to increase the discriminative power of face recognition models, by minimizing the intra-class variation and maximizing the inter-class variation. Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the geodesic distance between and within the different identities can be equally learned using a ﬁxed penalty margin. However, such a learning objective is not realistic for real data with inconsistent inter-and intra-class variation, which might limit the discriminative and generalizability of the face recognition model. In this paper, we relax the ﬁxed penalty margin constrain by proposing elastic penalty margin loss (ElasticFace) that allows ﬂexibility in the push for class separability. The main idea is to utilize random margin values drawn from a normal distribution in each training iteration. This aims at giving the decision boundary chances to extract and retract to allow space for ﬂexible class separability learning. We demonstrate the superiority of our ElasticFace loss over ArcFace and CosFace losses, using the same geometric transformation, on a large set of mainstream benchmarks. From a wider perspective, our ElasticFace has advanced the state-of-the-art face recognition performance on seven out of nine mainstream benchmarks. All training codes, pre-trained models, training logs are publicly released 1.
I. INTRODUCTION
Face recognition technologies are increasingly deployed to enhance the security and convenience of processes involving identity veriﬁcation, such as border control and ﬁnancial services. The typical pipeline of a face recognition system involves mapping the face image (after detection and alignment [30]) into a feature vector (embedding) [4], [26], [2]. Two face images are then compared by comparing their relative embeddings and therefore, measuring the degree of identity similarity between both faces. Knowing that it is intuitive that such embeddings should ideally have small intra-class and large inter-class variation, with the class here being an identity. This corresponds to a face recognition system that still makes correct genuine decisions (same identity) even when face images are largely varied (pose, age, expression, etc.), and make correct imposter (not same identity) decision even when the appearance of the face image pair of different identities is very similar. To achieve that, different solutions opted to train deep neural networks by either directly learning the embedding (e.g. Triplet loss [22]) or by learning an
1https://github.com/fdbtrs/ElasticFace

identity classiﬁcation problem (e.g. Softmax loss [2]). One of the main challenges for training with metric-based learning such as Triple [22], n-pair [24], or contrastive [3] losses, is training the model with a large-scale dataset as the number of possible triplets explodes with the number of samples. Alternatively, classiﬁcation-based losses such as softmax loss can be easily adopted for training a face recognition model as it does not pose that issue. However, the softmax loss does not directly optimize the feature embedding needed for face veriﬁcation. Liu et al. [16] proposed a large-margin softmax (L-Softmax) by incorporating angular margin constraints on softmax loss to encourage intra-class compactness and inter-class separability between learned features. SphereFace [15] extended L-Softmax by normalizing the weights of the last full-connected layer and deploying multiplicative angular penalty margin between the deep features and their corresponding weights. Different from SphereFace, CosFace [26] proposed additive cosine margin on the cosine angle between the deep features and their corresponding weights. CosFace also proposed to ﬁx the norm of the deep features and their corresponding weights to 1, then scaling the deep feature norm to a constant s, achieving better performance on mainstream face recognition benchmarks. Later, ArcFace [4] proposed additive angular margin by deploying angular penalty margin on the angle between the deep features and their corresponding weights. The great success of softmax loss with penalty margin motivated several works to propose a novel variant of softmax loss [11], [14], [5], [13], [25], [10], [18], [1]. All these solutions achieved notable accuracies on mainstream benchmarks [9], [23], [27], [17] for face recognition. Huang et al. [10] proposed an Adaptive Curriculum Learning loss based on margin-based softmax loss. The proposed loss targets the easy samples at an early stage of training and the hard ones at a later stage of training. Jiao et al. [11] proposed Dyn-arcface based on ArcFace loss [4] by replacing the ﬁxed margin value of ArcFace with an adaptive one. The margin value of Dyn-arcface is adjusted based on the distance between each class center and the other class centers. However, this might not reﬂect the real properties of the class separability, but rather their separability in the current stage of the model training. Kim et al. [13] proposed to enrich the feature representation learned by ArcFace loss with group-aware representations. UniformFace [5] suggested to equalize distances between all the classes centers by adding a new loss function to SphereFace loss [15]. A recent work by An et al. [1]

presented an efﬁcient distributed sampling algorithm (PartialFC). The Partial-FC method is based on randomly sampling a small subset of the complete training set of classes for the softmax-based loss function. Thus, it enables the training of the face recognition model on a massive number of identities. The authors experimentally proved that training with only 10% of training samples using CosFace [26] and ArcFace[4] can achieve comparable results on mainstream benchmarks to the case when training is performed on a complete set of classes. MagFace [18] deployed magnitude-aware margin on ArcFace loss to enhance intra-class compactness by pulling high-quality samples close to class centers while pushing low-quality samples away. However, this is based on the weak assumption of optimal face quality (utility) estimation. Moreover, this might prevent the model from convergence when the most of training samples in the training dataset are of low quality.
The main challenge for the majority of the previously listed works is the ﬁne selection of the ideal margin penalty value. In this work, we propose the ElasticFace loss that relaxes the ﬁxed single margin value by deploying a random margin drawn from a normal distribution. We additionally extended this concept by guiding the assignment of the drawn margin values to put more attention on hardly classiﬁed samples. We provided a simple toy example with an 8-class classiﬁcation problem to demonstrate the enhanced separability and robustness induced by our ElasticFace loss. To experimentally demonstrate the effect of our ElasticFace loss on face recognition accuracy, we report the results on nine different benchmarks. The achieved results are compared to the results reported in the recent state-of-the-art. In a detailed comparison, compared to ﬁxed margin penalties and recent state-of-the-art, our ElasticFace loss enhanced the face recognition accuracy on most of the considered benchmarks, consequently extending state-of-the-art face recognition performance on seven out of nine benchmarks and scoring close to the state-of-the-art in the remaining two. This is especially the case in the benchmarks where the intra-class variation is extremely high, such as frontal-to-proﬁle face veriﬁcation (CFP-FP [23]) and large age gap face veriﬁcation (AgeDB30 [19]), which points to the generalizability induced by the proposed ElasticFace.
In the rest of this paper, we will ﬁrst introduce our proposed ElasticFace loss by building up to its deﬁnition starting from the basic softmax loss. This rationalization will include an experimental toy example demonstrating the effect of the proposed loss. Later on, the experimental setup and implementation details are introduced. This is followed by a detailed comparative discussion of the achieved results and a ﬁnal conclusion.
II. ELASTICFACE LOSS
We propose in this work a novel learning loss strategy, ElasticFace loss, aiming at improving the accuracy of face recognition by targeting enhanced intra-class compactness and inter-class discrepancy in a ﬂexible manner. Unlike previous works [4], [15], [26] that utilize a ﬁxed penalty

margin value, our proposed ElasticFace loss accommodates ﬂexibility through relaxing this constraint by randomly drawing the margin value from a Gaussian distribution. Our proposed ElasticFace loss does not only target giving the model ﬂexibility in optimizing the separability between and within the classes but also avoiding overﬁtting (thus enhancing generalizability) the model as it incorporates random margin values for each sample in each training iteration. The randomized margin penalty can be easily integrated into any of the angular margin-based softmax losses, which we demonstrate on two state-of-the-art margin-based softmax losses. The angular margin-based losses and our ElasticFace loss extend over the softmax loss by manipulating the decision boundary to enhance intra-class compactness and inter-class discrepancy. Therefore, in this section, we ﬁrst revisit the conventional softmax loss. Then, we present the modiﬁed version of softmax loss and the angular marginbased softmax loss. This leads up to presenting our proposed ElasticFace loss and an extended deﬁnition, the ElasticFace+, where the assignment of the drawn margins to training samples is linked to their proximity to their class centers.
a) Softmax Loss: The widely used multi-class classiﬁcation loss, softmax loss [16], refers to applying crossentropy loss between the output of the logistic function (softmax activation function) and the ground-truth. Assume xi ∈ Rd is a feature representation of the i-th sample zi and yi is its corresponding class label (yi integer in the range [1, c]). Given that c is the number of classes (identities), the output of the softmax activation function is deﬁned as follows:

efyi

exiWyTi +byi

sof tmax(xi, yi) =

c

=

efj

c

, exiWjT +bj

(1)

j=1

j=1

where fyi is the activation of the last fully-connected layer with weight vector Wyi and bias byi . Wyi is the yi-th column of weights W ∈ Rcd and byi is the corresponding bias offset. The output of the softmax activation function is the
probability of xi being correctly classiﬁed as yi. Given a mini-batch of size N, the cross-entropy loss function that
measures the divergence between the model output and the
ground-truth labels can be deﬁned as follows:

1

exiWyTi +byi

LCE = N

−log

i∈N

c

. exiWjT +bj

(2)

j=1

In a simple binary class classiﬁcation, assuming that the input zi belong to class 1, the model will correctly classify zi if W1T xi+b1 > W2T xi+b2 and zi will be classiﬁed as class 2 if W2T xi + b2 > W1T xi + b1. Therefore, the decision boundary of softmax loss is x(W1T − W2T ) + b1 − b2 = 0. One of the main limitations of using softmax loss for learning face embeddings is that softmax loss does not explicitly optimize the feature representation needed for face veriﬁcation as there is no restriction on the minimum distance between the class centers. Thus, training with softmax loss is less than optimal

2

for achieving the maximum inter-class distances and the
minimum intra-class distances. To mitigate this limitation,
margin penalty-based cosine softmax loss was proposed as
an alternative to the conventional softmax loss and it became
a popular loss function for training face recognition models
[4], [26], [15]. To get there, [15] has proposed a modiﬁed
softmax loss (Cosine softmax loss) that optimized the angle cosine between features and the weights cos(θ) and then, incorporates a margin penalty on cos(θ).
b) Cosine Softmax Loss: Following [4], [26], [15], [16], the bias offset, for simplicity, can be ﬁxed to byi = 0. The logit fyi , in this case, can be reformulated as: xiWyTi = xi Wyi cos(θyi ), where θyi is the angle between the weights of the last fully-connected layer Wyi and the feature representation xi. By ﬁxing the weights norm and the feature norm to Wyi = 1 and xi = 1, respectively, and rescaling the xi to the constant s [26], the output of the softmax activation function is subject to the cosine of the angle θyi . The modiﬁed softmax loss (LML) can be deﬁned, as stated in [26], [15], as follows:

1

es(cos(θyi ))

LM L

=

N

i∈N

−log es(cos(θyi ))

+

c

. (3) es(cos(θj ))

j=1,j=yi

In the previous binary example, assume that the input zi belong to the class 1, zi will be correctly classiﬁed if cos(θ1) > cos(θ2). The decision boundary, in this case, is cos(θ1)−cos(θ2) = 0. Therefore, training with the modiﬁed (cosine) softmax loss emphasizes that the model prediction depends on the angle cosine between the features and the weights. However, and similar to conventional softmax loss, modiﬁed softmax loss does not explicitly optimize the feature representation needed for face veriﬁcation. This motivated the introduction of the angular margin penalty-based losses [4], [26], [15].
c) Angular Margin Penalty-based Loss: In recent works [4], [26], [15], different types of margin penalty are proposed to push the decision boundary of softmax, and thus enhance intra-class compactness and inter-class discrepancy aiming at improving the accuracy of face recognition. The general angular margin penalty-based loss (LAML) is deﬁned as follows:

L = −log , AML

1 N
i∈N

es(cos(m1θyi +m2)−m3)

c
es(cos(m1θyi +m2)−m3)+

es(cos(θj ))

j=1,j=yi

(4)

where m1, m2 and m3 are the margin penalty parameters

proposed by SphereFace [15], ArcFace [4] and CosFace

[26], respectively. In SphereFace [15], multiplicative angular

margin penalty is deployed by multiplying θ by m1 = α

and setting m2 = 0 and m3 = 0 ( α > 1.0). The decision

boundary of SphereFace is then cos(m1θyi ) − cos(θj) = 0. Differently, CosFace [26] proposed additive cosine margin

penalty by setting m1 = 1, m2 = 0 and m3 = α (0 <

α

<

1

−

cos(

π 4

)).

The

decision

boundary

of

CosFace

is

then

cos(θyi ) − cos(θj) − m3 = 0. Later, ArcFace [4] proposed additive angular margin penalty by setting up m1 = 1, m2 = α and m3 = 0 (0 < α < 1.0). The decision boundary of ArcFace is then cos(θyi + m2) − cos(θj) = 0.
Even though, ArcFace [4], CosFace [26] and SphereFace

[15] introduced the important concept of angular margin

penalty on softmax loss, selecting a single optimal margin

value (α) is a critical issue in these works. By setting up

m1 = 1, m2 = 0 and m3 = 0, ArcFace, CosFace and SphereFace are equivalent to the modiﬁed softmax loss. A

reasonable choice could be selecting a large margin value

that is close to the margin upper bound to enable higher

separability between the classes. However, when the margin

value is too large, the model fails to converge, as stated in

[26]. ArcFace, CosFace, and SphereFace selected the margin

value through trial and error assuming that the samples

are equally distributed in geodesic space around the class

centers. However, this assumption could not be held when

there are largely different intra-class variations leading to less

than optimal discriminative feature learning, especially when

there are large variations between the samples/classes in the

training dataset. This motivated us to propose ElasitcFace

loss by utilizing random margin penalty values drawn from

a Gaussian distribution aiming at giving the model space for

ﬂexible class separability learning.

d) Elastic Angular Margin Penalty-based Loss (Elas-

ticFace): The proposed ElasticFace loss is extended over

the angular margin penalty-based loss by deploying random

margin penalty values drawn from a Gaussian distribution.

Formally, the probability density function of a normal distri-

bution is deﬁned as follows:

f (x) =

1 √

e , −

1 2

(

x−µ σ

)2

(5)

σ 2π

where µ is the mean of the distribution and σ is its standard deviation. To demonstrate and prove our proposed elastic margin, we chose to integrate the randomized margin penalty in ArcFace (noted as ElasticFace-Arc) and CosFace (noted as ElasticFace-Cos) as they proved to have clearer geometric interpretation and achieved higher accuracy on mainstream benchmarks than the earlier SphereFace. ElasticFace-Arc (LEArc) can be deﬁned as follows:

L = −log , EArc

1 N
i∈N

es(cos(θyi +E(m,σ)))

es(cos(θyi +E(m,σ)))+

c

es(cos(θj ))

j=1,j=yi

(6)

and ElasticFace-Cos (LECos) can be deﬁned as follows:

L = −log , ECos

1 N
i∈N

es(cos(θyi )−E(m,σ))

c
es(cos(θyi )−E(m,σ))+

es(cos(θj ))

j=1,j=yi

(7)

where E(m, σ) is a normal function that return a random

value from a Gaussian distribution with the mean m and the

standard deviation σ.

The decision boundaries of ElasticFace-Arc and

ElasticFace-Cos are cos(θyi + E(m, σ)) − cos(θj) = 0 and cos(θyi ) − cos(θj) − E(m, σ) = 0, respectively. Figure 1 illustrates the decision boundary of ArcFace,

3

Fig. 1: Decision boundary of (a) ArcFace, (b) ElasticFace-Arc, (c) CosFace, and (d) ElasticFace-Cos for binary classiﬁcation. The dashed blue line is the decision boundary. The gray area illustrates the decision margin.

Loss
ArcFace (m=0.55) ArcFace (m=0.5) ArcFace(m=0.45) ElasticFace-Arc(m=0.5, σ=0.0125) ElasticFace-Arc(m=0.5, σ=0.0175) ElasitcFace-Arc(m=0.5,σ=0.025) ElasitcFace-Arc(m=0.5,σ=0.05) ElasitcFace-Arc+ (m=0.5,σ=0.0125) ElasitcFace-Arc+ (m=0.5, σ=0.0175) ElasitcFace-Arc+ (m=0.5, σ=0.025) ElasitcFace-Arc+ (m=0.5,σ=0.05)

LFW Accuracy (%) 99.52 99.46 99.43 99.53 99.47 99.52 99.52 99.53 99.53 99.42 99.45

AgeDB-30 Accuracy (%) 94.58 94.83 94.66 94.80 95.13 94.95 94.82 95.00 95.07 95.15 94.83

CALFW Accuracy (%) 93.82 93.88 93.80 93.68 93.67 93.78 93.90 93.68 93.95 93.73 94.00

CPLFW Accuracy (%) 89.05 89.72 89.42 89.72 89.53 89.50 89.79 89.58 89.37 89.48 89.50

CFP-FP Accuracy (%) 95.24 95.36 95.53 95.43 95.54 95.44 95.59 95.40 95.67 95.36 95.56

TABLE I: Parameter selection for ElasticFace-Arc and ElasticFace-Arc+. In all settings, the used architecture is ResNet-50 trained on CASIA [29].

Loss
CosFace (m=0.4) CosFace (m=0.35) CosFace (m=0.3) ElasticFace-Cos (m=0.35,σ=0.0125) ElasticFace-Cos (m=0.35,σ=0.0175) ElasticFace-Cos (m=0.35,σ=0.025) ElasticFace-Cos (m=0.35,σ=0.05) ElasticFace-Cos+(m=035, σ=0.0125 ElasticFace-Cos+(m=035, σ=0.0175) ElasticFace-Cos+(m=035, σ=0.025) ElasticFace-Cos+(m=035, σ=0.05)

LFW Accuracy (%) 99.42 99.55 99.45 99.45 99.50 99.42 99.52 99.38 99.45 99.55 99.48

AgeDB-30 Accuracy (%) 94.65 94.45 94.55 94.72 94.77 94.85 94.77 94.50 94.97 94.63 94.45

CALFW Accuracy (%) 93.45 93.78 93.46 93.83 93.97 93.88 93.93 93.67 93.48 93.65 93.77

CPLFW Accuracy (%) 90.38 89.95 90.12 90.12 90.10 90.20 90.38 89.85 89.98 90.28 90.01

CFP-FP Accuracy (%) 95.30 95.31 95.39 95.47 95.30 95.21 95.52 95.20 95.23 95.47 95.26

TABLE II: Parameter selection for ElasticFace-Cos and ElasticFace-Cos+. In all settings, the used architecture is ResNet-50 trained on CASIA [29].

ElasticFace-Arc, CosFace and ElasticFace-Cos. The sample push towards its center during training using ElasticFace-Arc and ElasticFace-Cos varies between training samples, based on the margin penalty drawn from E(m, σ). During the training phase, a new random margin is generated for each sample in each training iteration. This aims at giving the model ﬂexibility in the push for class separability. When σ is 0, our ElasticFace-Arc and ElasticFace-Cos are equivalent to ArcFace and CosFace, respectively.
e) ElasticFace+: We propose an extension to our ElasticFace, the ElasticFace+, that observes the intra-class variation during each training iteration and use this observation to assign a margin value to each sample based on its proximity to its class center. This causes the samples that are relatively far from their class center to be pushed with a larger penalty margin to their class center. This aims at giving the model space to push the samples that are relatively far from their class center to be closer to their centers while giving less penalty attention to the samples that are already close to their center. To achieve that, we sort (descending) the output of the Gaussian distribution function (Equation 5) based on cos(θyi ) value. Thus, the sample with small cos(θyi ) will be pushed with large value from E(m, σ) function, and vice versa.
f) Parameter Selection: The probability density function has its peak around m [21]. Thus, when ElasticFace is integrated into ArcFace [4], we select the best margin value (as a single value) by training three instances of ResNet-50 [8] on CASIA [29] with ArcFace loss using margins equal to 0.45, 0.50 and 0.55, respectively, to assure the advised margin in [4]. Then, based on the sum of the performance

ranking Borda count on LFW [9], AgeDB-30 [19], CALFW [33], CPLFW [32], and CFP-FP [23], we select the margin that achieved the highest Borda count sum and set it as m for E(m, σ) function, where our goal is to use the most optimal margin. The best margin observed in our experiment, in this case, is 0.5 (Table I). To select the σ value for E(m, σ) function, we conducted additional experiments on four instances of ResNet-50 trained on CASIA [29] with our proposed ElasticFace-Arc by setting up the σ to one of these values 0.0125, 0.015, 0.025 and 0.05. Then, we rank these models based on the sum of the performance ranking Borda count across all datasets. Finally, the σ value is chosen based on the highest Borda count sum. The best σ observed in our experiment, in this case, is 0.05 (Table I). Similarly, we follow the same procedure to select the parameters (m and σ) for ElasticFace-Cos. We ﬁrst choose the best margin value by training three different instances of ResNet-50 on CASIA [29] with CosFace using a margin equal to 0.3, 0.35, and 0.40. The best m observed in our experiment based on the sum of the performance ranking Borda count across all evaluated datasets, in this case, is 0.035 (Table II). Similar to σ selection approach of ElasticFace-Arc, we train four instance of ElasticFace-Cos to choose the best σ for E(m, σ) function. The best observed σ in our experiment, in this case, is 0.05 (Table II). For ElasticFace-Cos+ and ElasticFaceArc+, we followed the exact approach of parameter selection for ElasticFace-Arc and ElasticFace-Cos. The best observed σ for ElasticFace-Arc+ is 0.0175 and the best observed one for ElasticFace-Cos+ is 0.025 (Table I and II). These selected parameters are used to train our solutions (training details in Section III) evaluated in Section IV.

4

1.00

0.015

0.75

0.05

0.01

0.50

0.25

0.00

−0.25 −0.50

0.065

−0.75

55.943°3.17°

26.95°

66.58°

22.93°

30.65° 72.435° 1.36°

0.01 0.015

0.02

−1.00

0.04

−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00

1.00 0.75

0.015

0.025

0.50 0.02
0.25 0.00
−0.25 0.01

46.05° 49.22°
56.28°
43.95° 38.59°
42.04° 40.26°
43.6°

0.015

−0.50

0.01

−0.75 −1.00

0.01

0.01

−1.00 −0.75 −0.50 −0.25 0.00

0.25

0.50

0.75

1.00

1.00

0.75 0.50

0.05

0.25

0.00
0.03
−0.25

−0.50

0.07

0.07

55.393°9.45° 41.57°
44.24° 41.37°
49.21° 39.27°
49.51°

0.015 0.015

−0.75 −1.00

0.03

0.07

−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00

(a) ArcFace (m = 0.5)

(b) ElasticFace-Arc (m = 0.5, σ = 0.05) (c) ElasticFace+ (m = 0.5, σ = 0.0175)

Fig. 2: Toy example of 3 ResNet-18 networks trained under different experimental settings. The 2-D features are normalized. Thus, the feature embeddings are allocated around the class centers in the arc space with a ﬁxed radius. The numbers next to each class center indicate the mean of the standard deviation of each class feature embeddings. The angle in degree are calculated between each two consecutive classes to illustrate the decision margin between the classes. One can noticed that feature produced by ElasticFace and ElasticFace+ are more equally distributed around the class centers than ArcFace, in the arc space. Same colors always indicates same class across plots.

g) Toy example: To demonstrate the robustness and the class separability induced by our proposed ElasticFace and ElasticFace+, we present a simple toy example by training three ResNet-18 networks [8] to classify eight different identities and produce 2-D feature embeddings. All the networks are trained with a small batch size of 128 for 11200 iterations with stochastic gradient descent (SGD) and an initial learning rate of 0.1. The learning rate is reduced by a factor of 10 after 1680, 2800, 3360, and 8400 training iterations. To demonstrate a classiﬁcation case where the classes are not identically varied, these eight identities are selected to have four identities with small intra-class variation and another four identities with a large intra-class variation (measured as the average of all intra-class comparison scores for each identity). These identities were chosen from all the identities with more than 400 images per identity in the MS1MV2 dataset [4], we note this selected subset as MS1MV2-400. From these identities, we select the four identities with the highest intra-class variation and the four with the lowest intra-class variation. The features for this selection were extracted using an open-source 2 ResNet-100 [8] model trained with ArcFace loss [4], and the comparison is performed by a cosine similarity. The set of the selected eight identities is noted as MS1MV2-8. We use MS1MV2-8 to train the toy networks with ArcFace (m=0.5), ElasticArcFace (m=0.5, σ=0.05), and ElasticArcFace+ (m=0.5, σ=0.0175), based on our parameter selection. Figure 2 shows the classiﬁcation of MS1MV2-8 for each of the experimental settings. In each of the plots in Figure 2a, 2b and 2c, we calculate the angle between each consecutive identities to demonstrate the separability between the identities in the arc space (inter-class discrepancy). The optimal inter-class discrepancy
2https://github.com/deepinsight/insightface

may be achieved if the angle, in degree, between each of consecutive identities is close to 45 degrees i.e. 360 / 8. Also, we calculate the mean of the standard deviation of each class feature embeddings to illustrate intra-class compactness induced by ArcFace, ElasticFace, and ElasticFace+. The smaller standard deviation (shown at the edge of each class in Figure 2), in this case, indicates higher intra-class compactness. It can be noticed that our EalsticFace and EalsticFace+ achieved better intra-class compactness and inter-class discrepancy than ArcFace, while the differences in inter-class variation between EalsticFace and EalsticFace+ are minor (Figures 2a 2c, and 2b).
III. EXPERIMENTAL SETUP
a) Training settings:: The network architecture we used to demonstrate our ElasticFace is the ReseNet-100 [8]. This was motivated by the wide use of this architecture in the state-of-the-art face recognition solutions [4], [1], [5], [25], [10]. We follow the common setting [4], [1], [10] to set the scale parameter s to 64. We set the mini-batch size to 512 and train our model on one Linux machine (Ubuntu 20.04.2 LTS) with Intel(R) Xeon(R) Gold 5218 CPU 2.30GHz, 512 G RAM, and 4 Nvidia GeForce RTX 6000 GPUs. The proposed models in this paper are implemented using Pytorch [20]. All models are trained with Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 1e-1. We set the momentum to 0.9 and the weight decay to 5e-4. The learning rate is divided by 10 at 80k, 140k, 210k, and 280k training iterations. The total number of training iteration is 295K, which corresponds to the number of margin sampling from the normal distribution. During the training, we use random horizontal ﬂipping with a probability of 0.5 for data augmentation. The networks are trained (and evaluated) on images of the size 112 × 112 × 3 to produce 512 − d feature

5

Method
ArcFace[4] (CVPR2019) CosFace[26] (CVPR2018) Dynamic-AdaCos[31] (CVPR2019) AdaptiveFace[14] (CVPR2019) UniformFace[5] (CVPR2019) GroupFace[13] (CVPR2020) CircleLoss[25] (CVPR2020) CurricularFace[10] (CVPR2020) Dyn-arcFace [11] (MTAP2021) MagFace[18] (CVPR2021) Partial-FC-ArcFace [1] (ICCVW2021) Partial-FC-CosFace [1] (ICCVW2021) ElasticFace-Arc (ours) ElasticFace-Cos (ours) ElasticFace-Arc+ (ours) ElasticFace-Cos+ (ours)

Training Dataset MS1MV2 [7], [4] private clean MS1M [7], [31] + CASIA [29] clean MS1M [7], [28] clean MS1M [7], [4] + VGGFace2 [2] clean MS1M [7], [4] clean MS1M [7], [25] MS1MV2 [7], [4] clean MS1M [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4]

LFW Accuracy (%)
99.82 99.73 99.73 99.62 99.8 99.85 99.73 99.80 99.80 99.83 99.83 99.83 99.80 99.82 99.82 99.80

AgeDB-30 Accuracy (%)
98.15 -
98.28 -
98.32 97.76 98.17 98.20 98.03 98.35 98.27 98.35 98.28

CALFW Accuracy (%)
95.45 -
96.20 -
96.20 -
96.15 96.18 96.20 96.17 96.03 96.17 96.18

CPLFW Accuracy (%)
92.08 -
93.17 -
93.13 -
92.87 93.00 93.10 93.27 93.17 93.28 93.23

CFP-FP Accuracy (%)
98.27 -
98.63 96.02 98.37 94.25 98.46 98.45 98.51 98.67 98.61 98.60 98.73

TABLE III: The achieved results on the LFW, AgeDB-30, CALFW, CPLFW, and CFP-FP benchmarks. On large age gape (AgeDB-30) and frontal-to-proﬁle face comparisons (CFP-FP), the ElasticFace solutions consistently extend state-of-the-art performances. ElasticFace scores very close to the state-of-the-art on LFW and CALFW. All decimal points are provided as reported in the respective works. The top performance in each benchmark is in bold.

embeddings. These images are aligned and cropped using the Multi-task Cascaded Convolutional Networks (MTCNN) [30] following [4]. All the training and testing images are normalized to have pixel values between -1 and 1.
b) Training dataset:: We follow the trend in recent works [4], [1], [10], [18] in using the MS1MV2 dataset [4] to train the investigated models with the proposed ElasticFace loss. This enables a direct comparison with the state-of-theart as will be shown in Section IV. The MS1MV2 is a reﬁned version [4] of the MS-Celeb-1M [7] containing 5.8M images of 85K identities.
c) Evaluation benchmarks and metrics:: To demonstrate the effect of our proposed ElasticFace on face recognition accuracy and enable a wide comparison to stateof-the-art, we report the achieved results on nine benchmarks. These benchmarks are of a diverse nature, where some represent a special vulnerabilities of face recognition. The nine benchmarks are 1) Labeled Faces in the Wild (LFW) [9], 2) AgeDB-30 [19], 3) Cross-age LFW (CALFW) [33], 4) Cross-Pose LFW (CPLFW) [32], 5) Celebrities in Frontal-Proﬁle in the Wild (CFP-FP) [23], 6) IARPA Janus Benchmark-B (IJB-B) [27], 7) IARPA Janus BenchmarkC (IJB-C) [17], 8) MegaFace [12], and 9) MegaFace (R) [4]. The face recognition performance on LFW, AgeDB-30, CALFW, CPLFW, and CFP-FP is reported as veriﬁcation accuracy, following their evaluation protocol. The performance on IJB-C and IJB-B is reported (as deﬁned in [27], [17]) as true acceptance rates (TAR) at false acceptance rates (FAR) of 1e-4. The MegaFace and MegaFace(R) benchmarks report the face recognition performance as Rank-1 correct identiﬁcation rate and as TAR at FAR=1e–6 veriﬁcation accuracy.
We acknowledge the veriﬁcation and identiﬁcation performance evaluation metrics reported in ISO/IEC 19795-1 [?]. However, to enhance the reproducibility and comparability, we follow the evaluation protocols and metrics used in each of the benchmarks as listed above.

IV. RESULTS
Tables III and IV presents the achieved results on the nine considered benchmarks. The main observation is that our proposed ElasticFace solutions scored beyond the state-ofthe-art in seven out of the nine benchmarks, and very close to the state-of-the-art in the remaining two. When possible, and to build a fair comparison, the results of previous works are reported when trained on the MS1MV2 [7], [4] (or a reﬁned variant of MS1M [7]) as the ElasticFace results are based on training on this dataset. The proposed ElasticFace ranked ﬁrst in comparison to the state-of-the-art on the benchmarks AgeDB-30, CPLFW, CFP-FP, IJB-B, IJB-C, MegaFace (R), and MegaFace (veriﬁcation). In the remaining benchmarks, ElasticFace solutions ranked second on CALFW, third on LFW, and fourth on MegaFace (identiﬁcation).
A main outcome of the evaluation is concerning the databases with very large intra-user variations. These are the large age gape benchmark (AgeDB-30) and the frontal-toproﬁle face veriﬁcation benchmark (CFP-FP). On AgeDB30, our ElasticFace-Arc solution scored an accuracy of 98.35%, while the top state-of-the-art performance was 98.32% scored by the CurricularFace [10]. On CFP-FP, our ElasticFace-Arc+ solution scored an accuracy of 98.73% and our ElasticFace-Arc scored an accuracy of 98.67%, while the top state-of-the-art performances were 98.51% scored by the Partial-FC-CosFace [1] solution and 98.46% scored by the MagFace [18]. This signiﬁcantly enhanced performance in the extreme intra-class variation scenarios points out the generalizability induced by the ElasticFace loss. CALFW and CPLFW also considered age gaps and pose variation, however, with a lower variation than AgeDB-30 and CFPFP. In CALFW, ElasticFace-Cos+ scored a close second with 96.18% accuracy, with the lead going to the CurricularFace [10] with 96.20% accuracy. In CPLFW, our ElasticFace-Arc+ is ranked ﬁrst with 93.28% accuracy, while the top stateof-the-art performance was 93.17% accuracy scored by the GroupFace [13]. On the LFW benchmark [9], which is one

6

Method
ArcFace[4] (CVPR2019) CosFace[26] (CVPR2018) Dynamic-AdaCos[31] (CVPR2019) AdaptiveFace[14] (CVPR2019) UniformFace[5] (CVPR2019) GroupFace[13] (CVPR2020) CircleLoss[25] (CVPR2020) CurricularFace[10] (CVPR2020) Dyn-arcFace [11] (MTAP2021) MagFace[18] (CVPR2021) Partial-FC-ArcFace [1] (ICCVW2021) Partial-FC-CosFace [1] (ICCVW2021) ElasticFace-Arc (ours) ElasticFace-Cos (ours) ElasticFace-Arc+ (ours) ElasticFace-Cos+ (ours)

Training Dataset
MS1MV2 [7], [4] private
clean MS1M [7], [31] + CASIA [29] clean MS1M [7], [28]
clean MS1M [7], [4] + VGGFace2 [2] clean MS1M [7], [4] clean MS1M [7], [25] MS1MV2 [7], [4] clean MS1M [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4] MS1MV2 [7], [4]

IJB-B TAR at FAR1e–4 (%)
94.2 -
94.93 -
94.8 -
94.51 94.8 95.0 95.22 95.30 95.09 95.43

IJB-C TAR at FAR1e–4 (%)
95.6 -
92.40 -
96.26 93.95 96.1
95.97 96.2 96.4 96.49 96.57 96.40 96.65

MegaFace (R)

Rank-1 (%)

TAR at FAR1e–6 (%)

98.35

98.48

-

-

97.41

-

95.023

95.608

-

-

98.74

98.79

98.50

98.73

98.71

98.64

-

-

-

-

98.31

98.59

98.36

98.58

98.81

98.92

98.70

98.75

98.80

98.83

98.62

98.85

MegaFace

Rank-1 (%)

TAR at FAR1e–6 (%)

81.03

96.98

82.72

96.65

-

-

-

-

79.98

95.36

81.31

97.35

-

-

81.26

97.26

-

-

-

-

-

-

-

-

80.76

97.30

81.01

97.31

80.68

97.44

80.08

97.29

TABLE IV: The achieved results on the IJB-B, IJB-C, MegaFace (R), and MegaFace benchmarks. On the earlier three, and the veriﬁcation accuracy of the fourth, the ElasticFace solutions consistently extend state-of-the-art performances. ElasticFace scores very close to the state-of-the-art on MegaFace. MegaFace has been reﬁned in [4] to MegaFace (R) as it contains many face images with wrong labels. All decimal points are provided as reported in the respective works. The top performance in each benchmark is in bold.

of the oldest and nearly saturated benchmarks reported in the recent works, our ElasticFace-Cos and ElasticFace-Arc+ solutions scored an accuracy of 98.82%, very close behind the GroupFace [13] with 99.85%.
In Table IV, on IJB-B benchmark, our ElasticFace-Cos+ scored a TAR at FAR1e–4 of 95.43%, far ahead of the Partial-FC-CosFace [1] and the GroupFace [13] with 95.0% and 94.93%, respectively. Similarly, on the IJB-C benchmark, our ElasticFace-Cos+ scored a TAR at FAR1e–4 of 96.65%, ahead of the Partial-FC-CosFace [1] and the GroupFace [13] with 96.4% and 96.36% respectively. On the MegaFace (R), our ElasticFace-Arc scored 98.81% Rank-1 identiﬁcation rate and 98.92% TAR at FAR1e–6, ahead of the previous lead solution, the GroupFace [13] with 98.74% and 98.79%, respectively. On the MegaFace benchmark, our ElasticFace-Cos scored Rank-1 identiﬁcation rate of 81.01%, close to the state-of-the-art 82.72% score by CosFace [26], noting that CosFace was trained on a private dataset. On the same benchmark (MegaFace), our ElasticFace-Arc+ ranked ﬁrst with 97.44% TAR at FAR1e–6, while the top state-ofthe-art performances were 97.35% scored by the GroupFace [13]. It must be mentioned that the MegaFace benchmark has been reﬁned in [4] to MegaFace (R) as it contains many face images with wrong labels as reported in [4].
In comparison to the closely deﬁned losses in ArcFace [4], CosFace [26], and Partial-FC [1] solutions, our ElasticFace models did prove to provide a strong performance edge by scoring higher recognition performance on most benchmarks. When it comes to comparing ElasticFace and ElasticFace+, the ElasticFace-Arc and ElasticFace-Arc+ did achieve very close performances when considering all benchmarks. On the other hand, the ElasticFace-Cos+ did outperform ElasticFace-Cos on most benchmarks.
We acknowledge that the Partial-FC [1] solution reported additional performance rates when trained on their new collected database, the Glint360K [1]. However, we could not acquire this database as it requires an account on a cloud

platform, that in itself requires a SIM card registered in a speciﬁc country, which is very restrictive and we do not have access to. Therefore, and for a fair comparison, we opted to compare our results with the Partial-FC results when trained on the same dataset that our ElasticFace solution is using, the MS1MV2 [7], [4] dataset.
The slightly increased training computational cost is a minor limitation of our proposed ElasticFace. Training the ResNet-100 model on MS1MV2 dataset with CosFace or ArcFace using the speciﬁed machine and training details described in Section III requires around 57 hours. This training time is increased by around one minute for ElasticFace and by 11 hours for ElasticFace+. The minor increase in the ElasticFace training time is caused by the sampling of the margin values, while the larger increase in ElasticFace+ training time is additionally caused by the sorting algorithms.
On a less technical note, we stress that our efforts in the advancement of face recognition are aimed at enhancing the security, convenience, and life quality of the members of society, e.g. enabling convenient access to ﬁnancial and health services [6] and enhancing the security of border checks within clear legal frameworks and users consent. We acknowledge and reject the possible malicious or illegal use of this and other technologies.
V. CONCLUSION
In this paper, we propose an elastic margin penalty loss (ElasticFace) that avoids setting a single constant penalty margin. Our motivation considers that real training data is inconsistent in terms of inter and intra-class variation, and thus the assumption made by many margin softmax losses that the geodesic distance between and within the different identities can be equally learned using a ﬁxed margin is less than optimal. We, therefore, relax this ﬁxed margin constrain by using a random margin value drawn from a normal distribution in each training iteration. In an extended deﬁnition, the assignment of these margin values

7

to training samples corresponds to their proximity to their
class centers. We evaluated our ElasticFace loss, in com-
parison to state-of-the-art face recognition approaches, on
nine different benchmarks. This evaluation demonstrated that
our ElasticFace solution consistently extended state-of-the-
art face recognition performance on most benchmarks (seven
out of nine). This was speciﬁcally apparent in the challenging
benchmarks with large intra-class variations, such as large
age gaps and frontal-to-proﬁle face comparisons. Our code,
trained models, and training details will be released under
the Attribution-NonCommercial-ShareAlike 4.0 International
(CC BY-NC-SA 4.0) license.
REFERENCES
[1] X. An, X. Zhu, Y. Xiao, L. Wu, M. Zhang, Y. Gao, B. Qin, D. Zhang, and F. Ying. Partial fc: Training 10 million identities on a single machine. In Arxiv 2010.05222, 2020.
[2] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 13th IEEE International Conference on Automatic Face & Gesture Recognition, FG 2018, Xi’an, China, May 15-19, 2018, pages 67–74. IEEE Computer Society, 2018.
[3] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face veriﬁcation. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005), 20-26 June 2005, San Diego, CA, USA, pages 539–546. IEEE Computer Society, 2005.
[4] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4690– 4699, 2019.
[5] Y. Duan, J. Lu, and J. Zhou. Uniformface: Learning deep equidistributed representation for face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3415–3424. Computer Vision Foundation / IEEE, 2019.
[6] e-Aadhaar - Unique Identiﬁcation Authority of India. https:// eaadhaar.uidai.gov.in/, 2015.
[7] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In B. Leibe, J. Matas, N. Sebe, and M. Welling, editors, ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part III, volume 9907 of Lecture Notes in Computer Science, pages 87–102. Springer, 2016.
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016.
[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
[10] Y. Huang, Y. Wang, Y. Tai, X. Liu, P. Shen, S. Li, J. Li, and F. Huang. Curricularface: Adaptive curriculum learning loss for deep face recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 5900–5909. IEEE, 2020.
[11] J. Jiao, W. Liu, Y. M. J. Jiao, Z. Deng, and X. Chen. Dyn-arcface: dynamic additive angular margin loss for deep face recognition. Multim. Tools Appl., 2021.
[12] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1 million faces for recognition at scale. In 2016 IEEE CVPR, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4873–4882. IEEE Computer Society, 2016.
[13] Y. Kim, W. Park, M. Roh, and J. Shin. Groupface: Learning latent groups and constructing group-based representations for face recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 5620–5629. IEEE, 2020.
[14] H. Liu, X. Zhu, Z. Lei, and S. Z. Li. Adaptiveface: Adaptive margin and sampling for face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,

June 16-20, 2019, pages 11947–11956. Computer Vision Foundation / IEEE, 2019. [15] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face recognition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6738–6746. IEEE Computer Society, 2017. [16] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin softmax loss for convolutional neural networks. In M. Balcan and K. Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 507–516. JMLR.org, 2016. [17] B. Maze, J. C. Adams, J. A. Duncan, N. D. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Cheney, and P. Grother. IARPA janus benchmark - C: face dataset and protocol. In 2018 ICB, ICB 2018, Gold Coast, Australia, February 20-23, 2018, pages 158– 165. IEEE, 2018. [18] Q. Meng, S. Zhao, Z. Huang, and F. Zhou. Magface: A universal representation for face recognition and quality assessment. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR Virtual, June 19-25, 2020. IEEE, 2021. [19] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou. Agedb: The ﬁrst manually collected, in-the-wild age database. In 2017 IEEE CVPRW, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1997–2005. IEEE Computer Society, 2017. [20] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024– 8035. Curran Associates, Inc., 2019. [21] P. Z. Peebles. Probability, random variables, and random signal principles. McGraw Hill, 1987. [22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition and clustering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 815–823. IEEE Computer Society, 2015. [23] S. Sengupta, J. Chen, C. D. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs. Frontal to proﬁle face veriﬁcation in the wild. In 2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016, pages 1–9. IEEE Computer Society, 2016. [24] K. Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1849– 1857, 2016. [25] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei. Circle loss: A uniﬁed perspective of pair similarity optimization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 6397–6406. IEEE, 2020. [26] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu. Cosface: Large margin cosine loss for deep face recognition. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 5265–5274. IEEE Computer Society, 2018. [27] C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. C. Adams, T. Miller, N. D. Kalka, A. K. Jain, J. A. Duncan, K. Allen, J. Cheney, and P. Grother. IARPA janus benchmark-b face dataset. In 2017 IEEE CVPRW, CVPR Workshops 2017, Honolulu, HI, USA, July 2126, 2017, pages 592–600. IEEE Computer Society, 2017. [28] X. Wu, R. He, Z. Sun, and T. Tan. A light CNN for deep face representation with noisy labels. IEEE Trans. Inf. Forensics Secur., 13(11):2884–2896, 2018. [29] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representation from scratch. CoRR, abs/1411.7923, 2014. [30] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23(10):1499–1503, 2016. [31] X. Zhang, R. Zhao, Y. Qiao, X. Wang, and H. Li. Adacos: Adaptively

8

scaling cosine logits for effectively learning deep face representations. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10823– 10832. Computer Vision Foundation / IEEE, 2019. [32] T. Zheng and W. Deng. Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments. Technical Report 18-01, Beijing University of Posts and Telecommunications, February 2018. [33] T. Zheng, W. Deng, and J. Hu. Cross-age LFW: A database for studying cross-age face recognition in unconstrained environments. CoRR, abs/1708.08197, 2017.
9

