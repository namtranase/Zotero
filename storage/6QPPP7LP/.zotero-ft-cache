End-to-End Semi-Supervised Object Detection with Soft Teacher

Mengde Xu1‚Ä†*

Zheng Zhang1,2* ‚Ä° Han Hu2‚Ä° Jianfeng Wang2 Lijuan Wang2 Xiang Bai1 Zicheng Liu2
1Huazhong University of Science and Technology
{mdxu,xbai}@hust.edu.cn
2Microsoft
{zhez,hanhu,jianfw,lijuanw,fawe,zliu}@microsoft.com

Fangyun Wei2

arXiv:2106.09018v3 [cs.CV] 6 Aug 2021 COCO Det AP

Abstract
This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn beneÔ¨Åt object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classiÔ¨Åcation loss of each unlabeled bounding box is weighed by the classiÔ¨Åcation score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1%, 5% and 10%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-ofthe-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still signiÔ¨Åcantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pretrained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art. The code and models will be made publicly available at https://github.com/ microsoft/SoftTeacher.
1. Introduction
Data matters. In fact, large data such as ImageNet has largely triggered the boom of deep learning in computer vi-
*Equal contribution. ‚Ä†This work is done when Mengde Xu was intern in MSRA. ‚Ä°Contact person.

50.0 Supervised
45.0 STAC Ours
40.0

44.5 40.9 39.2

35.0

34.0 30.7

30.0

26.9 28.6

24.4

25.0

20.5 20.9

20.0

15.0

14.0

10.0

10.0

5.0

0.0 1%

5%

10%

100%

Proportion of used labeled data (%)

Figure 1. The proposed end-to-end pseudo-label based semisupervised object detection method outperforms the STAC [27] by a large margin on MS-COCO benchmark.

sion. However, obtaining labels can be a bottleneck, due to the time-consuming and expensive annotation process. This has encouraged learning methods to leverage unlabeled data in training deep neural models, such as selfsupervised learning and semi-supervised learning. This paper studies the problem of semi-supervised learning, in particular for object detection.
For semi-supervised object detection, we are concerned with the pseudo-label based approaches, which are the current state-of-the-art. These approaches [27, 36] conduct a multi-stage training schema, with the Ô¨Årst stage training an initial detector using labeled data, followed by a pseudo-labeling process for unlabeled data and a re-training step based on the pseudo labeled unannotated data. These multi-stage approaches achieve reasonably good accuracy, however, the Ô¨Ånal performance is limited by the quality of pseudo labels generated by an initial and probably inaccurate detector trained using a small amount of labeled data.

detector HTC++(Swin-L) w/ single-scale HTC++(Swin-L) w/ multi-scale

method
supervised ours ours‚àó
supervised ours ours‚àó

val2017 mAPdet mAPmask

57.1

49.6

59.1

51.0

60.1

51.9

58.2

50.5

59.9

51.9

60.7

52.5

test-dev2017 mAPdet mAPmask

-

-

-

-

-

-

58.9

51.2

60.4

52.4

61.3

53.0

Table 1. On the state-of-the-art detector HTC++(Swin-L), our method surpasses the supervised learning on both val2017 and test-dev2017. * indicates that models are pre-trained with Object365 [24] dataset.

To address this issue, we present an end-to-end pseudolabel based semi-supervised object detection framework, which simultaneously performs pseudo-labeling for unlabeled images and trains a detector using these pseudo labels along with a few labeled ones at each iteration. SpeciÔ¨Åcally, labeled and unlabeled images are randomly sampled with a preset ratio to form one data batch. Two models are applied on these images, with one conducting detection training and the other in charge of annotating pseudo labels for unlabeled images. The former is also referred to as a student, and the latter is a teacher, which is an exponential moving average (EMA) of the student model. This end-to-end approach avoids the complicated multi-stage training scheme. Moreover, it also enables a ‚ÄúÔ¨Çywheel effect‚Äù that the pseudo labeling and the detection training processes can mutually reinforce each other, so that both get better and better as the training goes on.
Another important beneÔ¨Åt of this end-to-end framework is that it allows for greater leverage of the teacher model to guide the training of the student model, rather than just providing ‚Äúsome generated pseudo boxes with hard category labels‚Äù as in previous approaches [27, 36]. A soft teacher approach is proposed to implement this insight. In this approach, the teacher model is used to directly assess all the box candidates that are generated by the student model, rather than providing ‚Äúpseudo boxes‚Äù to assign category labels and regression vectors to these student-generated box candidates. The direct assessment on these box candidates enables more extensive supervision information to be used in the student model training. SpeciÔ¨Åcally, we Ô¨Årst categorize the box candidates as foreground/background by their detection scores with a high foreground threshold to ensure a high precision of the positive pseudo labels, as in [27]. This high foreground threshold, however, results in many positive box candidates mistakenly assigned as background. To address this issue, we propose using a reliability measure to weight the loss of each ‚Äúbackground‚Äù box candidate. We empirically Ô¨Ånd that a simple detection score produced by the teacher model can well serve as the reliability measure, and is used in our approach. We Ô¨Ånd that this approach measure performs signiÔ¨Åcantly better than previous hard fore-

ground/background assignment methods (see Table 3 and Table 4), and we name it ‚Äúsoft teacher‚Äù.
Another approach instantiates this insight is to select reliable bounding boxes for the training of the student‚Äôs localization branch, by a box jittering approach. This approach Ô¨Årst jitters a pseudo-foreground box candidate several times. Then these jittered boxes are regressed according the teacher model‚Äôs location branch, and the variance of these regressed boxes is used as a reliability measure. The box candidate with adequately high reliability will be used for the training of the student‚Äôs localization branch.
On MS-COCO object detection benchmark [16], our approach achieves 20.5 mAP, 30.7 mAP and 34.0 mAP on val2017 with 1%, 5% and 10% labeled data using the Faster R-CNN [22] framework with ResNet-50 [8] and FPN [14], surpassing previous best method STAC [27] by +6.5, +6.4 and +5.4 mAP, respectively.
In addition, we also perform evaluation on a more challenge setting where the labelled data has been adequately large to train a reasonably accurate object detector. SpeciÔ¨Åcally, we adopt the complete COCO train2017 set as labeled data and the unlabeled2017 set as the unlabeled data. Under this setting, we improve the supervised baseline of a Faster R-CNN approach with ResNet-50 and ResNet101 backbones by +3.6 mAP and +3.0 mAP, respectively.
Moreover, on a state-of-the-art Swin-Transformer [18] based detector which achieves 58.9 mAP for object detection and 51.2 mAP for instance segmentation on COCO test-dev2017, the proposed approach can still improve the accuracy by +1.5 mAP and +1.2 mAP, respectively, reaching 60.4 mAP and 52.4 mAP. Further incorporating with the Object365 [24] pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, which is the new state-of-the-art on this benchmark.
2. Related works
Semi-supervised learning in image classiÔ¨Åcation Semisupervised learning in image classiÔ¨Åcation can be roughly categorized into two groups: consistency based and pseudolabel based. The consistency based methods [1, 23, 19, 11]

Unlabeled Data

Weak Aug
Strong Aug

Soft Teacher
EMA Update

Box Regression Variance Filter

Score Filter

Human Human Human Human

Prediction After NMS

Pseudo Boxes for Cls

Labeled Data

Student

ùêøùë† + ùêøùëüùë¢ùëíùëî + ùêøùëêùë¢ùëôùë†

Pseudo Boxes for Reg

Figure 2. The overview of the end-to-end pseudo-labeling framework for semi-supervised object detection. Unlabeled images and labeled images form the training data batch. In each training iteration, a soft teacher is applied to perform pseudo-labeling on weak augmented unlabeled images on the Ô¨Çy. Two sets of pseudo boxes are produced: one is used for classiÔ¨Åcation branch by Ô¨Åltering boxes according to the foreground score, and the other is used for box regression branch by Ô¨Åltering boxes according to box regression variance. The teacher model is updated by student model via exponential mean average (EMA) manner. The Ô¨Ånal loss is the sum of supervised detection loss Ls and unsupervised detection loss Lu.

leverage the unlabeled images to construct a regularization loss which encourages different perturbations of a same image to produce similar predictions. There are several ways to implement perturbations, including perturbing the model [1], augmenting the images [23] or adversarial training [19]. In [11], the training target is assembled by predicting different training steps. In [29], they develop [11] by ensembling the model itself instead of the model prediction, the so-called exponential mean average (EMA) of the student model. The pseudo-label approaches [33, 7, 12] (also named as self-training) annotate unlabeled images with pseudo labels by an initially trained classiÔ¨Åcation model, and the detector is reÔ¨Åned by these pseudo labeled images. Unlike our method which focuses on object detection, the pseudo-label does not have to solve the problem of assigning foreground/background labels and box regression when classifying images. Recently, some works [32, 3, 2, 26] explore the importance of data augmentation in semi-supervised learning, which inspire us to use the weak augmentation to generate pseudo-labels and the strong augmentation for the learning of detection models.
Semi-supervised learning in object detection Similar to the semi-supervised learning in image classiÔ¨Åcation, semisupervised object detection methods also have two categories: the consistency methods [10, 28] and pseudo-label methods [20, 36, 13, 27, 31]. Our method belongs to the pseudo-label category. In [20, 36], the predictions of different data augmentation are ensembled to form the pseudo labels for unlabeled images. In [13], a SelectiveNet is trained to select the pseudo-label. In [31], a box detected on an unlabeled image is pasted onto a labeled image, and

the localization consistency estimation is performed onto the pasted label image. As the image itself is modiÔ¨Åed, a very thorough detection process is required in [31]. In our method, only the lightweight detection head is processed. STAC [27] proposes to use a weak data augmentation for model training and a strong data augmentation is used for performing pseudo-label. However, like other pseudo-label methods [20, 36, 13, 27, 31], it also follows the multi-stage training scheme. In contrast, our method is an end-to-end pseudo-labeling framework, which avoids the complicated training process and also achieves better performance.
Object Detection Object detection focuses on designing efÔ¨Åcient and accurate detection framework. There are two mainstreams: single-stage object detectors [17, 21, 30] and two-stage object detectors [6, 22, 14, 34, 35]. The main difference between the two types of methods is whether to use a cascade to Ô¨Ålter a large number of object candidates (proposals). In theory, our method is compatible with both types of methods. However, to allow a fair comparison with previous works [28, 27] on semi-supervised object detection, we use Faster R-CNN [22] as our default detection framework to illustrate our method.
3. Methodology
Figure. 2 illustrates an overview of our end-to-end training framework. There are two models, a student model and a teacher model. The student model is learned by both the detection losses on the labeled images and on the unlabeled images using pseudo boxes. The unlabeled images have two sets of pseudo boxes, which are used to drive the training of the classiÔ¨Åcation branch and the regression branch, respec-

tively. The teacher model is an exponential moving average (EMA) of the student model. Within this end-to-end framework, there are two crucial designs: soft teacher and box jittering.
3.1. End-to-End Pseudo-Labeling Framework
We Ô¨Årst introduce the end-to-end framework for pseudolabel based semi-supervised object detection. Our approach follows the teacher-student training scheme. In each training iteration, labeled images and unlabeled images are randomly sampled according to a data sampling ratio sr to form a training data batch. The teacher model is performed to generate the pseudo boxes on unlabeled images and the student model is trained on both labeled images with the ground-truth and unlabeled images with the pseudo boxes as the ground-truth. Thus, the overall loss is deÔ¨Åned as the weighted sum of supervised loss and unsupervised loss:

L = Ls + Œ±Lu,

(1)

where Ls and Lu denote supervised loss of labeled images and unsupervised loss of unlabeled images respectively, Œ± controls contribution of unsupervised loss. Both of them are normalized by the respective number of images in the training data batch:

Ls

=

1 Nl

Nl
(Lcls(Ili) + Lreg(Ili)),
i=1

(2)

Lu

=

1 Nu

Nu
(Lcls(Iui ) + Lreg(Iui )),
i=1

(3)

where Ili indicates the i-th labeled image, Iui indicates the i-th unlabeled image, Lcls is the classiÔ¨Åcation loss, Lreg is the box regression loss, Nl and Nu denote the number of labeled images and unlabeled images, respectively.
At the beginning of training, both the teacher model and student model are randomly initialized. As the training progresses, the teacher model is continuously updated by the student model, and we follow the common practices [29, 26] that the teacher model is updated by exponential moving average (EMA) strategy.
In contrast to taking a simple probability distribution as the pseudo-label in image classiÔ¨Åcation, creating pseudolabel for object detection is more complicated since an image usually contains multiple objects and the annotation of objects consists of location and category. Given an unlabeled image, the teacher model is used to detect objects and thousands of box candidates are predicted. The nonmaximum suppression (NMS) is then performed to eliminate redundancy. Although most redundant boxes are removed, there are still some non-foreground candidates

left. Therefore, only candidates with the foreground score1 higher than a threshold are retained as the pseudo boxes.
In order to generate high-quality pseudo boxes and to facilitate the training of the student model, we draw on FixMatch [26] which is the latest advancement in semisupervised image classiÔ¨Åcation task. Strong augmentation is applied for detection training of the student model and weak augmentation is used for pseudo-labeling of the teacher model.
In theory, our framework is applicable to mainstream object detectors, including single-stage object detectors [15, 17, 21, 30] and two-stage object detectors [22, 9, 5, 35, 34]. To allow a fair comparison with previous methods, we use Faster R-CNN [22] as our default detection framework to illustrate our method.
3.2. Soft Teacher
The performance of the detector depends on the quality of the pseudo-label. In practice, we Ô¨Ånd that using a higher threshold on foreground score to Ô¨Ålter out most of the student-generated box candidates with low-conÔ¨Ådence can achieve better results than using a lower threshold. As shown in Table. 9, the best performance is achieved when the threshold is set to 0.9. However, while the strict criteria (higher threshold) leads to higher foreground precision, the recall of the retained box candidates also falls off quickly. As shown in Figure. 3 (a), when the foreground threshold is set to 0.9, the recall is low, as 33%, while the precision reaches 89%. In this case, if we use IoU between student generated box candidates and teachergenerated pseudo boxes to assign foreground and background labels, as a general object detection framework does when real box annotations are provided, some foreground box candidates will be mistakenly assigned as negatives, which may hinder the training and harm the performance.
To alleviate this issue, we propose a soft teacher approach which leverages richer information from the teacher model, thanks to the Ô¨Çexibility of the end-to-end framework. SpeciÔ¨Åcally, we assess the reliability of each studentgenerated box candidate to be a real background, which is then used to weigh its background classiÔ¨Åcation loss. Given two box sets {bfig} and {bbig}, with {bfig} denoting boxes assigned as foreground and {bbig} denoting the boxes assigned as background, the classiÔ¨Åcation loss of an unlabeled image with the reliable weighting is deÔ¨Åned as:

Lculs

=

1 Nbfg

Nbfg

Nbbg

lcls(bfig, Gcls) +

wj lcls(bbjg, Gcls),

i=1

j=1

(4)

1The foreground score is deÔ¨Åned as the maximum probability of all non-background categories.

Figure 3. We randomly sampled 10k unlabeled training images from train2017 to draw Ô¨Ågures based on the model trained with 10% labeled images. (a) precision and recall of foreground under different foreground score thresholds. (b) the correlation between the IoU with ground-truth and box foreground score. (c) the correlation between the IoU with ground-truth and box regression variance. Each point in (b) and (c) represents a box candidate.

wj =

rj
Nbbg k=1

rk

,

(5)

where Gcls denotes the set of (teacher-generated) pseudo
boxes used for classiÔ¨Åcation, lcls is the box classiÔ¨Åcation
loss, rj is the reliability score for j-th background box candidate, Nbfg and Nbbg are the number of box candidates of the box set {bfig} and {bbig}, respectively.
Estimating the reliability score r is challenging. We

Ô¨Ånd empirically that the background score produced by the

teacher model with weak augmented image can well serve as a proxy indicator of r and is easily obtained in our end-

to-end training framework. SpeciÔ¨Åcally, given a student-

generated box candidate, its background score can be ob-

tained simply by using the teacher (BG-T) to process the

box through its detection head. It is worth noting that this

approach, unlike the widely used hard negative mining ap-

proaches, e.g., OHEM [25] or Focal Loss [15], is more like

a ‚Äúsimple‚Äù negative mining. For comparison, we also ex-

amine several other indicators:

‚Ä¢ Background score of student model (BG-S): Another natural way to generate the background score is to use the prediction of student model directly.

‚Ä¢ Prediction difference (Pred-Diff): The prediction difference between the student model and teacher model is also a possible indicator. In our approach, we simply use the difference between the background scores of the two models to deÔ¨Åne the reliability score:

r = 1 ‚àí |pbSg(b) ‚àí pbTg(b)|,

(6)

where pbSg and pbTg are the predicted probability of the background class of the student and the teacher model,
respectively.

‚Ä¢ Intersection-over-Union: The IoU between groundtruths and box candidates is a commonly used criterion

for foreground/background assignment. There are two different hypotheses about how to use IoU to measure whether a box candidate belongs to the background. In the Ô¨Årst hypothesis, if the IoU between a box candidate and a ground-truth box is less than a threshold (e.g., 0.5), a larger IoU indicates the box candidate has greater probability of being background. This can be viewed as an IoU-based hard negative mining which is adopted by Fast R-CNN [6] and Faster RCNN [22] in the early implementation. In contrast, the other hypothesis suggests that box candidates with a smaller IoU with ground-truths are more likely to be backgrounds. In our experiments, we validate both hypotheses and name them as IoU and Reverse-IoU.

3.3. Box Jittering

As shown in Figure. 3 (b), the localization accuracy and the foreground score of the box candidates do not show a strong positive correlation, which means that the boxes with high foreground score may not provide accurate localization information. This indicates that the selection of the teachergenerated pseudo boxes according to the foreground score is not suitable for box regression, and a better criterion is needed.
We introduce an intuitive approach to estimate the localization reliability of a candidate pseudo box by measuring the consistency of its regression prediction. SpeciÔ¨Åcally, given a teacher-generated pseudo box candidate bi, we sample a jittered box around bi and feed the jittered box into the teacher model to obtain the reÔ¨Åned box ÀÜbi, which is formulated as follows:

ÀÜbi = reÔ¨Åne(jitter(bi)).

(7)

The above procedure is repeated several times to collect a set of Njitter reÔ¨Åned jittered boxes {ÀÜbi,j}, and we deÔ¨Åne the
localization reliability as the box regression variance:

14

œÉ¬Øi = 4 œÉÀÜk,

(8)

k=1

œÉÀÜk

=

œÉk 0.5(h(bi) +

, w(bi))

(9)

where œÉk is the standard derivation of the k-th coordinate of the reÔ¨Åned jittered boxes set {ÀÜbi,j}, œÉÀÜk is the normalized œÉk, h(bi) and w(bi) represent the height and width of box candidate bi, respectively.
A smaller box regression variance indicates a higher localization reliability. However, computing the box regression variances of all pseudo box candidates is unbearable

during training. Therefore, in practice, we only calculate the reliability for the boxes with a foreground score greater than 0.5. In this way, the number of boxes that need to be

estimated is reduced from an average of hundreds to around 17 per image and thus the computation cost is almost negligible.
In Figure. 3 (c), we illustrate the correlation between

the localization accuracy and our box regression variance. Compared with the foreground score, the box regression variance can better measure the localization accuracy. This

motivates us to select box candidates whose box regression variance is smaller than a threshold as pseudo-label to train the box regression branch on unlabeled images. Given the pseudo boxes Greg for training the box regression on unlabeled data, the regression loss is formulated as:

Lrueg

=

1 Nbfg

Nbfg i=1

lreg(bfig, Greg),

(10)

where bfig is i-th box assigned as foreground, Nbfg is the total number of foreground box, lreg is the box regression loss. Therefore, by substituting Equ. 4 and Equ. 10 into Equ. 3,
the loss of unlabeled images is:

Lu

=

1 Nu

Nu
(Lculs(Iui , Gcils) + Lrueg(Iui , Grieg)).
i=1

(11)

Here we use the pseudo boxes Gcls and Greg as the inputs of the loss to highlight the fact that the pseudo boxes used in classiÔ¨Åcation and box regression are different in our approach.

4. Experiments
4.1. Dataset and Evaluation Protocol
We validate our method on the MS-COCO benchmark [16]. Two training datasets are provided, the train2017 set contains 118k labeled images and the unlabeled2017 set contains 123k unlabeled images. In addition, the val2017 set with 5k images is also provided for validation. In previous methods [27, 28, 10], there are two settings for validating the performance: Partially Labeled Data: STAC [27] Ô¨Årst introduced this setting. 1%, 5% and 10% images of train2017 set are

sampled as the labeled training data, and the remaining unsampled images of train2017 are used as the unlabeled data. For each protocol, STAC provides 5 different data folds and the Ô¨Ånal performance is the average of all 5 folds. Fully Labeled Data: In this setting, the entire train2017 is used as the labeled data and unlabeled2017 is used as the additional unlabeled data. This setting is more challenging. Its goal is to use the additional unlabeled data to improve a well-trained detector on large-scale labeled data.
We evaluate our method on both settings and follow the convention to report the performance on val2017 with the standard mean average precision (mAP) as the evaluation metrics.
4.2. Implementation Details
We use the Faster R-CNN [22] equipped with FPN [14] (Feature Pyramid Network) as our default detection framework to evaluate the effectiveness of our method, and an ImageNet pre-trained ResNet-50 [8] is adopted as the backbone. Our implementation and hyper-parameters are based on MMDetection [4]. Anchors with 5 scales and 3 aspect ratios are used. 2k and 1k region proposals are generated with a non-maximum suppression threshold of 0.7 for training and inference. In each training step, 512 proposals are sampled from 2k proposals as the box candidates to train RCNN. Since the amount of training data of Partially Labeled Data setting and Full Labeled Data setting has large differences, the training parameters under the two settings are slightly different. Partially Labeled Data: The model is trained for 180k iterations on 8 GPUs with 5 image per GPU. With SGD training, the learning rate is initialized to 0.01 and is divided by 10 at 110k iteration and 160k iteration. The weight decay and the momentum are set to 0.0001 and 0.9, respectively. The foreground threshold is set to 0.9 and the data sampling ratio sr is set to 0.2 and gradually decreases to 0 over the last 10k iterations. Fully Labeled Data: The model is trained for 720k iterations on 8 GPUs with 8 image per GPU. In SGD training, the learning rate is initialized to 0.01 and is divided by 10 at 480k iteration and 680k iteration. The weight decay and the momentum are set to 0.0001 and 0.9, respectively. The foreground threshold is set to 0.9 and the data sampling ratio sr is set to 0.5 and gradually decreases to 0 in the last 20k iterations.
For estimating the box localization reliability, we set Njitter as 10, and threshold is set as 0.02 to select the pseudolabels for box regression. The jittered boxes are randomly sampled by adding the offsets on four coordinates, and the offsets are uniformly sampled from [-6%, 6%] of the height or width of the pseudo box candidates. In addition, we follow STAC and FixMatch to use different data augmentation

Augmentation Scale jitter
Solarize jitter Brightness jitter Constrast jitter Sharpness jitter
Translation Rotate Shift Cutout

Labeled image training short edge ‚àà (0.5, 1.5) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1)
num ‚àà (1, 5), ratio ‚àà (0.05, 0.2)

Unlabeled image training
short edge ‚àà (0.5, 1.5) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1) p=0.25, ratio ‚àà (0, 1) p=0.3, translation ratio ‚àà (0, 0.1) p=0.3, angle ‚àà (0, 30‚ó¶) p=0.3, angle ‚àà (0, 30‚ó¶) num ‚àà (1, 5), ratio ‚àà (0.05, 0.2)

Pseudo-label generation short edge ‚àà (0.5, 1.5)
-

Table 2. The summary of the data augmentation used in our approach. We follow the practice of STAC [27] and FixMatch [26] to provide different data augmentation for pseudo-label generation, labeled image training and unlabeled image training. ‚Äú-‚Äù indicates the augmentation is not used.

Method Supervised baseline (Ours) Supervised baseline (STAC) [27]
STAC [27] Ours

1% 10.0 ¬± 0.26 9.83 ¬± 0.23 13.97 ¬± 0.35 20.46¬±0.39

5% 20.92 ¬± 0.15 21.18 ¬± 0.20 24.38 ¬± 0.12 30.74¬±0.08

10% 26.94 ¬± 0.111 26.18 ¬± 0.12 28.64 ¬± 0.21 34.04¬±0.14

Table 3. System level comparison with STAC on val2017 under the Partially Labeled Data setting. All the results are the average of all 5 folds. For benchmarking, we also compare the supervised benchmark performance between our method and STAC, and their performance is similar.

Method Proposal learning [28]
STAC [27] Self-training [36]
Ours

Extra dataset unlabeled2017 unlabeled2017 ImageNet+OpenImages unlabeled2017

mAP 37.4 ‚àí+‚àí1‚Üí.0 38.4 39.5 ‚àí-‚àí0‚Üí.3 39.2 41.1 ‚àí+‚àí0‚Üí.8 41.9 40.9 ‚àí+‚àí3‚Üí.6 44.5

Table 4. Comparison with other state-of-the-arts under the setting of using all data of train2017 set. Particularly, Self-training uses ImageNet (1.2M images) and OpenImages (1.7M images) as additional unlabeled images, which is 20√ó larger than unlabeled2017 (123k images).

for pseudo-label generation, labeled image training and unlabeled image training. The details are summarized in Table .2.
4.3. System Comparison
In this section, we compare our method with previous state-of-the-arts on MS-COCO. We Ô¨Årst evaluate on the Partially Labeled Data setting and compare our method with STAC. For benchmarking, we compare the supervised baseline of our method with the results reported in STAC and Ô¨Ånd they perform similarly, the results are shown in Table. 3. In this case, we further compare our method with STAC at the system level, and our method shows a signiÔ¨Åcant performance improvement in different protocols. SpeciÔ¨Åcally, our method outperforms the STAC by 6.5 points, 6.4 points and 5.4 points when there are 1%, 5%, and 10% labeled data, respectively. The qualitative results of our method compared with supervised baseline are

Proportion of used labeled data

10%

5%

1%

(a)

(b)

(c)

(d)

Figure 4. The qualitative results of our method. (a), (c) are the

results of the supervised baseline. (b), (d) are the results of our

method.

shown in Figure. 4.

detector Faster R-CNN Faster R-CNN
HTC++ HTC++(multi-scale)

backbone ResNet-50 ResNet-101
Swin-L Swin-L

method supervised
ours supervised
ours supervised
ours supervised
ours

mAPdet 40.9
44.5(+3.6) 43.8
46.8(+3.0) 57.1
59.1(+2.0) 58.2
59.9(+1.7)

mAPmask -
49.6 51.0(+1.4)
50.5 51.9(+1.4)

Table 5. Compared with various supervised trained detectors on val2017. The entire train2017 is used as the labeled images, and the unlabeled2017 is used as the additional unlabeled images.

Method Supervised Multi-Stage
E2E E2E+EMA

mAP 27.1 28.7 30.0 31.2

mAP@0.5 44.6 47.0 47.4 48.8

mAP@0.75 28.6 30.9 32.4 34.0

Table 6. Multi-Stage vs. End-to-End. The end-to-end (E2E) method outperforms the multi-stage framework. Updating the teacher network through the exponential moving average (EMA) strategy further improves the performance.

Then we compare our method with other state-of-theart methods in Fully Labeled Data setting. Since the reported performance of supervised baseline varies in different works, we report the results of the comparison methods and their baseline at the same time. The results are shown in Table. 4.
We Ô¨Årst compare with the Proposal Learning [28] and STAC [27] which also use unlabeled2017 as additional unlabeled data. Because of the better hyper-parameters and more adequate training, our supervised baseline achieved better performance than other methods. Under the stronger baseline, our method still shows a greater performance gain (+3.6 points) than Proposal Learning (+1.0 points) and STAC (-0.3 points). Self-training [36] uses ImageNet (1.2M images) and OpenImages (1.7M images) as the additional unlabeled data, which is 20√ó larger than the unlabeled2017 (123k images) that we use. With similar baseline performance, our method also shows better result with less unlabeled data.
In addition, we further evaluate our method on other stronger detectors, and the results evaluated on val2017 set are shown in Table. 5. Our method consistently improves the performance of different detectors by a notable margin. Even in the state-of-the-art detector HTC++ with Swin-L backbone, we still show 1.8 improvement on detection AP and 1.4 improvement on mask AP. Moreover, we also report the results on test-dev2017 set. As shown in Tabel. 1, our method improves the HTC++ with Swin-L backbone by 1.5 mAP on detection, which is the Ô¨Årst work

to surpass 60 mAP on COCO object detection benchmark.
4.4. Ablation Studies
In this section, we validate our key designs. If not speciÔ¨Åed, all the ablation experiments are performed on the single data fold provided by [27] with 10% labeled images from train2017 set.
Multi-Stage vs. End-to-End. We compare our end-toend method with the multi-stage framework as shown in Table 6. By simply switching from the multi-stage framework to our end-to-end framework, performance is increased by 1.3 points. By updating the teacher model with the student model through the exponential moving average (EMA) strategy, our method further achieves 31.2 mAP.
Effects of Soft Teacher and Box Jittering. We ablate the effects of soft teacher and box jittering. The results are shown in Table. 7. Based on our end-to-end model equipped with EMA (E2E+EMA), integrating the soft teacher improves the performance by 2.4 points. Further applying the box jittering, the performance reaches 34.2 mAP, which is 3 points better than E2E+EMA.
Different Indicators in Soft Teacher. In Section. 3.2, several different indicators are explored for reliability estimation. Here, we evaluate the different indicators and the results are shown in Table. 8. The background score predicted by the teacher model achieves the best performance. Simply switching the model from teacher to student will make the performance worse. In addition, the improvement of IoU and Revearse-IoU is negligible compared with BG-T. These results prove the necessity of leveraging the teacher model.
Effects of other hyper-parameters. We study the effects of hyper-parameters used in our method. Table. 9 studies the effects of different foreground score thresholds. The best performance is achieved when the threshold is set to 0.9, and lower or higher thresholds will cause signiÔ¨Åcant

Soft teacher Box jittering mAP mAP@0.5 31.2 48.8 33.6 52.9 34.2 52.6

mAP@0.75 34.0 36.6 37.3

Table 7. We study the effects of soft teacher and box jittering techniques.

Indicator w/o weight
IoU Reverse-IoU
Pred-Diff BG-S BG-T

mAP 31.2 31.7 31.6 32.3 25.9 33.6

mAP@0.5 48.8 51.4 49.5 51.0 44.4 52.9

mAP@0.75 34.0 34.2 34.1 34.6 27.0 36.6

Table 8. Comparison of different indicators in soft teacher.

Threshold 0.70 0.80 0.90 0.95

mAP 29.9 33.2 33.6 32.1

mAP@0.5 48.6 52.8 52.9 50.6

mAP@0.75 32.1 35.9 36.6 34.7

Table 9. Ablation study on the effects of different foreground thresholds.

Threshold 0.04 0.03 0.02 0.01

mAP 33.8 34.0 34.2 32.9

mAP@0.5 52.3 52.5 52.6 52.2

mAP@0.75 36.7 36.9 37.3 35.8

Table 10. Ablation study on the effects of different thresholds for selecting pseudo boxes for box regression according to box regression variance.

Njitter 5 10 15

mAP 34.0 34.2 34.2

mAP@0.5 52.3 52.6 52.5

mAP@0.75 37.0 37.3 37.4

Table 11. Ablation study on the effects of different number of jittered boxes used to estimate the box regression variance.

performance degradation. In Table. 10, we study the box regression variance threshold. The best performance is achieved when the threshold is set to 0.02. In Table. 11, we study the effects of different number of jittered boxes, and the performance is saturated when Njitter is set to 10.
5. Conclusion
In this paper, we proposed an end-to-end training framework for semi-supervised object detection, which discards the complicated multi-stage schema adopted by previous

approaches. Our method simultaneously improves the detector and pseudo labels by leveraging a student model for detection training, and a teacher model which is continuously updated by the student model through the exponential moving average strategy for online pseudo-labeling. Within the end-to-end training, we present two simple techniques named soft teacher and box jittering to facilitate the efÔ¨Åcient leverage of the teacher model. The proposed framework outperforms the state-of-the-art methods by a large margin on MS-COCO benchmark in both partially labeled data and fully labeled data settings.

6. Acknowledgement
We would like to thank Yue Cao for his valuable suggestions and discussions; Yutong Lin and Yixuan Wei for help on Swin Transformer experiments.

References

[1] Philip Bachman, Ouais Alsharif, and Doina Precup.

Learning with pseudo-ensembles.

arXiv preprint

arXiv:1412.4864, 2014. 2, 3

[2] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex

Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.

Remixmatch: Semi-supervised learning with distribution

alignment and augmentation anchoring. arXiv preprint

arXiv:1911.09785, 2019. 3

[3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas

Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A

holistic approach to semi-supervised learning. arXiv preprint

arXiv:1905.02249, 2019. 3

[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu

Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,

Jiarui Xu, et al. Mmdetection: Open mmlab detection tool-

box and benchmark. arXiv preprint arXiv:1906.07155, 2019.

6

[5] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen

Lin, and Han Hu. Reppoints v2: VeriÔ¨Åcation meets regres-

sion for object detection. NIPS, 2020. 4

[6] Ross Girshick. Fast r-cnn. In ICCV, 2015. 3, 5

[7] Yves Grandvalet, Yoshua Bengio, et al. Semi-supervised

learning by entropy minimization. In CAP, 2005. 3

[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR,

2016. 2, 6

[9] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen

Wei. Relation networks for object detection. In CVPR, 2018.

4

[10] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object detection. NIPS, 2019. 3, 6
[11] Samuli Laine and Timo Aila. Temporal ensembling for semisupervised learning. ICLR, 2016. 2, 3
[12] Dong-Hyun Lee et al. Pseudo-label: The simple and efÔ¨Åcient semi-supervised learning method for deep neural networks. In Workshop of ICML, 2013. 3
[13] Yandong Li, Di Huang, Danfeng Qin, Liqiang Wang, and Boqing Gong. Improving object detection with selective selfsupervised self-training. In ECCV, 2020. 3
[14] Tsung-Yi Lin, Piotr Dolla¬¥r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 2, 3, 6
[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dolla¬¥r. Focal loss for dense object detection. In ICCV, 2017. 4, 5
[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla¬¥r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 2, 6
[17] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 3, 4
[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021. 2
[19] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. TPAMI, 2018. 2, 3
[20] Ilija Radosavovic, Piotr Dolla¬¥r, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation: Towards omnisupervised learning. In CVPR, 2018. 3
[21] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: UniÔ¨Åed, real-time object detection. In CVPR, 2016. 3, 4
[22] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015. 2, 3, 4, 5, 6
[23] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. arXiv preprint arXiv:1606.04586, 2016. 2, 3
[24] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In CVPR, 2019. 2
[25] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016. 5
[26] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semisupervised learning with consistency and conÔ¨Ådence. NIPS, 2020. 3, 4, 7

[27] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas PÔ¨Åster. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020. 1, 2, 3, 6, 7, 8
[28] Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caiming Xiong. Proposal learning for semi-supervised object detection. In WACV, 2021. 3, 6, 7, 8
[29] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. NIPS, 2017. 3, 4
[30] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In ICCV, 2019. 3, 4
[31] Keze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, and Liang Lin. Towards human-machine cooperation: Selfsupervised sample mining for object detection. In CVPR, 2018. 3
[32] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consistency training. NIPS, 2020. 3
[33] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classiÔ¨Åcation. In CVPR, 2020. 3
[34] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. In ICCV, 2019. 3, 4
[35] Ze Yang, Yinghao Xu, Han Xue, Zheng Zhang, Raquel Urtasun, Liwei Wang, Stephen Lin, and Han Hu. Dense reppoints: Representing visual objects with dense point sets. ECCV, 2019. 3, 4
[36] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training and self-training. NIPS, 2020. 1, 2, 3, 7, 8

