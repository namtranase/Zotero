IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

1

Deep Learning for Person Re-identiÔ¨Åcation: A Survey and Outlook

Mang Ye, Jianbing Shen, Senior Member, IEEE, Gaojie Lin, Tao Xiang Ling Shao and Steven C. H. Hoi, Fellow, IEEE

arXiv:2001.04193v2 [cs.CV] 6 Jan 2021

Abstract‚ÄîPerson re-identiÔ¨Åcation (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained signiÔ¨Åcantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We Ô¨Årst conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under speciÔ¨Åc scenarios. We summarize the open-world Re-ID in terms of Ô¨Åve different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for Ô¨Ånding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.
Index Terms‚ÄîPerson Re-IdentiÔ¨Åcation, Pedestrian Retrieval, Literature Survey, Evaluation Metric, Deep Learning
!

1 INTRODUCTION

P ERSON re-identiÔ¨Åcation (Re-ID) has been widely studied as a speciÔ¨Åc person retrieval problem across non-overlapping cameras [1], [2]. Given a query person-of-interest, the goal of Re-ID is to determine whether this person has appeared in another place at a distinct time captured by a different camera, or even the same camera at a different time instant [3]. The query person can be represented by an image [4], [5], [6], a video sequence [7], [8], and even a text description [9], [10]. Due to the urgent demand of public safety and increasing number of surveillance cameras, person Re-ID is imperative in intelligent surveillance systems with signiÔ¨Åcant research impact and practical importance. Re-ID is a challenging task due to the presence of different viewpoints [11], [12], varying low-image resolutions [13], [14], illumination changes [15], unconstrained poses [16], [17], [18], occlusions [19], [20], heterogeneous modalities [10], [21], complex camera environments, background clutter [22], unreliable bounding box generations, etc. These result in varying variations and uncertainty. In addition, for practical model deployment, the dynamic updated camera network [23], [24], large scale gallery with efÔ¨Åcient retrieval
‚Ä¢ M. Ye is with the School of Computer Science, Wuhan University, China and Inception Institute of ArtiÔ¨Åcial Intelligence, UAE.
‚Ä¢ J. Shen and L. Shao are with the Inception Institute of ArtiÔ¨Åcial Intelligence, UAE. E-mail:{mangye16, shenjianbingcg}@gmail.com
‚Ä¢ G. Lin is with the School of Computer Science, Beijing Institute of Technology, China.
‚Ä¢ T. Xiang is with the Centre for Vision Speech and Signal Processing, University of Surrey, UK. Email: t.xiang@surrey.ac.uk
‚Ä¢ S. C. H. Hoi is with the Singapore Management University, and Salesforce Research Asia, Singapore. Email: stevenhoi@gmail.com

[25], group uncertainty [26], signiÔ¨Åcant domain shift [27], unseen testing scenarios [28], incremental model updating [29] and changing cloths [30] also greatly increase the difÔ¨Åculties. These challenges lead that Re-ID is still unsolved problem. Early research efforts mainly focus on the handcrafted feature construction with body structures [31], [32], [33], [34], [35] or distance metric learning [36], [37], [38], [39], [40], [41]. With the advancement of deep learning, person Re-ID has achieved inspiring performance on the widely used benchmarks [5], [42], [43], [44]. However, there is still a large gap between the research-oriented scenarios and practical applications [45]. This motivates us to conduct a comprehensive survey, develop a powerful baseline for different Re-ID tasks and discuss several future directions.
Though some surveys have also summarized the deep learning techniques [2], [46], [47], our survey makes three major differences: 1) We provide an in-depth and comprehensive analysis of existing deep learning methods by discussing their advantages and limitations, analyzing the state-of-the-arts. This provides insights for future algorithm design and new topic exploration. 2) We design a new powerful baseline (AGW: Attention Generalized mean pooling with Weighted triplet loss) and a new evaluation metric (mINP: mean Inverse Negative Penalty) for future developments. AGW achieves state-of-the-art performance on twelve datasets for four different Re-ID tasks. mINP provides a supplement metric to existing CMC/mAP, indicating the cost to Ô¨Ånd all the correct matches. 3) We make an attempt to discuss several important research directions with under-investigated open issues to narrow the gap between the closed-world and open-world applications, taking a step towards real-world Re-ID system design.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

2

Query

Search

Re-ID Model

‚Ä¶
Gallery

Fig. 1: 1T.hReawÔ¨ÇoDwataoCf odlelescitgionning a prac2t.icBaolupndeirnsgoBnoRx eG-eIDnersaytisotnem, inc3l.uTdrianingDÔ¨ÅavtaeAmnnaointatsiotenps:41. )TrRaainwingDMatoadCelollec5ti.oTne,st(i2n)gBounding Box Generation, 3) Training Data Annotation, 4) Model Training and 5) Pedestrian Retrieval.

TABLE 1: Closed-world vs. Open-world Person Re-ID.

Closed-world (Section 2) Single-modality Data Bounding Boxes Generation SufÔ¨Åcient Annotated Data Correct Annotation Query Exists in Gallery

Open-world (Section 3) Heterogeneous Data (¬ß 3.1) Raw Images/Videos (¬ß 3.2) Unavailable/Limited Labels (¬ß 3.3) Noisy Annotation (¬ß 3.4) Open-set (¬ß 3.5)

Unless otherwise speciÔ¨Åed, person Re-ID in this survey refers to the pedestrian retrieval problem across multiple surveillance cameras, from a computer vision perspective. Generally, building a person Re-ID system for a speciÔ¨Åc scenario requires Ô¨Åve main steps (as shown in Fig. 1):
1) Step 1: Raw Data Collection: Obtaining raw video data from surveillance cameras is the primary requirement of practical video investigation. These cameras are usually located in different places under varying environments [48]. Most likely, this raw data contains a large amount of complex and noisy background clutter.
2) Step 2: Bounding Box Generation: Extracting the bounding boxes which contain the person images from the raw video data. Generally, it is impossible to manually crop all the person images in large-scale applications. The bounding boxes are usually obtained by the person detection [49], [50] or tracking algorithms [51], [52].
3) Step 3: Training Data Annotation: Annotating the crosscamera labels. Training data annotation is usually indispensable for discriminative Re-ID model learning due to the large cross-camera variations. In the existence of large domain shift [53], we often need to annotate the training data in every new scenario.
4) Step 4: Model Training: Training a discriminative and robust Re-ID model with the previous annotated person images/videos. This step is the core for developing a Re-ID system and it is also the most widely studied paradigm in the literature. Extensive models have been developed to handle the various challenges, concentrating on feature representation learning [54], [55], distance metric learning [56], [57] or their combinations.
5) Step 5: Pedestrian Retrieval: The testing phase conducts the pedestrian retrieval. Given a person-of-interest (query) and a gallery set, we extract the feature representations using the Re-ID model learned in previous stage. A retrieved ranking list is obtained by sorting the calculated query-to-gallery similarity. Some methods have also investigated the ranking optimization to improve the retrieval performance [58], [59].
According to the Ô¨Åve steps mentioned above, we categorize existing Re-ID methods into two main trends: closedworld and open-world settings, as summarized in Table 1. A step-by-step comparison is in the following Ô¨Åve aspects:

1) Single-modality vs. Heterogeneous Data: For the raw data collection in Step 1, all the persons are represented by images/videos captured by single-modality visible cameras in the closed-world setting [5], [8], [31], [42], [43], [44]. However, in practical open-world applications, we might also need to process heterogeneous data, which are infrared images [21], [60], sketches [61], depth images [62], or even text descriptions [63]. This motivates the heterogeneous Re-ID in ¬ß 3.1.
2) Bounding Box Generation vs. Raw Images/Videos : For the bounding box generation in Step 2, the closed-world person Re-ID usually performs the training and testing based on the generated bounding boxes, where the bounding boxes mainly contain the person appearance information. In contrast, some practical open-world applications require end-to-end person search from the raw images or videos [55], [64]. This leads to another open-world topic, i.e., end-to-end person search in ¬ß 3.2.
3) SufÔ¨Åcient Annotated Data vs. Unavailable/Limited Labels: For the training data annotation in Step 3, the closedworld person Re-ID usually assumes that we have enough annotated training data for supervised Re-ID model training. However, label annotation for each camera pair in every new environment is time consuming and labor intensive, incurring high costs. In openworld scenarios, we might not have enough annotated data (i.e., limited labels) [65] or even without any label information [66]. This inspires the discussion of the unsupervised and semi-supervised Re-ID in ¬ß 3.3.
4) Correct Annotation vs. Noisy Annotation: For Step 4, existing closed-world person Re-ID systems usually assume that all the annotations are correct, with clean labels. However, annotation noise is usually unavoidable due to annotation error (i.e., label noise) or imperfect detection/tracking results (i.e., sample noise, partial Re-ID [67]). This leads to the analysis of noise-robust person Re-ID under different noise types in ¬ß 3.4.
5) Query Exists in Gallery vs. Open-set: In the pedestrian retrieval stage (Step 5), most existing closed-world person Re-ID works assume that the query must occur in the gallery set by calculating the CMC [68] and mAP [5]. However, in many scenarios, the query person may not appear in the gallery set [69], [70], or we need to perform the veriÔ¨Åcation rather than retrieval [26]. This brings us to the open-set person Re-ID in ¬ß 3.5.
This survey Ô¨Årst introduces the widely studied person Re-ID under closed-world settings in ¬ß 2. A detailed review on the datasets and the state-of-the-arts are conducted in ¬ß 2.4. We then introduce the open-world person Re-ID in ¬ß 3. An outlook for future Re-ID is presented in ¬ß 4, including

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

3

a new evaluation metric (¬ß 4.1), a new powerful AGW baseline (¬ß 4.2). We discuss several under-investigated open issues for future study (¬ß 4.3). Conclusions will be drawn in ¬ß 5. A structure overview is shown in the supplementary.
2 CLOSED-WORLD PERSON RE-IDENTIFICATION
This section provides an overview for closed-world person Re-ID. As discussed in ¬ß 1, this setting usually has the following assumptions: 1) person appearances are captured by single-modality visible cameras, either by image or video; 2) The persons are represented by bounding boxes, where most of the bounding box area belongs the same identity; 3) The training has enough annotated training data for supervised discriminative Re-ID model learning; 4) The annotations are generally correct; 5) The query person must appear in the gallery set. Typically, a standard closed-world Re-ID system contains three main components: Feature Representation Learning (¬ß 2.1), which focuses on developing the feature construction strategies; Deep Metric Learning (¬ß 2.2), which aims at designing the training objectives with different loss functions or sampling strategies; and Ranking Optimization (¬ß 2.3), which concentrates on optimizing the retrieved ranking list. An overview of the datasets and state-of-thearts with in-depth analysis is provided in ¬ß 2.4.2.
2.1 Feature Representation Learning
We Ô¨Årstly discuss the feature learning strategies in closedworld person Re-ID. There are four main categories (as shown in Fig. 2): a) Global Feature (¬ß 2.1.1), it extracts a global feature representation vector for each person image without additional annotation cues [55]; b) Local Feature (¬ß 2.1.2), it aggregates part-level local features to formulate a combined representation for each person image [75], [76], [77]; c) Auxiliary Feature (¬ß 2.1.3), it improves the feature representation learning using auxiliary information, e.g., attributes [71], [72], [78], GAN generated images [42], etc. d) Video Feature (¬ß 2.1.4), it learns video representation for video-based Re-ID [7] using multiple image frames and temporal information [73], [74]. We also review several speciÔ¨Åc architecture designs for person Re-ID in ¬ß 2.1.5.
2.1.1 Global Feature Representation Learning
Global feature representation learning extracts a global feature vector for each person image, as shown in Fig. 2(a). Since deep neural networks are originally applied in image classiÔ¨Åcation [79], [80], global feature learning is the primary choice when integrating advanced deep learning techniques into the person Re-ID Ô¨Åeld in early years.
To capture the Ô¨Åne-grained cues in global feature learning, A joint learning framework consisting of a singleimage representation (SIR) and cross-image representation (CIR) is developed in [81], trained with triplet loss using speciÔ¨Åc sub-networks. The widely-used ID-discriminative Embedding (IDE) model [55] constructs the training process as a multi-class classiÔ¨Åcation problem by treating each identity as a distinct class. It is now widely used in Re-ID community [42], [58], [77], [82], [83]. Qian et al. [84] develop a multi-scale deep representation learning model to capture discriminative cues at different scales.

Attention Information. Attention schemes have been widely studied in literature to enhance representation learning [85]. 1) Group 1: Attention within the person image. Typical strategies include the pixel level attention [86] and the channel-wise feature response re-weighting [86], [87], [88], [89], or background suppressing [22]. The spatial information is integrated in [90]. 2) Group 2: attention across multiple person images. A context-aware attentive feature learning method is proposed in [91], incorporating both an intra-sequence and inter-sequence attention for pair-wise feature alignment and reÔ¨Ånement. The attention consistency property is added in [92], [93]. Group similarity [94], [95] is another popular approach to leverage the cross-image attention, which involves multiple images for local and global similarity modeling. The Ô¨Årst group mainly enhances the robustness against misalignment/imperfect detection, and the second improves the feature learning by mining the relations across multiple images.
2.1.2 Local Feature Representation Learning
It learns part/region aggregated features, making it robust against misalignment [77], [96]. The body parts are either automatically generated by human parsing/pose estimation (Group 1) or roughly horizontal division (Group 2).
With automatic body part detection, the popular solution is to combine the full body representation and local part features [97], [98]. SpeciÔ¨Åcally, the multi-channel aggregation [99], multi-scale context-aware convolutions [100], multistage feature decomposition [17] and bilinear-pooling [97] are designed to improve the local feature learning. Rather than feature level fusion, the part-level similarity combination is also studied in [98]. Another popular solution is to enhance the robustness against background clutter, using the pose-driven matching [101], pose-guided part attention module [102], semantically part alignment [103], [104].
For horizontal-divided region features, multiple partlevel classiÔ¨Åers are learned in Part-based Convolutional Baseline (PCB) [77], which now serves as a strong part feature learning baseline in the current state-of-the-art [28], [105], [106]. To capture the relations across multiple body parts, the Siamese Long Short-Term Memory (LSTM) architecture [96], second-order non-local attention [107], Interaction-and-Aggregation (IA) [108] are designed to reinforce the feature learning.
The Ô¨Årst group uses human parsing techniques to obtain semantically meaningful body parts, which provides wellalign part features. However, they require an additional pose detector and are prone to noisy pose detections [77]. The second group uses a uniform partition to obtain the horizontal stripe parts, which is more Ô¨Çexible, but it is sensitive to heavy occlusions and large background clutter.
2.1.3 Auxiliary Feature Representation Learning
Auxiliary feature representation learning usually requires additional annotated information (e.g., semantic attributes [71]) or generated/augmented training samples to reinforce the feature representation [19], [42].
Semantic Attributes. A joint identity and attribute learning baseline is introduced in [72]. Su et al. [71] propose a deep attribute learning framework by incorporating the predicted semantic attribute information, enhancing the

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

CNN

CNN

‚Äúmale‚Äù, ‚Äúshort hair‚Äù
CNN

4
CNN

(a) Global

(b) Local

(c) Auxiliary

(d) Video

Fig. 2: Four different feature learning strategies. a) Global Feature, learning a global representation for each person image in ¬ß 2.1.1; b) Local Feature, learning part-aggregated local features in ¬ß 2.1.2; c) Auxiliary Feature, learning the feature representation using auxiliary information, e.g., attributes [71], [72] in ¬ß 2.1.3 and d) Video Feature , learning the video representation using multiple image frames and temporal information [73], [74] in ¬ß 2.1.4.

generalizability and robustness of the feature representation in a semi-supervised learning manner. Both the semantic attributes and the attention scheme are incorporated to improve part feature learning [109]. Semantic attributes are also adopted in [110] for video Re-ID feature representation learning. They are also leveraged as the auxiliary supervision information in unsupervised learning [111].
Viewpoint Information. The viewpoint information is also leveraged to enhance the feature representation learning [112], [113]. Multi-Level Factorisation Net (MLFN) [112] also tries to learn the identity-discriminative and viewinvariant feature representations at multiple semantic levels. Liu et al. [113] extract a combination of view-generic and view-speciÔ¨Åc learning. An angular regularization is incorporated in [114] in the viewpoint-aware feature learning.
Domain Information. A Domain Guided Dropout (DGD) algorithm [54] is designed to adaptively mine the domain-sharable and domain-speciÔ¨Åc neurons for multidomain deep feature representation learning. Treating each camera as a distinct domain, Lin et al. [115] propose a multicamera consistent matching constraint to obtain a globally optimal representation in a deep learning framework. Similarly, the camera view information or the detected camera location is also applied in [18] to improve the feature representation with camera-speciÔ¨Åc information modeling.
GAN Generation. This section discusses the use of GAN generated images as the auxiliary information. Zheng et al. [42] start the Ô¨Årst attempt to apply the GAN technique for person Re-ID. It improves the supervised feature representation learning with the generated person images. Pose constraints are incorporated in [116] to improve the quality of the generated person images, generating the person images with new pose variants. A pose-normalized image generation approach is designed in [117], which enhances the robustness against pose variations. Camera style information [118] is also integrated in the image generation process to address the cross camera variations. A joint discriminative and generative learning model [119] separately learns the appearance and structure codes to improve the image generation quality. Using the GAN generated images is also a widely used approach in unsupervised domain adaptation Re-ID [120], [121], approximating the target distribution.
Data Augmentation. For Re-ID, custom operations are random resize, cropping and horizontal Ô¨Çip [122]. Besides, adversarially occluded samples [19] are generated to augment the variation of training data. A similar random erasing strategy is proposed in [123], adding random noise to the input images. A batch DropBlock [124] randomly

drops a region block in the feature map to reinforce the attentive feature learning. Bak et al. [125] generate the virtual humans rendered under different illumination conditions. These methods enrich the supervision with the augmented samples, improving the generalizability on the testing set.
2.1.4 Video Feature Representation Learning
Video-based Re-ID is another popular topic [126], where each person is represented by a video sequence with multiple frames. Due to the rich appearance and temporal information, it has gained increasing interest in the ReID community. This also brings in additional challenges in video feature representation learning with multiple images.
The primary challenge is to accurately capture the temporal information. A recurrent neural network architecture is designed for video-based person Re-ID [127], which jointly optimizes the Ô¨Ånal recurrent layer for temporal information propagation and the temporal pooling layer. A weighted scheme for spatial and temporal streams is developed in [128]. Yan et al. [129] present a progressive/sequential fusion framework to aggregate the framelevel human region representations. Semantic attributes are also adopted in [110] for video Re-ID with feature disentangling and frame re-weighting. Jointly aggregating the framelevel feature and spatio-temporal appearance information is crucial for video representation learning [130], [131], [132].
Another major challenge is the unavoidable outlier tracking frames within the videos. Informative frames are selected in a joint Spatial and Temporal Attention Pooling Network (ASTPN) [131], and the contextual information is integrated in [130]. A co-segmentation inspired attention model [132] detects salient features across multiple video frames with mutual consensus estimation. A diversity regularization [133] is employed to mine multiple discriminative body parts in each video sequence. An afÔ¨Åne hull is adopted to handle the outlier frames within the video sequence [83]. An interesting work [20] utilizes the multiple video frames to auto-complete occluded regions. These works demonstrate that handling the noisy frames can greatly improve the video representation learning.
It is also challenging to handle the varying lengths of video sequences, Chen et al. [134] divide the long video sequences into multiple short snippets, aggregating the topranked snippets to learn a compact embedding. A clip-level learning strategy [135] exploits both spatial and temporal dimensional attention cues to produce a robust clip-level representation. Both the short- and long-term relations [136] are integrated in a self-attention scheme.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

5

2.1.5 Architecture Design
Framing person Re-ID as a speciÔ¨Åc pedestrian retrieval problem, most existing works adopt the network architectures [79], [80] designed for image classiÔ¨Åcation as the backbone. Some works have tried to modify the backbone architecture to achieve better Re-ID features. For the widely used ResNet50 backbone [80], the important modiÔ¨Åcations include changing the last convolutional stripe/size to 1 [77], employing adaptive average pooling in the last pooling layer [77], and adding bottleneck layer with batch normalization after the pooling layer [82].
Accuracy is the major concern for speciÔ¨Åc Re-ID network architecture design to improve the accuracy, Li et al. [43] start the Ô¨Årst attempt by designing a Ô¨Ålter pairing neural network (FPNN), which jointly handles misalignment and occlusions with part discriminative information mining. Wang et al. [89] propose a BraidNet with a specially designed WConv layer and Channel Scaling layer. The WConv layer extracts the difference information of two images to enhance the robustness against misalignments and Channel Scaling layer optimizes the scaling factor of each input channel. A Multi-Level Factorisation Net (MLFN) [112] contains multiple stacked blocks to model various latent factors at a speciÔ¨Åc level, and the factors are dynamically selected to formulate the Ô¨Ånal representation. An efÔ¨Åcient fully convolutional Siamese network [137] with convolution similarity module is developed to optimize multi-level similarity measurement. The similarity is efÔ¨Åciently captured and optimized by using the depth-wise convolution.
EfÔ¨Åciency is another important factor for Re-ID architecture design. An efÔ¨Åcient small scale network, namely OmniScale Network (OSNet) [138], is designed by incorporating the point-wise and depth-wise convolutions. To achieve multi-scale feature learning, a residual block composed of multiple convolutional streams is introduced.
With the increasing interest in auto-machine learning, an Auto-ReID [139] model is proposed. Auto-ReID provides an efÔ¨Åcient and effective automated neural architecture design based on a set of basic architecture components, using a part-aware module to capture the discriminative local ReID features. This provides a potential research direction in exploring powerful domain-speciÔ¨Åc architectures.
2.2 Deep Metric Learning
Metric learning has been extensively studied before the deep learning era by learning a Mahalanobis distance function [36], [37] or projection matrix [40]. The role of metric learning has been replaced by the loss function designs to guide the feature representation learning. We will Ô¨Årst review the widely used loss functions in ¬ß 2.2.1 and then summarize the training strategies with speciÔ¨Åc sampling designs ¬ß 2.2.2.
2.2.1 Loss Function Design
This survey only focuses on the loss functions designed for deep learning [56]. An overview of the distance metric learning designed for hand-crafted systems can be found in [2], [143]. There are three widely studied loss functions with their variants in the literature for person Re-ID, including the identity loss, veriÔ¨Åcation loss and triplet loss. An illustration of three loss functions is shown in Fig. 3.

CNN

Classifier ‚ÄúSuman‚Äù

(a) Identity Loss

CNN

CNN CNN

Push Pull Before

‚Ä¶ ‚Ä¶

CNN

?
Same / Not

CNN

Margin

After

(b) Verification Loss

(c) Triplet Loss

Fig. 3: Three kinds of widely used loss functions in the literature. (a) Identity Loss [42], [82], [118], [140] ; (b) VeriÔ¨Åcation

Loss [94], [141] and (c) Triplet Loss [14], [22], [57]. Many

works employ their combinations [87], [137], [141], [142].

Identity Loss. It treats the training process of person Re-ID as an image classiÔ¨Åcation problem [55], i.e., each identity is a distinct class. In the testing phase, the output of the pooling layer or embedding layer is adopted as the feature extractor. Given an input image xi with label yi, the predicted probability of xi being recognized as class yi is encoded with a softmax function, represented by p(yi|xi). The identity loss is then computed by the cross-entropy

1n

Lid

=

‚àí n

i=1 log(p(yi|xi)),

(1)

where n represents the number of training samples within each batch. The identity loss has been widely used in existing methods [19], [42], [82], [92], [95], [106], [118], [120], [140], [144]. Generally, it is easy to train and automatically mine the hard samples during the training process, as demonstrated in [145]. Several works have also investigated the softmax variants [146], such as the sphere loss in [147] and AM softmax in [95]. Another simple yet effective strategy, i.e., label smoothing [42], [122], is generally integrated into the standard softmax cross-entropy loss. Its basic idea is to avoid the model Ô¨Åtting to over-conÔ¨Ådent annotated labels, improving the generalizability [148].
VeriÔ¨Åcation Loss. It optimizes the pairwise relationship, either with a contrastive loss [96], [120] or binary veriÔ¨Åcation loss [43], [141]. The contrastive loss improves the relative pairwise distance comparison, formulated by

Lcon = (1 ‚àí Œ¥ij ){max(0, œÅ ‚àí dij )}2 + Œ¥ij d2ij ,

(2)

where dij represents the Euclidean distance between the embedding features of two input samples xi and xj. Œ¥ij is a binary label indicator (Œ¥ij = 1 when xi and xj belong to the same identity, and Œ¥ij = 0, otherwise). œÅ is a margin parameter. There are several variants, e.g., the pairwise comparison with ranking SVM in [81].
Binary veriÔ¨Åcation [43], [141] discriminates the positive and negative of a input image pair. Generally, a differential feature fij is obtained by fij = (fj ‚àí fj)2 [141], where fi and fj are the embedding features of two samples xi and xj. The veriÔ¨Åcation network classiÔ¨Åes the differential feature

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

6

Cam 2 into posiCtiavme3 or negatiCvaem. 4We use p(Œ¥ij |fij ) to represent the probability of an input pair (xi and xj) being recognized as

Cam 1

Cam 2

Cam 3

Cam 4

arch

‚Ä¶ ‚Ä¶ ‚Ä¶ Œ¥ij (0 or 1). The veriÔ¨Åcation loss with cross-entropy is
Lveri(i, j) = ‚àíŒ¥ij log(p(Œ¥ij|fij))‚àí(1‚àíŒ¥ij) log(1‚àíp(Œ¥ij|fij)).

Search

‚Ä¶

‚Ä¶

‚Ä¶

(3)

Easy Match TheHvaredrMiÔ¨Åatcchation isHaordftMeantchcombined with the identity loss (a) Inittioal iRmanpkrinogve the performance [94], [96], [120], [141]. Triplet loss. It treats the Re-ID model training process

Query

(1) Easy

(2) Hard

(a) Initial Ranking

(3) Hard

as a retrieval ranking problem. The basic idea is that the

distance between the positi‚Ä¶ve pair should be smaller than

‚Ä¶

the negative pair by a pre-deÔ¨Åned margin [57]. Typically, a

triplet contains one anchor sample xi, one positive sample

h

Search xj with the same idennteiwtyr,anaknlidst one negative sample xk from

Search

Search

new rank list

(b) Rae-dRiafnfkeirnegnt identity. The triplet loss with a margin parameter

(b) Re-Ranking

is represented by

Fig. 4: An illustration of re-ranking in person Re-ID. Given

Ltri(i, j, k) = max(œÅ + dij ‚àí dik, 0),

(4)

where d(¬∑) measures the Euclidean distance between two samples. The large proportion of easy triplets will dominate the training process if we directly optimize above loss function, resulting in limited discriminability. To alleviate

a query example, an initial rank list is retrieved, where the hard matches are ranked in the bottom. Using the topranked easy positive match (1) as query to search in the gallery, we can get the hard match (2) and (3) with similarity propagation in the gallery set.

this issue, various informative triplet mining methods have been designed [14], [22], [57], [97]. The basic idea is to select 2.2.2 Training strategy

the informative triplets [57], [149]. SpeciÔ¨Åcally, a moderate The batch sampling strategy plays an important role in

positive mining with a weight constraint is introduced in discriminative Re-ID model learning. It is challenging since

[149], which directly optimizes the feature difference. Her- the number of annotated training images for each identity

mans et al. [57] demonstrate that the online hardest positive varies signiÔ¨Åcantly [5]. Meanwhile, the severely imbalanced

and negative mining within each training batch is beneÔ¨Åcial positive and negative sample pairs increases additional

for discriminative Re-ID model learning. Some methods also difÔ¨Åculty for the training strategy design [40].

studied the point to set similarity strategy for informative

The most commonly used training strategy for handling

triplet mining [150], [151]. This enhances robustness against the imbalanced issue is identity sampling [57], [122]. For

the outlier samples with a soft hard-mining scheme.

each training batch, a certain number of identities are

To further enrich the triplet supervision, a quadruplet randomly selected, and then several images are sampled

deep network is developed in [152], where each quadruplet from each selected identity. This batch sampling strategy

contains one anchor sample, one positive sample and two guarantees the informative positive and negative mining.

mined negative samples. The quadruplets are formulated

To handle the imbalance issue between the positive and

with a margin-based online hard negative mining. Optimiz- negative, adaptive sampling is the popular approach to

ing the quadruplet relationship results in smaller intra-class adjust the contribution of positive and negative samples,

variation and larger inter-class variation.

such as Sample Rate Learning (SRL) [89], curriculum sam-

The combination of triplet loss and identity loss is one of pling [87]. Another approach is sample re-weighting, using

the most popular solutions for deep Re-ID model learning the sample distribution [87] or similarity difference [52] to

[28], [87], [90], [93], [103], [104], [116], [137], [142], [153], adjust the sample weight. An efÔ¨Åcient reference constraint

[154]. These two components are mutually beneÔ¨Åcial for is designed in [155] to transform the pairwise/triplet sim-

discriminative feature representation learning. OIM loss. In addition to the above three kinds of loss
functions, an Online Instance Matching (OIM) loss [64] is designed with a memory bank scheme. A memory bank {vk, k = 1, 2, ¬∑ ¬∑ ¬∑ , c} contains the stored instance features, where c denotes the class number. The OIM loss is then formulated by

ilarity to a sample-to-reference similarity, addressing the imbalance issue and enhancing the discriminability, which is also robust to outliers.
To adaptively combine multiple loss functions, a multiloss dynamic training strategy [156] adaptively reweights the identity loss and triplet loss, extracting appropriate component shared between them. This multi-loss training

1

Loim

=

‚àí n

n
log
i=1

exp(viT fi/œÑ )

c k=1

exp(vkT

fi

/œÑ

)

,

(5)

where vi represents the corresponding stored memory fea-

strategy leads to consistent performance gain. 2.3 Ranking Optimization

ture for class yi, and œÑ is a temperature parameter that Ranking optimization plays a crucial role in improving controls the similarity space [145]. viT fi measures the online the retrieval performance in the testing stage. Given an
instance matching score. The comparison with a memorized initial ranking list, it optimizes the ranking order, either by

feature set of unlabelled identities is further included to automatic gallery-to-gallery similarity mining [58], [157] or

calculate the denominator [64], handling the large instance human interaction [158], [159]. Rank/Metric fusion [160],

number of non-targeted identities. This memory scheme is [161] is another popular approach for improving the ranking

also adopted in unsupervised domain adaptive Re-ID [106]. performance with multiple ranking list inputs.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

7

2.3.1 Re-ranking
The basic idea of re-ranking is to utilize the gallery-togallery similarity to optimize the initial ranking list, as shown in Fig. 4. The top-ranked similarity pulling and bottom-ranked dissimilarity pushing is proposed in [157]. The widely-used k-reciprocal reranking [58] mines the contextual information. Similar idea for contextual information modeling is applied in [25]. Bai et al. [162] utilize the geometric structure of the underlying manifold. An expanded cross neighborhood re-ranking method [18] is introduced by integrating the cross neighborhood distance. A local blurring re-ranking [95] employs the clustering structure to improve neighborhood similarity measurement.
Query Adaptive. Considering the query difference, some methods have designed the query adaptive retrieval strategy to replace the uniform searching engine to improve the performance [163], [164]. Andy et al. [163] propose a query adaptive re-ranking method using locality preserving projections. An efÔ¨Åcient online local metric adaptation method is presented in [164], which learns a strictly local metric with mined negative samples for each probe.
Human Interaction. It involves using human feedback to optimize the ranking list [158]. This provides reliable supervision during the re-ranking process. A hybrid humancomputer incremental learning model is presented in [159], which cumulatively learns from human feedback, improving the Re-ID ranking performance on-the-Ô¨Çy.
2.3.2 Rank Fusion
Rank fusion exploits multiple ranking lists obtained with different methods to improve the retrieval performance [59]. Zheng et al. [165] propose a query adaptive late fusion method on top of a ‚ÄúL‚Äù shaped observation to fuse methods. A rank aggregation method by employing the similarity and dissimilarity is developed in [59]. The rank fusion process in person Re-ID is formulated as a consensus-based decision problem with graph theory [166], mapping the similarity scores obtained by multiple algorithms into a graph with path searching. An UniÔ¨Åed Ensemble Diffusion (UED) [161] is recently designed for metric fusion. UED maintains the advantages of three existing fusion algorithms, optimized by a new objective function and derivation. The metric ensemble learning is also studied in [160].
2.4 Datasets and Evaluation
2.4.1 Datasets and Evaluation Metrics
Datasets. We Ô¨Årst review the widely used datasets for the closed-world setting, including 11 image datasets (VIPeR [31], iLIDS [167], GRID [168], PRID2011 [126], CUHK0103 [43], Market-1501 [5], DukeMTMC [42], Airport [169] and MSMT17 [44]) and 7 video datasets (PRID-2011 [126], iLIDS-VID [7], MARS [8], Duke-Video [144], Duke-Tracklet [170], LPW [171] and LS-VID [136]). The statistics of these datasets are shown in Table 2. This survey only focuses on the general large-scale datsets for deep learning methods. A comprehensive summarization of the Re-ID datasets can be found in [169] and their website1. Several observations can be made in terms of the dataset collection over recent years:
1. https://github.com/NEU-Gou/awesome-reid-dataset

TABLE 2: Statistics of some commonly used datasets for closed-world person Re-ID. ‚Äúboth‚Äù means that it contains both hand-cropped and detected bounding boxes. ‚ÄúC&M‚Äù means both CMC and mAP are evaluated.

Dataset VIPeR iLIDS GRID PRID2011 CUHK01 CUHK02 CUHK03 Market-1501 DukeMTMC Airport MSMT17
Dataset PRID-2011 iLIDS-VID MARS Duke-Video Duke-Tracklet LPW LS-VID

Time 2007 2009 2009 2011 2012 2013 2014 2015 2017 2017 2018
time 2011 2014 2016 2018 2018 2018 2019

#ID 632 119 250 200 971 1,816 1,467 1,501 1,404 9,651 4,101
#ID 200 300 1261 1,812 1,788 2,731 3,772

Image datasets

#image #cam.

1,264

2

476

2

1,275

8

1,134

2

3,884

2

7,264

10

13,164

2

32,668

6

36,411

8

39,902

6

126,441

15

Video datasets

#track(#bbox) #cam.

400 (40k)

2

600 (44k)

2

20,715 (1M) 6

4,832 (-)

8

12,647 (-)

8

7,694(590K) 4

14,943 (3M) 15

Label hand hand hand hand hand hand both both both auto auto
label hand hand auto auto auto auto auto

Res. Ô¨Åxed vary vary Ô¨Åxed Ô¨Åxed Ô¨Åxed vary Ô¨Åxed Ô¨Åxed Ô¨Åxed vary
Res. Ô¨Åxed vary Ô¨Åxed Ô¨Åxed C&M Ô¨Åxed Ô¨Åxed

Eval. CMC CMC CMC CMC CMC CMC CMC C&M C&M C&M C&M
Eval CMC CMC C&M C&M
C&M C&M

1) The dataset scale (both #image and #ID) has increased rapidly. Generally, the deep learning approach can beneÔ¨Åt from more training samples. This also increases the annotation difÔ¨Åculty needed in closed-world person Re-ID. 2) The camera number is also greatly increased to approximate the large-scale camera network in practical scenarios. This also introduces additional challenges for model generalizability in a dynamically updated network. 3) The bounding boxes generation is usually performed automatically detected/tracked, rather than mannually cropped. This simulates the real-world scenario with tracking/detection errors.
Evaluation Metrics. To evaluate a Re-ID system, Cumulative Matching Characteristics (CMC) [68] and mean Average Precision (mAP) [5] are two widely used measurements.
CMC-k (a.k.a, Rank-k matching accuracy) [68] represents the probability that a correct match appears in the top-k ranked retrieved results. CMC is accurate when only one ground truth exists for each query, since it only considers the Ô¨Årst match in evaluation process. However, the gallery set usually contains multiple groundtruths in a large camera network, and CMC cannot completely reÔ¨Çect the discriminability of a model across multiple cameras.
Another metric, i.e., mean Average Precision (mAP) [5], measures the average retrieval performance with multiple grountruths. It is originally widely used in image retrieval. For Re-ID evaluation, it can address the issue of two systems performing equally well in searching the Ô¨Årst ground truth (might be easy match as in Fig. 4), but having different retrieval abilities for other hard matches.
Considering the efÔ¨Åciency and complexity of training a Re-ID model, some recent works [138], [139] also report the FLoating-point Operations Per second (FLOPs) and the network parameter size as the evaluation metrics. These two metrics are crucial when the training/testing device has limited computational resources.

2.4.2 In-depth Analysis on State-of-The-Arts
We review the state-of-the-arts from both image-based and video-based perspectives. We include methods published in top CV venues over the past three years.

5DQN 5DQN 5DQN 5DQN

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE



&RQV$WW

9$/



0*1'HQVH6 3\UD6P&L$G /



0+1 31HW%'% $%' 621$



261HW'*1HW



,$1H$W XWR5H,'%DJ7ULFNV

 3&%

3\U1HW



6)7

+XPDQ

        3&%

621$

261HW

%'%0$+%1'

6&$/ 3\UDPLG

0*1

,$1HW3\U1HW 6)7&3RQ1V$H'WWWH'Q*VH16H%W DJ7ULFNV

9$/          3&%













P$3

        P$3

 

621$ 3\UDPLG  'HQVH6

%'%



31HW 6)7



$XWR5H,'

0+1 261HW





3\U1HW 0*1

6&$/



     P$3

 3&%
 

,$1HW 6)7

261HW $XWR5H,' '*1HW







P$3

8
$%'


(a) SOTA on Market-1501 [5]

(b) SOTA on DukeMTMC [42]

(c) SOTA on CUHK03 [43]

(d) SOTA on MSMT17 [44]

Fig. 5: State-of-the-arts (SOTA) on four image-based person Re-ID datasets. Both the Rank-1 accuracy (%) and mAP value (%) are reported. For CUHK03 [43], the detected data under the setting [58] is reported. For Market-1501, the single query setting is used. The best result is highlighted with a red star. All the listed results do not use re-ranking or additional annotated information.

Image-based Re-ID. There are a large number of published papers for image-based Re-ID2. We mainly review the works published in 2019 as well as some representative works in 2018. SpeciÔ¨Åcally, we include PCB [77], MGN [172], PyrNet [6], Auto-ReID [139], ABD-Net [173], BagTricks [122], OSNet [138], DGNet [119], SCAL [90], MHN [174], P2Net [104], BDB [124], SONA [107], SFT [95], ConsAtt [93], DenseS [103], Pyramid [156], IANet [108], VAL [114]. We summarize the results on four datasets (Fig. 5). This overview motivates Ô¨Åve major insights, as discussed below.
First, with the advancement of deep learning, most of the image-based Re-ID methods have achieved higher rank-1 accuracy than humans (93.5% [175]) on the widely used Market-1501 dataset. In particular, VAL [114] obtains the best mAP of 91.6% and Rank-1 accuracy of 96.2% on Market-1501 dataset. The major advantage of VAL is the usage of viewpoint information. The performance can be further improved when using re-ranking or metric fusion. The success of deep learning on these closed-world datasets also motivates the shift focus to more challenging scenarios, i.e., large data size [136] or unsupervised learning [176].
Second, part-level feature learning is beneÔ¨Åcial for discriminative Re-ID model learning. Global feature learning directly learns the representation on the whole image without the part constraints [122]. It is discriminative when the person detection/ tracking can accurately locate the human body. When the person images suffer from large background clutter or heavy occlusions, part-level feature learning usually achieves better performance by mining discriminative body regions [67]. Due to its advantage in handling misalignment/occlusions, we observe that most of the state-of-the-art methods developed recently adopt the features aggregation paradigm, combining the part-level and full human body features [139], [156].
Third, attention is beneÔ¨Åcial for discriminative Re-ID model learning. We observe that all the methods (ConsAtt [93], SCAL [90], SONA [107], ABD-Net [173]) achieving the best performance on each dataset adopt an attention scheme. The attention captures the relationship between different convolutional channels, multiple feature maps, hierarchical layers, different body parts/regions, and even multiple images. Meanwhile, discriminative [173], diverse [133], consistent [93] and high-order [107] properties are
2. https://paperswithcode.com/task/person-re-identiÔ¨Åcation

incorporated to enhance the attentive feature learning. Considering the powerful attention schemes and the speciÔ¨Åcity of the Re-ID problem, it is highly possible that attentive deeply learned systems will continue dominating the Re-ID community, with more domain speciÔ¨Åc properties.
Fourth, multi-loss training can improve the Re-ID model learning. Different loss functions optimize the network from a multi-view perspective. Combining multiple loss functions can improve the performance, evidenced by the multi-loss training strategy in the state-of-the-art methods, including ConsAtt [93], ABD-Net [173] and SONA [107]. In addition, a dynamic multi-loss training strategy is designed in [156] to adaptively integrated two loss functions. The combination of identity loss and triplet loss with hard mining is the primary choice. Moreover, due to the imbalanced issue, sample weighting strategy generally improves the performance by mining informative triplets [52], [89].
Finally, there is still much room for further improvement due to the increasing size of datasets, complex environment, limited training samples. For example, the Rank-1 accuracy (82.3%) and mAP (60.8%) on the newly released MSMT17 dataset [44] are much lower than that on Market-1501 (Rank1: 96.2% and mAP 91.7%) and DukeMTMC (Rank-1: 91.6% and mAP 84.5%). On some other challenging datasets with limited training samples (e.g., GRID [168] and VIPeR [31]), the performance is still very low. In addition, Re-ID models usually suffers signiÔ¨Åcantly on cross-dataset evaluation [28], [54], and the performance drops dramatically under adversarial attack [177]. We are optimistic that there would be important breakthroughs in person Re-ID, with increasing discriminability, robustness, and generalizability.
Video-based Re-ID. Video-based Re-ID has received less interest, compared to image-based Re-ID. We review the deeply learned Re-ID models, including CoSeg [132], GLTR [136], STA [135], ADFD [110], STC [20], DRSA [133], Snippet [134], ETAP [144], DuATM [91], SDM [178], TwoS [128], ASTPN [131], RQEN [171], Forest [130], RNN [127] and IDEX [8]. We also summarize the results on four video Re-ID datasets, as shown in Fig. 6. From these results, the following observations can be drawn.
First, a clear trend of increasing performance can be seen over the years with the development of deep learning techniques. SpeciÔ¨Åcally, the Rank-1 accuracy increases from 70% (RNN [127] in 2016) to 95.5% (GLTR [136] in 2019) on PRID-2011 dataset, and from 58% (RNN [127]) to 86.3%

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE



'65Q45LS6(S$1HW

*/75 
$')'

6QLSSHW

$')' */75



67&





'5456($1

 &R6HJ





6'0





67&

6QLSS$HW')' */7657$ 

'56$ 'X$70 (7$3

&R6HJ



9
*/7567$ &R6HJ
67&

5DQN 5DQN 5DQN 5DQN

 ,'(;

 511 

)RUHVW 7$Z67R361





<HDU


 511
 ,'(;

$6731

7ZR6

6'0

)RUHVW



54(1

)RUHVW 
,'(;

 (7$3









<HDU











P$3









P$3

(a) SOTA on PRID-2011 [126]

(b) SOTA on iLIDS-VID [7]

(c) SOTA on MARS [8]

(d) SOTA on Duke-Video [144]

Fig. 6: State-of-the-arts (SOTA) on four widely used video-based person Re-ID datasets. The Rank-1 accuracies (%) over years are reported. mAP values (%) on MARS [8] and Duke-Video [144] are reported. For Duke-Video, we refer to the settings in [144]. The best result is highlighted with a red star. All the listed results do not use re-ranking or additional annotated information.

(ADFD [110]) on iLIDS-VID dataset. On the large-scale MARS dataset, the Rank-1 accuracy/mAP increase from 68.3%/49.3% (IDEX [8]) to 88.5%/82.3% (STC [20]). On the Duke-Video dataset [144], STA [135] also achieves a Rank-1 accuracy of 96.2%, and the mAP is 94.9%.
Second, spatial and temporal modeling is crucial for discriminative video representation learning. We observe that all the methods (STA [135], STC [20], GLTR [136]) design spatial-temporal aggregation strategies to improve the video Re-ID performance. Similar to image-based ReID, the attention scheme across multiple frames [110], [135] also greatly enhances the discriminability. Another interesting observation in [20] demonstrates that utilizing multiple frames within the video sequence can Ô¨Åll in the occluded regions, which provides a possible solution for handling the challenging occlusion problem in the future.
Finally, the performance on these datases has reached a saturation state, usually about less than 1% accuracy gain on these four video datasets. However, there is still large room for improvements on the challenging cases. For example, on the newly collected video dataset, LS-VID [136], the Rank1 accuracy/mAP of GLTR [136] are only 63.1%/44.43%, while GLTR [136] can achieve state-of-the-art or at least comparable performance on the other four daatsets. LS-VID [136] contains signiÔ¨Åcantly more identities and video sequences. This provides a challenging benchmark for future breakthroughs in video based Re-ID.
3 OPEN-WORLD PERSON RE-IDENTIFICATION
This section reviews open-world person Re-ID as discussed in ¬ß 1, including heterogeneous Re-ID by matching person images across heterogeneous modalities (¬ß 3.1), endto-end Re-ID from the raw images/videos (¬ß 3.2), semi/unsupervised learning with limited/unavailable annotated labels (¬ß 3.3), robust Re-ID model learning with noisy annotations (¬ß 3.4) and open-set person Re-ID when the correct match does not occur in the gallery (¬ß 3.5).

3.1.1 Depth-based Re-ID
Depth images capture the body shape and skeleton information. This provides the possibility for Re-ID under illumination/clothes changing environments, which is also important for personalized human interaction applications.
A recurrent attention-based model is proposed in [179] to address the depth-based person identiÔ¨Åcation. In a reinforcement learning framework, they combine the convolutional and recurrent neural networks to identify small, discriminative local regions of the human body.
Karianakis et al. [180] leverage the large RGB datasets to design a split-rate RGB-to-Depth transfer method, which bridges the gap between the depth images and the RGB images. Their model further incorporates a temporal attention to enhance video representation for depth Re-ID.
Some methods [62], [181] have also studied the combination of RGB and depth information to improve the Re-ID performance, addressing the clothes-changing challenge.
3.1.2 Text-to-Image Re-ID
Text-to-image Re-ID addresses the matching between a text description and RGB images [63]. It is imperative when the visual image of query person cannot be obtained, and only a text description can be alternatively provided.
A gated neural attention model [63] with recurrent neural network learns the shared features between the text description and the person images. This enables the endto-end training for text to image pedestrian retrieval. Cheng et al. [182] propose a global discriminative image-language association learning method, capturing the identity discriminative information and local reconstructive image-language association under a reconstruction process. A cross projection learning method [183] also learns a shared space with image-to-text matching. A deep adversarial graph attention convolution network is designed in [184] with graph relation mining. However, the large semantic gap between the text descriptions and the visual images is still challenging. Meanwhile, how to combine the texts and hand-painting sketch image is also worth studying in the future.

3.1 Heterogeneous Re-ID
This subsection summarizes four main kinds of heterogeneous Re-ID, including Re-ID between depth and RGB images (¬ß 3.1.1), text-to-image Re-ID (¬ß 3.1.2), visible-toinfrared Re-ID (¬ß 3.1.3) and cross resolution Re-ID (¬ß 3.1.4).

3.1.3 Visible-Infrared Re-ID
Visible-Infrared Re-ID handles the cross-modality matching between the daytime visible and night-time infrared images. It is important in low-lighting conditions, where the images can only be captured by infrared cameras [21], [60], [185].

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

10

Wu et al. [21] start the Ô¨Årst attempt to address this issue, by proposing a deep zero-padding framework [21] to adaptively learn the modality sharable features. A two stream network is introduced in [142], [186] to model the modalitysharable and -speciÔ¨Åc information, addressing the intra- and cross-modality variations simultaneously. Besides the crossmodality shared embedding learning [187], the classiÔ¨Åerlevel discrepancy is also investigated in [188]. Recent methods [189], [190] adopt the GAN technique to generate crossmodality person images to reduce the cross-modality discrepancy at both image and feature level. Hierarchical crossModality disentanglement factors are modeled in [191]. A dual-attentive aggregation learning method is presented in [192] to capture multi-level relations.
3.1.4 Cross-Resolution Re-ID
Cross-Resolution Re-ID conducts the matching between low-resolution and high-resolution images, addressing the large resolution variations [13], [14]. A cascaded SR-GAN [193] generates the high-resolution person images in a cascaded manner, incorporating the identity information. Li et al. [194] adopt the adversarial learning technique to obtain resolution-invariant image representations.
3.2 End-to-End Re-ID
End-to-end Re-ID alleviates the reliance on additional step for bounding boxes generation. It involves the person Re-ID from raw images or videos, and multi-camera tracking.
Re-ID in Raw Images/Videos This task requires that the model jointly performs the person detection and reidentiÔ¨Åcation in a single framework [55], [64]. It is challenging due to the different focuses of two major components.
Zheng et al. [55] present a two-stage framework, and systematically evaluate the beneÔ¨Åts and limitations of person detection for the later stage person Re-ID. Xiao et al. [64] design an end-to-end person search system using a single convolutional neural network for joint person detection and re-identiÔ¨Åcation. A Neural Person Search Machine (NPSM) [195] is developed to recursively reÔ¨Åne the searching area and locate the target person by fully exploiting the contextual information between the query and the detected candidate region. Similarly, a contextual instance expansion module [196] is learned in a graph learning framework to improve the end-to-end person search. A query-guided end-to-end person search system [197] is developed using the Siamese squeeze-and-excitation network to capture the global context information with query-guided region proposal generation. A localization reÔ¨Ånement scheme with discriminative Re-ID feature learning is introduced in [198] to generate more reliable bounding boxes. An Identity DiscriminativE Attention reinforcement Learning (IDEAL) method [199] selects informative regions for auto-generated bounding boxes, improving the Re-ID performance.
Yamaguchi et al. [200] investigate a more challenging problem, i.e., searching for the person from raw videos with text description. A multi-stage method with spatio-temporal person detection and multi-modal retrieval is proposed. Further exploration along this direction is expected.
Multi-camera Tracking End-to-end person Re-ID is also closely related to multi-person, multi-camera tracking [52].

A graph-based formulation to link person hypotheses is proposed for multi-person tracking [201], where the holistic features of the full human body and body pose layout are combined as the representation for each person. Ristani et al. [52] learn the correlation between the multi-target multicamera tracking and person Re-ID by hard-identity mining and adaptive weighted triplet learning. Recently, a locality aware appearance metric (LAAM) [202] with both intra- and inter-camera relation modeling is proposed.
3.3 Semi-supervised and Unsupervised Re-ID
3.3.1 Unsupervised Re-ID
Early unsupervised Re-ID mainly learns invariant components, i.e., dictionary [203], metric [204] or saliency [66], which leads to limited discriminability or scalability.
For deeply unsupervised methods, cross-camera label estimation is one the popular approaches [176], [205]. Dynamic graph matching (DGM) [206] formulates the label estimation as a bipartite graph matching problem. To further improve the performance, global camera network constraints [207] are exploited for consistent matching. Liu et al. progressively mine the labels with step-wise metric promotion [204]. A robust anchor embedding method [83] iteratively assigns labels to the unlabelled tracklets to enlarge the anchor video sequences set. With the estimated labels, deep learning can be applied to learn Re-ID models.
For end-to-end unsupervised Re-ID, an iterative clustering and Re-ID model learning is presented in [205]. Similarly, the relations among samples are utilized in a hierarchical clustering framework [208]. Soft multi-label learning [209] mines the soft label information from a reference set for unsupervised learning. A Tracklet Association Unsupervised Deep Learning (TAUDL) framework [170] jointly conducts the within-camera tracklet association and model the cross-camera tracklet correlation. Similarly, an unsupervised camera-aware similarity consistency mining method [210] is also presented in a coarse-to-Ô¨Åne consistency learning scheme. The intra-camera mining and inter-camera association is applied in a graph association framework [211]. The semantic attributes are also adopted in Transferable Joint Attribute-Identity Deep Learning (TJAIDL) framework [111]. However, it is still challenging for model updating with newly arriving unlabelled data.
Besides, several methods have also tried to learn a partlevel representation based on the observation that it is easier to mine the label information in local parts than that of a whole image. A PatchNet [153] is designed to learn discriminative patch features by mining patch level similarity. A Self-similarity Grouping (SSG) approach [212] iteratively conducts grouping (exploits both the global body and local parts similarity for pseudo labeling) and Re-ID model training in a self-paced manner.
Semi-/Weakly supervised Re-ID. With limited label information, a one-shot metric learning method is proposed in [213], which incorporates a deep texture representation and a color metric. A stepwise one-shot learning method (EUG) is proposed in [144] for video-based Re-ID, gradually selecting a few candidates from unlabeled tracklets to enrich the labeled tracklet set. A multiple instance attention learning framework [214] uses the video-level labels for representation learning, alleviating the reliance on full annotation.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

11

3.3.2 Unsupervised Domain Adaptation
Unsupervised domain adaptation (UDA) transfers the knowledge on a labeled source dataset to the unlabeled target dataset [53]. Due to the large domain shift and powerful supervision in source dataset, it is another popular approach for unsupervised Re-ID without target dataset labels.
Target Image Generation. Using GAN generation to transfer the source domain images to target-domain style is a popular approach for UDA Re-ID. With the generated images, this enables supervised Re-ID model learning in the unlabeled target domain. Wei et al. [44] propose a Person Transfer Generative Adversarial Network (PTGAN), transferring the knowledge from one labeled source dataset to the unlabeled target dataset. Preserved self-similarity and domain-dissimilarity [120] is trained with a similarity preserving generative adversarial network (SPGAN). A HeteroHomogeneous Learning (HHL) method [215] simultaneously considers the camera invariance with homogeneous learning and domain connectedness with heterogeneous learning. An adaptive transfer network [216] decomposes the adaptation process into certain imaging factors, including illumination, resolution, camera view, etc. This strategy improves the cross-dataset performance. Huang et al. [217] try to suppress the background shift to minimize the domain shift problem. Chen et al. [218] design an instance-guided context rendering scheme to transfer the person identities from source domain into diverse contexts in the target domain. Besides, a pose disentanglement scheme is added to improve the image generation [121]. A mutual mean-teacher learning scheme is also developed in [219]. However, the scalability and stability of the image generation for practical large-scale changing environment are still challenging.
Bak et al. [125] generate a synthetic dataset with different illumination conditions to model realistic indoor and outdoor lighting. The synthesized dataset increases generalizability of the learned model and can be easily adapted to a new dataset without additional supervision [220].
Target Domain Supervision Mining. Some methods directly mine the supervision on the unlabeled target dataset with a well trained model from source dataset. An exemplar memory learning scheme [106] considers three invariant cues as the supervision, including exemplar-invariance, camera invariance and neighborhood-invariance. The Domain-Invariant Mapping Network (DIMN) [28] formulates a meta-learning pipeline for the domain transfer task, and a subset of source domain is sampled at each training episode to update the memory bank, enhancing the scalability and discriminability. The camera view information is also applied in [221] as the supervision signal to reduce the domain gap. A self-training method with progressive augmentation [222] jointly captures the local structure and global data distribution on the target dataset. Recently, a self-paced contrastive learning framework with hybrid memory [223] is developed with great success, which dynamically generates multi-level supervision signals.
The spatio-temporal information is also utilized as the supervision in TFusion [224]. TFusion transfers the spatiotemporal patterns learned in the source domain to the target domain with a Bayesian fusion model. Similarly, QueryAdaptive Convolution (QAConv) [225] is developed to improve cross-dataset accuracy.

TABLE 3: Statistics of SOTA unsupervised person Re-ID on two image-based datasets. ‚ÄúSource‚Äù represents if it utilizes the source annotated data in training the target Re-ID model. ‚ÄúGen.‚Äù indicates if it contains an image generation process. Rank-1 accuracy (%) and mAP (%) are reported.

Market-1501 DukeMTMC

Methods

Source Gen. R1 mAP R1 mAP

CAMEL [226] ICCV17 Model No 54.5 26.3

-

-

PUL [205] TOMM18

Model No 45.5 20.5 30.0 16.4

PTGAN [120] CVPR18

Data

Yes 58.1 26.9 46.9 26.4

TJ-AIDL‚Ä†[111] CVPR18

Data

No

58.2 26.5 44.3 23.0

HHL [215] ECCV18 MAR‚Ä° [209] CVPR19

Data Data

Yes 62.2 31.4 46.9 27.2 No 67.7 40.0 67.1 48.0

ENC [106] CVPR19

Data No 75.1 43.0 63.3 40.4

ATNet [216] CVPR19 PAUL‚Ä° [153] CVPR19

Data Yes 55.7 25.6 45.1 24.9 Model No 68.5 40.1 72.0 53.2

SBGAN [217] ICCV19

Data Yes 58.5 27.3 53.5 30.8

UCDA [221] ICCV19 CASC‚Ä° [210] ICCV19

Data No 64.3 34.5 55.4 36.7 Model No 65.4 35.5 59.3 37.8

PDA [121] ICCV19

Data Yes 75.2 47.6 63.2 45.1

CR-GAN [218] ICCV19 Data Yes 77.7 54.0 68.9 48.6

PAST [222] ICCV19

Model No 78.4 54.6 72.4 54.3

SSG [212] ICCV19

Model No 80.0 58.3 73.0 53.4

HCT [208] CVPR20

Model No 80.0 56.4 69.6 50.7

SNR [227] CVPR20

Data No 82.8 61.7 76.3 58.1

MMT [219] ICLR20

Data No 87.7 71.2 78.0 65.1

MEB-Net [228] ECCV20 Data No 89.9 76.0 79.6 66.1

SpCL [223] NeurIPS20

Data No 90.3 76.7 82.9 68.8

‚Ä¢ ‚Ä† TJ-AIDL [111] requires additional attribute annotation. ‚Ä¢ ¬ß DAS [125] generates synthesized virtual humans under vairous lightings. ‚Ä¢ ‚Ä° PAUL [153], MAR [209] and CASC [210] use MSMT17 as source dataset.

3.3.3 State-of-The-Arts for Unsupervised Re-ID
Unsupervised Re-ID has achieved increasing attention in recent years, evidenced by the increasing number of publications in top venues. We review the SOTA for unsupervised deeply learned methods on two widely-used image-based Re-ID datasets. The results are summarized in Table 3. From these results, the following insights can be drawn.
First, the unsupervised Re-ID performance has increased signiÔ¨Åcantly over the years. The Rank-1 accuracy/mAP increases from 54.5%/26.3% (CAMEL [226]) to 90.3%/76.7% (SpCL [223]) on the Market-1501 dataset within three years. The performance for DukeMTMC dataset increases from 30.0%/16.4% to 82.9%/68.8%. The gap between the supervised upper bound and the unsupervised learning is narrowed signiÔ¨Åcantly. This demonstrates the success of unsupervised Re-ID with deep learning.
Second, current unsupervised Re-ID is still underdeveloped and it can be further improved in the following aspects: 1) The powerful attention scheme in supervised ReID methods has rarely been applied in unsupervised ReID. 2) Target domain image generation has been proved effective in some methods, but they are not applied in two best methods (PAST [222], SSG [212]). 3) Using the annotated source data in the training process of the target domain is beneÔ¨Åcial for cross-dataset learning, but it is also not included in above two methods. These observations provide the potential basis for further improvements.
Third, there is still a large gap between the unsupervised and supervised Re-ID. For example, the rank-1 accuracy of supervised ConsAtt [93] has achieved 96.1% on the Market1501 dataset, while the highest accuracy of unsupervised SpCL [223] is about 90.3%. Recently, He et al. [229] have demonstrated that unsupervised learning with large-scale unlabeled training data has the ability to outperform the supervised learning on various tasks [230]. We expect that several breakthroughs in future unsupervised Re-ID.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

12

3.4 Noise-Robust Re-ID
Re-ID usually suffers from unavoidable noise due to data collection and annotation difÔ¨Åculty. We review noise-robust Re-ID from three aspects: Partial Re-ID with heavy occlusion, Re-ID with sample noise caused by detection or tracking errors, and Re-ID with label noise caused by annotation error.
Partial Re-ID. This addresses the Re-ID problem with heavy occlusions, i.e., only part of the human body is visible [231]. A fully convolutional network [232] is adopted to generate Ô¨Åx-sized spatial feature maps for the incomplete person images. Deep Spatial feature Reconstruction (DSR) is further incorporated to avoid explicit alignment by exploiting the reconstructing error. Sun et al. [67] design a Visibility-aware Part Model (VPM) to extract sharable region-level features, thus suppressing the spatial misalignment in the incomplete images. A foreground-aware pyramid reconstruction scheme [233] also tries to learn from the unoccluded regions. The Pose-Guided Feature Alignment (PGFA) [234] exploits the pose landmarks to mine discriminative part information from occlusion noise. However, it is still challenging due to the severe partial misalignment, unpredictable visible regions and distracting unshared body regions. Meanwhile, how to adaptively adjust the matching model for different queries still needs further investigation.
Re-ID with Sample Noise. This refers to the problem of the person images or the video sequence containing outlying regions/frames, either caused by poor detection/inaccurate tracking results. To handle the outlying regions or background clutter within the person image, pose estimation cues [17], [18] or attention cues [22], [66], [199] are exploited. The basic idea is to suppress the contribution of the noisy regions in the Ô¨Ånal holistic representation. For video sequences, set-level feature learning [83] or frame level re-weighting [134] are the commonly used approaches to reduce the impact of noisy frames. Hou et al. [20] also utilize multiple video frames to auto-complete occluded regions. It is expected that more domain-speciÔ¨Åc sample noise handling designs in the future.
Re-ID with Label Noise. Label noise is usually unavoidable due to annotation error. Zheng et al. adopt a label smoothing technique to avoid label overÔ¨Åting issues [42]. A Distribution Net (DNet) that models the feature uncertainty is proposed in [235] for robust Re-ID model learning against label noise, reducing the impact of samples with high feature uncertainty. Different from the general classiÔ¨Åcation problem, robust Re-ID model learning suffers from limited training samples for each identity [236]. In addition, the unknown new identities increase additional difÔ¨Åculty for the robust Re-ID model learning.
3.5 Open-set Re-ID and Beyond
Open-set Re-ID is usually formulated as a person veriÔ¨Åcation problem, i.e., discriminating whether or not two person images belong to the same identity [69], [70]. The veriÔ¨Åcation usually requires a learned condition œÑ , i.e., sim(query, gallery) > œÑ . Early researches design handcrafted systems [26], [69], [70]. For deep learning methods, an Adversarial PersonNet (APN) is proposed in [237], which jointly learns a GAN module and the Re-ID feature extractor. The basic idea of this GAN is to generate realistic target-

Rank List 1 1

2 3 4 5 ...
(a) CMC = 1 AP = 0.77 INP = 0.3

10 ‚àí 3
10 NP = 10 = 0.7

Rank List 2 1

2

34

5

...

5‚àí3
10 NP = 5 = 0.4

(b) CMC = 1 AP = 0.70 INP = 0.6

INP = 1- NP

Fig. 7: Difference between the widely used CMC, AP and the negative penalty (NP) measurements. True matching and false matching are bounded in green and red boxes, respectively. Assume that only three correct matches exist in the gallery, rank list 1 gets better AP, but gets much worse NP than rank list 2. The main reason is that rank list 1 contains too many false matchings before Ô¨Ånding the hardest true matching. For consistency with CMC and mAP, we compute the inverse negative penalty (INP), e.g., INP = 1- NP. Larger INP means better performance.

like images (imposters) and enforce the feature extractor is robust to the generated image attack. Modeling feature uncertainty is also investigated in [235]. However, it remains quite challenging to achieve a high true target recognition and maintain low false target recognition rate [238].
Group Re-ID. It aims at associating the persons in groups rather than individuals [167]. Early researches mainly focus on group representation extraction with sparse dictionary learning [239] or covariance descriptor aggregation [240]. The multi-grain information is integrated in [241] to fully capture the characteristics of a group. Recently, the graph convoltuional network is applied in [242], representing the group as a graph. The group similarity is also applied in the end-to-end person search [196] and the individual re-identiÔ¨Åcation [197], [243] to improve the accuracy. However, group Re-ID is still challenging since the group variation is more complicated than the individuals.
Dynamic Multi-Camera Network. Dynamic updated multi-camera network is another challenging issue [23], [24], [27], [29], which needs model adaptation for new cameras or probes. A human in-the-loop incremental learning method is introduced in [24] to update the Re-ID model, adapting the representation for different probe galleries. Early research also applies the active learning [27] for continuous Re-ID in multi-camera network. A continuous adaptation method based on sparse non-redundant representative selection is introduced in [23]. A transitive inference algorithm [244] is designed to exploit the best source camera model based on a geodesic Ô¨Çow kernel. Multiple environmental constraints (e.g., Camera Topology) in dense crowds and social relationships are integrated for an open-world person Re-ID system [245]. The model adaptation and environmental factors of cameras are crucial in practical dynamic multicamera network. Moreover, how to apply the deep learning technique for the dynamic multi-camera network is still less investigated.

4 AN OUTLOOK: RE-ID IN NEXT ERA
This section Ô¨Årstly presents a new evaluation metric in ¬ß 4.1, a strong baseline (in ¬ß 4.2) for person Re-ID. It provides an important guidance for future Re-ID research. Finally, we discuss some under-investigated open issues in ¬ß 4.3.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

13

TABLE 4: Comparison with the state-of-the-arts on singlemodality image-based Re-ID. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported on two public datasets.

Method BagTricks [122] CVPR19W ABD-Net [173] ICCV19 B (ours) B + Att [246] B + WRT B + GeM [247] B + WRT + GeM AGW (Full)

Market-1501 [5] R1 mAP mINP 94.5 85.9 59.4 95.6 88.3 66.2
94.2 85.4 58.3 94.9 86.9 62.2 94.6 86.8 61.9 94.4 86.3 60.1 94.9 87.1 62.5 95.1 87.8 65.0

DukeMTMC [42] R1 mAP mINP 86.4 76.4 40.7 89.0 78.6 42.1 86.1 76.1 40.3 87.5 77.6 41.9 87.1 77.0 41.4 87.3 77.3 41.9 88.2 78.1 43.4 89.0 79.6 45.7

Feature

1 √ó 1 ùëêùëúùëõùë£

ResNet50 Backbone

Generalized Mean Pooling (2) GeM
ùë§1ùëù

BN
ID loss ùë§2ùëù

1 √ó 1 ùëêùëúùëõùë£

1 √ó 1 ùëêùëúùëõùë£

Z

X

1 √ó 1 ùëêùëúùëõùë£

(1) Non-local Attention

(3) Weighted Regularization Triplet

Fig. 8: The framework of the proposed AGW baseline using the widely used ResNet50 [80] as the backbone network.

4.1 mINP: A New Evaluation Metric for Re-ID

For a good Re-ID system, the target person should be re-

trieved as accurately as possible, i.e., all the correct matches

should have low rank values. Considering that the target

person should not be neglected in the top-ranked retrieved

list, especially for multi-camera network, so as to accurately

track the target. When the target person appears in the

gallery set at multiple time stamps, the rank position of

the hardest correct match determines the workload of the

inspectors for further investigation. However, the currently

widely used CMC and mAP metrics cannot evaluate this

property, as shown in Fig. 7. With the same CMC, rank

list 1 achieves a better AP than rank list 2, but it requires

more efforts to Ô¨Ånd all the correct matches. To address this

issue, we design a computationally efÔ¨Åcient metric, namely

a negative penalty (NP), which measures the penalty to Ô¨Ånd

the hardest correct match

NPi

=

Rihard ‚àí |Gi| Rihard

,

(6)

where Rihard indicates the rank position of the hardest match, and |Gi| represents the total number of correct

matches for query i. Naturally, a smaller NP represents

better performance. For consistency with CMC and mAP, we

prefer to use the inverse negative penalty (INP), an inverse

operation of NP. Overall, the mean INP of all the queries is

represented by

1 mINP =
n

1

(1 ‚àí
i

NPi)

=

n

i

|Gi| Rihard

.

(7)

The calculation of mINP is quite efÔ¨Åcient and can be seamlessly integrated in the CMC/mAP calculating process. mINP avoids the domination of easy matches in the mAP/CMC evaluation. One limitation is that mINP value difference for large gallery size would be much smaller compared to small galleries. But it still can reÔ¨Çect the relative performance of a Re-ID model, providing a supplement to the widely-used CMC and mAP metrics.

4.2 A New Baseline for Single-/Cross-Modality Re-ID
According to the discussion in ¬ß 2.4.2, we design a new AGW3 baseline for person Re-ID, which achieves competitive performance on both single-modality (image and video) and cross-modality Re-ID tasks. SpeciÔ¨Åcally, our new baseline is designed on top of BagTricks [122], and AGW contains the following three major improved components:
3. Details are in https://github.com/mangye16/ReID-Survey and comprehensive comparison is shown in the supplementary material.

(1) Non-local Attention (Att) Block. As discussed in

¬ß 2.4.2, the attention scheme plays a crucial role in discrim-

inative Re-ID model learning. We adopt the powerful non-

local attention block [246] to obtain the weighted sum of the

features at all positions, represented by

zi = Wz ‚àó œÜ(xi) + xi,

(8)

where Wz is a weight matrix to be learned, œÜ(¬∑) represents a non-local operation, and +xi formulates a residual learning strategy. Details can be found in [246]. We adopt the default setting from [246] to insert the non-local attention block.
(2) Generalized-mean (GeM) Pooling. As a Ô¨Åne-grained instance retrieval, the widely-used max-pooling or average pooling cannot capture the domain-speciÔ¨Åc discriminative features. We adopt a learnable pooling layer, named generalized-mean (GeM) pooling [247], formulated by

f

=

[f1

¬∑ ¬∑ ¬∑ fk

¬∑ ¬∑ ¬∑ fK ]T , fk

=

1 (
|Xk |

xi ‚ààXk

xpi k

)

1 pk

,

(9)

where fk represents the feature map, and K is number of

feature maps in the last layer. Xk is the set of W √ó H acti-

vations for feature map k ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , K}. pk is a pooling

hyper-parameter, which is learned in the back-propagation

process [247]. The above operation approximates max pool-

ing when pk ‚Üí ‚àû and average pooling when pk = 1.

(3) Weighted Regularization Triplet (WRT) loss. In

addition to the baseline identity loss with softmax cross-

entropy, we integrate with another weighted regularized

triplet loss, Lwrt(i) = log(1 + exp( j wipj dpij ‚àí

k winkdnik)). (10)

wipj =

exp (dpij ) dpij ‚ààPi exp(dpij

)

,

wink

=

exp (‚àídnik) dnik‚ààNi exp(‚àídnik

)

,

(11)

where (i, j, k) represents a hard triplet within each training batch. For anchor i, Pi is the corresponding positive set, and Ni is the negative set. dpij/dnik represents the pairwise distance of a positive/negative sample pair. The above weighted regularization inherits the advantage of relative distance optimization between positive and negative pairs, but it avoids introducing any additional margin parameters. Our weighting strategy is similar to [248], but our solution does not introduce additional hyper-parameters.
The overall framework of AGW is shown in Fig 8. Other components are exactly the same as [122]. In the testing phase, the output of BN layer is adopted as the feature representation for Re-ID. The implementation details and more experimental results are in the supplementary material.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

14

TABLE 5: Comparison with the state-of-the-arts on two TABLE 7: Comparison with the state-of-the-arts on two image Re-ID datasets, including CUHK03 and MSMT17. partial Re-ID datasets, including Partial-REID and PartialRank-1 accuracy (%), mAP (%) and mINP (%) are reported. iLIDS. Rank-1, -3 accuracy (%) and mINP (%) are reported.

Method BagTricks [122] CVPR19W AGW (Full)

CUHK03 [43] R1 mAP mINP 58.0 56.6 43.8 63.6 62.0 50.3

MSMT17 [44] R1 mAP mINP 63.4 45.1 12.4 68.3 49.3 14.7

TABLE 6: Comparison with the state-of-the-arts on four video-based Re-ID datasets, including MARS [8], DukeVideo [144], PRID2011 [126] and iLIDS-VID [7]. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported.

Method BagTricks [122] CVPR19W CoSeg [132] ICCV19 AGW (Ours) AGW+ (Ours)
Method BagTricks [122] CVPR19W AGW (Ours) AGW+ (Ours)

MARS [8] R1 mAP mINP 85.8 81.6 62.0 84.9 79.9 57.8 87.0 82.2 62.8 87.6 83.0 63.9
PRID2011 [126] R1 R5 mINP 84.3 93.3 88.5 87.8 96.6 91.7 94.4 98.4 95.4

DukeVideo [144] R1 mAP mINP 92.6 92.4 88.3 95.4 94.1 89.8 94.6 93.4 89.2 95.4 94.9 91.9
iLIDS-VID [7] R1 R5 mINP 74.0 93.3 82.2 78.0 97.0 85.5 83.2 98.3 89.0

Results on Single-modality Image Re-ID. We Ô¨Årst evaluate each component on two image-based datasets (Market1501 and DukeMTMC) in Table 4. We also list two stateof-the-art methods, BagTricks [122] and ABD-Net [173]. We report the results on CUHK03 and MSMT17 datasets in Table 5. We obtain the following two observations:
1) All the components consistently contribute the accuracy gain, and AGW performs much better than the original BagTricks under various metrics. AGW provides a strong baseline for future improvements. We have also tried to incorporate part-level feature learning [77], but extensive experiments show that it does not improve the performance. How to aggregate part-level feature learning with AGW needs further study in the future. 2) Compared to the current state-of-the-art, ABD-Net [173], AGW performs favorably in most cases. In particular, we achieve much higher mINP on DukeMTMC dataset, 45.7% vs. 42.1%. This demonstrates that AGW requires less effort to Ô¨Ånd all the correct matches, verifying the ability of mINP.
Results on Single-modality Video Re-ID. We also evaluate the proposed AGW on four widely used single modality video-based datasets ( MARS [8], DukeVideo [144], PRID2011 [126] and iLIDS-VID [7], as shown in Table 6. We also compare two state-of-the-art methods, BagTricks [122] and Co-Seg [132]. For video data, we develop a variant (AGW+) to capture the temporal information with framelevel average pooling for sequence representation. Meanwhile, constraint random sampling strategy [133] is applied for training. Compared to Co-Seg [132], our AGW+ obtains better Rank-1, mAP and mINP in most cases.
Results on Partial Re-ID. We also test the performance of AGW on two partial Re-ID datasets, as shown in Table 7. The experimental setting are from DSR [232]. We also achieve comparable performance with the state-of-the-art VPM method [67]. This experiment further demonstrates the superiority of AGW for the open-world partial Re-ID task. Meanwhile, the mINP also shows the applicability for this open-world Re-ID problem.
Results on Cross-modality Re-ID. We also test the performance of AGW using a two-stream architecture on the cross-modality visible-infrared Re-ID task. The comparison

Method
DSR [232] CVPR18 SFR [249] ArXiv18 VPM [67] CVPR19 BagTricks [122] CVPR19W AGW

Partial-REID R1 R3 mINP 50.7 70.0 56.9 78.5 67.7 81.9 62.0 74.0 45.4 69.7 80.0 56.7

Partial-iLIDS R1 R3 mINP 58.8 67.2 63.9 74.8 67.2 76.5 58.8 73.9 68.7 64.7 79.8 73.3

TABLE 8: Comparison with the state-of-the-arts on crossmodality visible-infrared Re-ID. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported on two public datasets.

Method Zero-Pad [21] ICCV17 HCML [186] AAAI18 eBDTR [142] TIFS19 HSME [187] AAAI19 D2RL [189] CVPR19 AlignG [190] ICCV19 Hi-CMD [191] CVPR20
AGW (Ours)

RegDB [60] Visible-Thermal
R1 mAP 17.75 18.90 24.44 20.08 34.62 33.46 50.85 47.00
43.4 44.1 57.9 53.6 70.93 66.04 70.05 66.37 mINP = 50.19

SYSU-MM01 [21]

All Search Indoor Search

R1 mAP R1 mAP

14.8 15.95 20.58 26.92

14.32 16.16 24.52 30.08

27.82 28.42 32.46 42.46

20.68 23.12 -

-

28.9 29.2 -

-

42.4 40.7 45.9 54.3

34.9 35.9

47.50 47.65 54.17 62.97

mINP =35.30 mINP = 59.23

with the current state-of-the-arts on two datasets is shown in Table 8. We follow the settings in AlignG [190] to perform the experiments. Results show that AGW achieves much higher accuracy than existing cross-modality Re-ID models, verifying the effectiveness for the open-world Re-ID task.

4.3 Under-Investigated Open Issues
We discuss the open-issues from Ô¨Åve different aspects according to the Ô¨Åve steps in ¬ß1, including uncontrollable data collection, human annotation minimization, domainspeciÔ¨Åc/generalizable architecture design, dynamic model updating and efÔ¨Åcient model deployment.
4.3.1 Uncontrollable Data Collection
Most existing Re-ID works evaluate their method on a welldeÔ¨Åned data collection environment. However, the data collection in real complex environment is uncontrollable. The data might be captured from unpredictable modality, modality combinations, or even cloth changing data [30].
Multi-Heterogeneous Data. In real applications, the ReID data might be captured from multiple heterogeneous modalities, i.e., the resolutions of person images vary a lot [193], both the query and gallery sets may contain different modalities (visible, thermal [21], depth [62] or text description [10]). This results in a challenging multiple heterogeneous person Re-ID. A good person Re-ID system would be able to automatically handle the changing resolutions, different modalities, various environments and multiple domains. Future work with broad generalizability is expected, evaluating their method for different Re-ID tasks.
Cloth-Changing Data. In practical surveillance system, it is very likely to contain a large number of target persons with changing clothes. A cloth-Clothing Change Aware Network (CCAN) [250] addresses this issue by separately extracting the face and body context representation, and similar idea is applied in [251]. Yang et al. [30] present a spatial polar transformation (SPT) to learn cross-cloth invariant representation. However, they still rely heavily on the face and body appearance, which might be unavailable

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

15

and unstable in real scenarios. It would be interesting to further explore the possibility of other discriminative cues (e.g., gait, shape) to address the cloth-changing issue.
4.3.2 Human Annotation Minimization
Besides the unsupervised learning, active learning or human interaction [24], [27], [154], [159] provides another possible solution to alleviate the reliance on human annotation.
Active Learning. Incorporating human interaction, labels are easily provided for newly arriving data and the model can be subsequently updated [24], [27]. A pairwise subset selection framework [252] minimizes human labeling effort by Ô¨Årstly constructing an edge-weighted complete kpartite graph and then solving it as a triangle free subgraph maximization problem. Along this line, a deep reinforcement active learning method [154] iteratively reÔ¨Ånes the learning policy and trains a Re-ID network with human-inthe-loop supervision. For video data, an interpretable reinforcement learning method with sequential decision making [178] is designed. The active learning is crucial in practical Re-ID system design, but it has received less attention in the research community. Additionally, the newly arriving identities is extremely challenging, even for human. EfÔ¨Åcient human in-the-loop active learning is expected in the future.
Learning for Virtual Data. This provides an alternative for minimizing the human annotation. A synthetic dataset is collected in [220] for training, and they achieve competitive performance on real-world datasets when trained on this synthesized dataset. Bak et al. [125] generate a new synthetic dataset with different illumination conditions to model realistic indoor and outdoor lighting. A large-scale synthetic PersonX dataset is collected in [105] to systematically study the effect of viewpoint for a person Re-ID system. Recently, the 3D person images are also studied in [253], generating the 3D body structure from 2D images. However, how to bridge the gap between synthesized images and real-world datasets remains challenging.
4.3.3 Domain-SpeciÔ¨Åc/Generalizable Architecture Design
Re-ID SpeciÔ¨Åc Architecture. Existing Re-ID methods usually adopt architectures designed for image classiÔ¨Åcation as the backbone. Some methods modify the architecture to achieve better Re-ID features [82], [122]. Very recently, researchers have started to design domain speciÔ¨Åc architectures, e.g., OSNet with omni-scale feature learning [138]. It detects the small-scale discriminative features at a certain scale. OSNet is extremely lightweight and achieves competitive performance. With the advancement of automatic neural architecture search (e.g., Auto-ReID [139]), more domainspeciÔ¨Åc powerful architectures are expected to address the task-speciÔ¨Åc Re-ID challenges. Limited training samples in Re-ID also increase the difÔ¨Åculty in architecture design.
Domain Generalizable Re-ID. It is well recognized that there is a large domain gap between different datsets [56], [225]. Most existing methods adopt domain adaptation for cross-dataset training. A more practical solution would be learning a domain generalized model with a number of source datasets, such that the learned model can be generalized to new unseen datasets for discriminative ReID without additional training [28]. Hu et al. [254] studied the cross-dataset person Re-ID by introducing a part-level

CNN framework. The Domain-Invariant Mapping Network (DIMN) [28] designs a meta-learning pipeline for domain generalizable Re-ID, learning a mapping between a person image and its identity classiÔ¨Åer. The domain generalizability is crucial to deploy the learned Re-ID model under an unknown scenario.
4.3.4 Dynamic Model Updating
Fixed model is inappropriate for practical dynamically updated surveillance system. To alleviate this issue, dynamic model updating is imperative, either to a new domain/camera or adaptation with newly collected data.
Model Adaptation to New Domain/Camera. Model adaptation to a new domain has been widely studied in the literature as a domain adaptation problem [125], [216]. In practical dynamic camera network, a new camera may be temporarily inserted into an existing surveillance system. Model adaptation is crucial for continuous identiÔ¨Åcation in a multi-camera network [23], [29]. To adapt a learned model to a new camera, a transitive inference algorithm [244] is designed to exploit the best source camera model based on a geodesic Ô¨Çow kernel. However, it is still challenging when the newly collected data by the new camera has totally different distributions. In addition, the privacy and efÔ¨Åciency issue [255] also need further consideration.
Model Updating with Newly Arriving Data. With the newly collected data, it is impractical to training the previously learned model from the scratch [24]. An incremental learning approach together with human interaction is designed in [24]. For deeply learned model, an addition using covariance loss [256] is integrated in the overall learning function. However, this problem is not well studied since the deep model training require large amount of training data. Besides, the unknown new identities in the newly arriving data is hard to be identiÔ¨Åed for the model updating.
4.3.5 EfÔ¨Åcient Model Deployment
It is important to design efÔ¨Åcient and adaptive models to address scalability issue for practical model deployment.
Fast Re-ID. For fast retrieval, hashing has been extensively studied to boost the searching speed, approximating the nearest neighbor search [257]. Cross-camera Semantic Binary Transformation (CSBT) [258] transforms the original high-dimensional feature representations into compact lowdimensional identity-preserving binary codes. A Coarse-toFine (CtF) hashing code search strategy is developed in [259], complementarily using short and long codes. However, the domain-speciÔ¨Åc hashing still needs further study.
Lightweight Model. Another direction for addressing the scalability issue is to design a lightweight Re-ID model. Modifying the network architecture to achieve a lightweight model is investigated in [86], [138], [139]. Model distillation is another approach, e.g., a multi-teacher adaptive similarity distillation framework is proposed in [260], which learns a user-speciÔ¨Åed lightweight student model from multiple teacher models, without access to source domain data.
Resource Aware Re-ID. Adaptively adjusting the model according to the hardware conÔ¨Ågurations also provides a solution to handle the scalability issue. Deep Anytime ReID (DaRe) [14] employs a simple distance based routing

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

16

strategy to adaptively adjust the model, Ô¨Åtting to hardware devices with different computational resources.
5 CONCLUDING REMARKS
This paper presents a comprehensive survey with in-depth analysis from a both closed-world and open-world perspectives. We Ô¨Årst introduce the widely studied person Re-ID under the closed-world setting from three aspects: feature representation learning, deep metric learning and ranking optimization. With powerful deep learning, the closedworld person Re-ID has achieved performance saturation on several datasets. Correspondingly, the open-world setting has recently gained increasing attention, with efforts to address various practical challenges. We also design a new AGW baseline, which achieves competitive performance on four Re-ID tasks under various metrics. It provides a strong baseline for future improvements. This survey also introduces a new evaluation metric to measure the cost of Ô¨Ånding all the correct matches. We believe this survey will provide important guidance for future Re-ID research.
REFERENCES
[1] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai, ‚ÄúPerson reidentiÔ¨Åcation by camera correlation aware feature augmentation,‚Äù IEEE TPAMI, vol. 40, no. 2, pp. 392‚Äì408, 2018.
[2] L. Zheng, Y. Yang, and A. G. Hauptmann, ‚ÄúPerson reidentiÔ¨Åcation: Past, present and future,‚Äù arXiv preprint arXiv:1610.02984, 2016.
[3] N. Gheissari, T. B. Sebastian, and R. Hartley, ‚ÄúPerson reidentiÔ¨Åcation using spatiotemporal appearance,‚Äù in CVPR, 2006, pp. 1528‚Äì1535.
[4] J. Almazan, B. Gajic, N. Murray, and D. Larlus, ‚ÄúRe-id done right: towards good practices for person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:1801.05339, 2018.
[5] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, ‚ÄúScalable person re-identiÔ¨Åcation: A benchmark,‚Äù in ICCV, 2015, pp. 1116‚Äì1124.
[6] N. Martinel, G. Luca Foresti, and C. Micheloni, ‚ÄúAggregating deep pyramidal representations for person re-identiÔ¨Åcation,‚Äù in CVPR Workshops, 2019, pp. 0‚Äì0.
[7] T. Wang, S. Gong, X. Zhu, and S. Wang, ‚ÄúPerson re-identiÔ¨Åcation by video ranking,‚Äù in ECCV, 2014, pp. 688‚Äì703.
[8] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, ‚ÄúMars: A video benchmark for large-scale person reidentiÔ¨Åcation,‚Äù in ECCV, 2016.
[9] M. Ye, C. Liang, Z. Wang, Q. Leng, J. Chen, and J. Liu, ‚ÄúSpeciÔ¨Åc person retrieval via incomplete text description,‚Äù in ACM ICMR, 2015, pp. 547‚Äì550.
[10] S. Li, T. Xiao, H. Li, W. Yang, and X. Wang, ‚ÄúIdentity-aware textual-visual matching with latent co-attention,‚Äù in ICCV, 2017, pp. 1890‚Äì1899.
[11] S. Karanam, Y. Li, and R. J. Radke, ‚ÄúPerson re-identiÔ¨Åcation with discriminatively trained viewpoint invariant dictionaries,‚Äù in ICCV, 2015, pp. 4516‚Äì4524.
[12] S. Bak, S. Zaidenberg, B. Boulay, and F. Bremond, ‚ÄúImproving person re-identiÔ¨Åcation by viewpoint cues,‚Äù in AVSS, 2014, pp. 175‚Äì180.
[13] X. Li, W.-S. Zheng, X. Wang, T. Xiang, and S. Gong, ‚ÄúMulti-scale learning for low-resolution person re-identiÔ¨Åcation,‚Äù in ICCV, 2015, pp. 3765‚Äì3773.
[14] Y. Wang, L. Wang, Y. You, X. Zou, V. Chen, S. Li, G. Huang, B. Hariharan, and K. Q. Weinberger, ‚ÄúResource aware person reidentiÔ¨Åcation across multiple resolutions,‚Äù in CVPR, 2018, pp. 8042‚Äì8051.
[15] Y. Huang, Z.-J. Zha, X. Fu, and W. Zhang, ‚ÄúIllumination-invariant person re-identiÔ¨Åcation,‚Äù in ACM MM, 2019, pp. 365‚Äì373.
[16] Y.-J. Cho and K.-J. Yoon, ‚ÄúImproving person re-identiÔ¨Åcation via pose-aware multi-shot matching,‚Äù in CVPR, 2016, pp. 1354‚Äì1362.

[17] H. Zhao, M. Tian, S. Sun, and et al, ‚ÄúSpindle net: Person reidentiÔ¨Åcation with human body region guided feature decomposition and fusion,‚Äù in CVPR, 2017, pp. 1077‚Äì1085.
[18] M. S. Sarfraz, A. Schumann, A. Eberle, and R. Stiefelhagen, ‚ÄúA pose-sensitive embedding for person re-identiÔ¨Åcation with expanded cross neighborhood re-ranking,‚Äù in CVPR, 2018, pp. 420‚Äì429.
[19] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, ‚ÄúAdversarially occluded samples for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 5098‚Äì5107.
[20] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, ‚ÄúVrstc: Occlusion-free video person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 7183‚Äì7192.
[21] A. Wu, W.-s. Zheng, H.-X. Yu, S. Gong, and J. Lai, ‚ÄúRgb-infrared cross-modality person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 5380‚Äì 5389.
[22] C. Song, Y. Huang, W. Ouyang, and L. Wang, ‚ÄúMask-guided contrastive attention model for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 1179‚Äì1188.
[23] A. Das, R. Panda, and A. K. Roy-Chowdhury, ‚ÄúContinuous adaptation of multi-camera person identiÔ¨Åcation models through sparse non-redundant representative selection,‚Äù CVIU, vol. 156, pp. 66‚Äì78, 2017.
[24] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-Chowdhury, ‚ÄúTemporal model adaptation for person re-identiÔ¨Åcation,‚Äù in ECCV, 2016, pp. 858‚Äì877.
[25] J. Garcia, N. Martinel, A. Gardel, I. Bravo, G. L. Foresti, and C. Micheloni, ‚ÄúDiscriminant context information analysis for post-ranking person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing, vol. 26, no. 4, pp. 1650‚Äì1665, 2017.
[26] W.-S. Zheng, S. Gong, and T. Xiang, ‚ÄúTowards open-world person re-identiÔ¨Åcation by one-shot group-based veriÔ¨Åcation,‚Äù IEEE TPAMI, vol. 38, no. 3, pp. 591‚Äì606, 2015.
[27] A. Das, R. Panda, and A. Roy-Chowdhury, ‚ÄúActive image pair selection for continuous person re-identiÔ¨Åcation,‚Äù in ICIP, 2015, pp. 4263‚Äì4267.
[28] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, ‚ÄúGeneralizable person re-identiÔ¨Åcation by domain-invariant mapping network,‚Äù in CVPR, 2019, pp. 719‚Äì728.
[29] A. Das, A. Chakraborty, and A. K. Roy-Chowdhury, ‚ÄúConsistent re-identiÔ¨Åcation in a camera network,‚Äù in ECCV, 2014, pp. 330‚Äì 345.
[30] Q. Yang, A. Wu, and W. Zheng, ‚ÄúPerson re-identiÔ¨Åcation by contour sketch under moderate clothing change.‚Äù IEEE TPAMI, 2019.
[31] D. Gray and H. Tao, ‚ÄúViewpoint invariant pedestrian recognition with an ensemble of localized features,‚Äù in ECCV, 2008, pp. 262‚Äì 275.
[32] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, ‚ÄúPerson re-identiÔ¨Åcation by symmetry-driven accumulation of local features,‚Äù in CVPR, 2010, pp. 2360‚Äì2367.
[33] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li, ‚ÄúSalient color names for person re-identiÔ¨Åcation,‚Äù in ECCV, 2014, pp. 536‚Äì551.
[34] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, ‚ÄúPerson re-identiÔ¨Åcation by local maximal occurrence representation and metric learning,‚Äù in CVPR, 2015, pp. 2197‚Äì2206.
[35] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato, ‚ÄúHierarchical gaussian descriptor for person re-identiÔ¨Åcation,‚Äù in CVPR, 2016, pp. 1363‚Äì1372.
[36] M. Kostinger, M. Hirzer, P. Wohlhart, and et al, ‚ÄúLarge scale metric learning from equivalence constraints,‚Äù in CVPR, 2012, pp. 2288‚Äì2295.
[37] W.-S. Zheng, S. Gong, and T. Xiang, ‚ÄúPerson re-identiÔ¨Åcation by probabilistic relative distance comparison,‚Äù in CVPR, 2011, pp. 649‚Äì656.
[38] F. Xiong, M. Gou, O. Camps, and M. Sznaier, ‚ÄúPerson reidentiÔ¨Åcation using kernel-based metric learning methods,‚Äù in ECCV, 2014, pp. 1‚Äì16.
[39] M. Hirzer, P. M. Roth, M. Ko¬® stinger, and H. Bischof, ‚ÄúRelaxed pairwise learned metric for person re-identiÔ¨Åcation,‚Äù in ECCV, 2012, pp. 780‚Äì793.
[40] S. Liao and S. Z. Li, ‚ÄúEfÔ¨Åcient psd constrained asymmetric metric learning for person re-identiÔ¨Åcation,‚Äù in ICCV, 2015, pp. 3685‚Äì 3693.
[41] H.-X. Yu, A. Wu, and W.-S. Zheng, ‚ÄúUnsupervised person reidentiÔ¨Åcation by deep asymmetric metric embedding,‚Äù IEEE TPAMI, 2018.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

17

[42] Z. Zheng, L. Zheng, and Y. Yang, ‚ÄúUnlabeled samples generated by gan improve the person re-identiÔ¨Åcation baseline in vitro,‚Äù in ICCV, 2017, pp. 3754‚Äì3762.
[43] W. Li, R. Zhao, T. Xiao, and X. Wang, ‚ÄúDeepreid: Deep Ô¨Ålter pairing neural network for person re-identiÔ¨Åcation,‚Äù in CVPR, 2014, pp. 152‚Äì159.
[44] L. Wei, S. Zhang, W. Gao, and Q. Tian, ‚ÄúPerson transfer gan to bridge domain gap for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 79‚Äì88.
[45] Q. Leng, M. Ye, and Q. Tian, ‚ÄúA survey of open-world person reidentiÔ¨Åcation,‚Äù IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2019.
[46] D. Wu, S.-J. Zheng, X.-P. Zhang, C.-A. Yuan, F. Cheng, Y. Zhao, Y.J. Lin, Z.-Q. Zhao, Y.-L. Jiang, and D.-S. Huang, ‚ÄúDeep learningbased methods for person re-identiÔ¨Åcation: A comprehensive review,‚Äù Neurocomputing, 2019.
[47] B. Lavi, M. F. Serj, and I. Ullah, ‚ÄúSurvey on deep learning techniques for person re-identiÔ¨Åcation task,‚Äù arXiv preprint arXiv:1807.05284, 2018.
[48] X. Wang, ‚ÄúIntelligent multi-camera video surveillance: A review,‚Äù Pattern recognition letters, vol. 34, no. 1, pp. 3‚Äì19, 2013.
[49] D. Geronimo, A. M. Lopez, A. D. Sappa, and T. Graf, ‚ÄúSurvey of pedestrian detection for advanced driver assistance systems,‚Äù IEEE TPAMI, no. 7, pp. 1239‚Äì1258, 2009.
[50] P. Dollar, C. Wojek, B. Schiele, and P. Perona, ‚ÄúPedestrian detection: A benchmark,‚Äù in CVPR, 2009, pp. 304‚Äì311.
[51] E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang, E. Levinkov, B. Andres, and B. Schiele, ‚ÄúArttrack: Articulated multi-person tracking in the wild,‚Äù in CVPR, 2017, pp. 6457‚Äì6465.
[52] E. Ristani and C. Tomasi, ‚ÄúFeatures for multi-target multi-camera tracking and re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 6036‚Äì6046.
[53] A. J. Ma, P. C. Yuen, and J. Li, ‚ÄúDomain transfer support vector ranking for person re-identiÔ¨Åcation without target camera label information,‚Äù in ICCV, 2013, pp. 3567‚Äì3574.
[54] T. Xiao, H. Li, W. Ouyang, and X. Wang, ‚ÄúLearning deep feature representations with domain guided dropout for person reidentiÔ¨Åcation,‚Äù in CVPR, 2016, pp. 1249‚Äì1258.
[55] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, and Q. Tian, ‚ÄúPerson re-identiÔ¨Åcation in the wild,‚Äù in CVPR, 2017, pp. 1367‚Äì 1376.
[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li, ‚ÄúDeep metric learning for person re-identiÔ¨Åcation,‚Äù in ICPR, 2014, pp. 34‚Äì39.
[57] A. Hermans, L. Beyer, and B. Leibe, ‚ÄúIn defense of the triplet loss for person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:1703.07737, 2017.
[58] Z. Zhong, L. Zheng, D. Cao, and S. Li, ‚ÄúRe-ranking person reidentiÔ¨Åcation with k-reciprocal encoding,‚Äù in CVPR, 2017, pp. 1318‚Äì1327.
[59] M. Ye, C. Liang, Y. Yu, Z. Wang, Q. Leng, C. Xiao, J. Chen, and R. Hu, ‚ÄúPerson reidentiÔ¨Åcation via ranking aggregation of similarity pulling and dissimilarity pushing,‚Äù IEEE Transactions on Multimedia (TMM), vol. 18, no. 12, pp. 2553‚Äì2566, 2016.
[60] D. T. Nguyen, H. G. Hong, K. W. Kim, and K. R. Park, ‚ÄúPerson recognition system based on a combination of body images from visible light and thermal cameras,‚Äù Sensors, vol. 17, no. 3, p. 605, 2017.
[61] W.-H. Li, Z. Zhong, and W.-S. Zheng, ‚ÄúOne-pass person reidentiÔ¨Åcation by sketch online discriminant analysis,‚Äù Pattern Recognition, vol. 93, pp. 237‚Äì250, 2019.
[62] A. Wu, W.-S. Zheng, and J.-H. Lai, ‚ÄúRobust depth-based person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing (TIP), vol. 26, no. 6, pp. 2588‚Äì2603, 2017.
[63] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, and X. Wang, ‚ÄúPerson search with natural language description,‚Äù in CVPR, 2017, pp. 1345‚Äì 1353.
[64] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, ‚ÄúJoint detection and identiÔ¨Åcation feature learning for person search,‚Äù in CVPR, 2017, pp. 3415‚Äì3424.
[65] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, and J. Bu, ‚ÄúSemi-supervised coupled dictionary learning for person reidentiÔ¨Åcation,‚Äù in CVPR, 2014, pp. 3550‚Äì3557.
[66] R. Zhao, W. Ouyang, and X. Wang, ‚ÄúUnsupervised salience learning for person re-identiÔ¨Åcation,‚Äù in CVPR, 2013, pp. 3586‚Äì 3593.
[67] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, and J. Sun, ‚ÄúPerceive where to focus: Learning visibility-aware part-level features for partial person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 393‚Äì402.

[68] X. Wang, G. Doretto, T. Sebastian, J. Rittscher, and P. Tu, ‚ÄúShape and appearance context modeling,‚Äù in ICCV, 2007, pp. 1‚Äì8.
[69] H. Wang, X. Zhu, T. Xiang, and S. Gong, ‚ÄúTowards unsupervised open-set person re-identiÔ¨Åcation,‚Äù in ICIP, 2016, pp. 769‚Äì773.
[70] X. Zhu, B. Wu, D. Huang, and W.-S. Zheng, ‚ÄúFast open-world person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing (TIP), vol. 27, no. 5, pp. 2286 ‚Äì 2300, 2018.
[71] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, ‚ÄúDeep attributes driven multi-camera person re-identiÔ¨Åcation,‚Äù in ECCV, 2016, pp. 475‚Äì491.
[72] Y. Lin, L. Zheng, Z. Zheng, Y. Wu, and Y. Yang, ‚ÄúImproving person re-identiÔ¨Åcation by attribute and identity learning,‚Äù arXiv preprint arXiv:1703.07220, 2017.
[73] K. Liu, B. Ma, W. Zhang, and R. Huang, ‚ÄúA spatiotemporal appearance representation for viceo-based pedestrian re-identiÔ¨Åcation,‚Äù in ICCV, 2015, pp. 3810‚Äì3818.
[74] J. Dai, P. Zhang, D. Wang, H. Lu, and H. Wang, ‚ÄúVideo person reidentiÔ¨Åcation by temporal residual learning,‚Äù IEEE Transactions on Image Processing (TIP), vol. 28, no. 3, pp. 1366‚Äì1377, 2018.
[75] L. Zhao, X. Li, Y. Zhuang, and J. Wang, ‚ÄúDeeply-learned partaligned representations for person re-identiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 3219‚Äì3228.
[76] H. Yao, S. Zhang, R. Hong, Y. Zhang, C. Xu, and Q. Tian, ‚ÄúDeep representation learning with part loss for person reidentiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing (TIP), 2019.
[77] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, ‚ÄúBeyond part models: Person retrieval with reÔ¨Åned part pooling,‚Äù in ECCV, 2018, pp. 480‚Äì496.
[78] T. Matsukawa and E. Suzuki, ‚ÄúPerson re-identiÔ¨Åcation using cnn features learned from combination of attributes,‚Äù in ICPR, 2016, pp. 2428‚Äì2433.
[79] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556, 2014.
[80] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778.
[81] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, ‚ÄúJoint learning of single-image and cross-image representations for person reidentiÔ¨Åcation,‚Äù in CVPR, 2016, pp. 1288‚Äì1296.
[82] Y. Sun, L. Zheng, W. Deng, and S. Wang, ‚ÄúSvdnet for pedestrian retrieval,‚Äù in ICCV, 2017, pp. 3800‚Äì3808.
[83] M. Ye, X. Lan, and P. C. Yuen, ‚ÄúRobust anchor embedding for unsupervised video person re-identiÔ¨Åcation in the wild,‚Äù in ECCV, 2018, pp. 170‚Äì186.
[84] X. Qian, Y. Fu, Y.-G. Jiang, T. Xiang, and X. Xue, ‚ÄúMulti-scale deep learning architectures for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 5399‚Äì5408.
[85] F. Yang, K. Yan, S. Lu, H. Jia, X. Xie, and W. Gao, ‚ÄúAttention driven person re-identiÔ¨Åcation,‚Äù Pattern Recognition, vol. 86, pp. 143‚Äì155, 2019.
[86] W. Li, X. Zhu, and S. Gong, ‚ÄúHarmonious attention network for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 2285‚Äì2294.
[87] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang, ‚ÄúMancs: A multi-task attentional network with curriculum sampling for person re-identiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 365‚Äì381.
[88] Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang, ‚ÄúEnd-to-end deep kronecker-product matching for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 6886‚Äì6895.
[89] Y. Wang, Z. Chen, F. Wu, and G. Wang, ‚ÄúPerson re-identiÔ¨Åcation with cascaded pairwise convolutions,‚Äù in CVPR, 2018, pp. 1470‚Äì 1478.
[90] G. Chen, C. Lin, L. Ren, J. Lu, and J. Zhou, ‚ÄúSelf-critical attention learning for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 9637‚Äì 9646.
[91] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and G. Wang, ‚ÄúDual attention matching network for context-aware feature sequence based person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 5363‚Äì5372.
[92] M. Zheng, S. Karanam, Z. Wu, and R. J. Radke, ‚ÄúRe-identiÔ¨Åcation with consistent attentive siamese networks,‚Äù in CVPR, 2019, pp. 5735‚Äì5744.
[93] S. Zhou, F. Wang, Z. Huang, and J. Wang, ‚ÄúDiscriminative feature learning with consistent attention regularization for person reidentiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8040‚Äì8049.
[94] D. Chen, D. Xu, H. Li, N. Sebe, and X. Wang, ‚ÄúGroup consistent similarity learning via deep crf for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 8649‚Äì8658.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

18

[95] C. Luo, Y. Chen, N. Wang, and Z. Zhang, ‚ÄúSpectral feature transformation for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 4976‚Äì4985.
[96] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang, ‚ÄúA siamese long short-term memory architecture for human re-identiÔ¨Åcation,‚Äù in ECCV, 2016, pp. 135‚Äì153.
[97] Y. Suh, J. Wang, S. Tang, T. Mei, and K. Mu Lee, ‚ÄúPart-aligned bilinear representations for person re-identiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 402‚Äì419.
[98] L. Zhao, X. Li, Y. Zhuang, and J. Wang, ‚ÄúDeeply-learned partaligned representations for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 3219‚Äì3228.
[99] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, ‚ÄúPerson reidentiÔ¨Åcation by multi-channel parts-based cnn with improved triplet loss function,‚Äù in CVPR, 2016, pp. 1335‚Äì1344.
[100] D. Li, X. Chen, Z. Zhang, and K. Huang, ‚ÄúLearning deep context-aware features over body and latent parts for person reidentiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 384‚Äì393.
[101] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, and Q. Tian, ‚ÄúPose-driven deep convolutional model for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 3960‚Äì3969.
[102] J. Xu, R. Zhao, F. Zhu, H. Wang, and W. Ouyang, ‚ÄúAttentionaware compositional network for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 2119‚Äì2128.
[103] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, ‚ÄúDensely semantically aligned person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 667‚Äì676.
[104] J. Guo, Y. Yuan, L. Huang, C. Zhang, J.-G. Yao, and K. Han, ‚ÄúBeyond human parts: Dual part-aligned representations for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 3642‚Äì3651.
[105] X. Sun and L. Zheng, ‚ÄúDissecting person re-identiÔ¨Åcation from the viewpoint of viewpoint,‚Äù in CVPR, 2019, pp. 608‚Äì617.
[106] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, ‚ÄúInvariance matters: Exemplar memory for domain adaptive person reidentiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 598‚Äì607.
[107] B. N. Xia, Y. Gong, Y. Zhang, and C. Poellabauer, ‚ÄúSecondorder non-local attention networks for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 3760‚Äì3769.
[108] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, ‚ÄúInteraction-and-aggregation network for person reidentiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 9317‚Äì9326.
[109] C.-P. Tay, S. Roy, and K.-H. Yap, ‚ÄúAanet: Attribute attention network for person re-identiÔ¨Åcations,‚Äù in CVPR, 2019, pp. 7134‚Äì 7143.
[110] Y. Zhao, X. Shen, Z. Jin, H. Lu, and X.-s. Hua, ‚ÄúAttribute-driven feature disentangling and temporal aggregation for video person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 4913‚Äì4922.
[111] W. Jingya, Z. Xiatian, G. Shaogang, and L. Wei, ‚ÄúTransferable joint attribute-identity deep learning for unsupervised person reidentiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 2275‚Äì2284.
[112] X. Chang, T. M. Hospedales, and T. Xiang, ‚ÄúMulti-level factorisation net for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 2109‚Äì 2118.
[113] F. Liu and L. Zhang, ‚ÄúView confusion feature learning for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 6639‚Äì6648.
[114] Z. Zhu, X. Jiang, F. Zheng, X. Guo, F. Huang, W. Zheng, and X. Sun, ‚ÄúAware loss with angular regularization for person reidentiÔ¨Åcation,‚Äù in AAAI, 2020.
[115] J. Lin, L. Ren, J. Lu, J. Feng, and J. Zhou, ‚ÄúConsistent-aware deep learning for person re-identiÔ¨Åcation in a camera network,‚Äù in CVPR, 2017, pp. 5771‚Äì5780.
[116] J. Liu, B. Ni, Y. Yan, and et al., ‚ÄúPose transferrable person reidentiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 4099‚Äì4108.
[117] X. Qian, Y. Fu, T. Xiang, W. Wang, J. Qiu, Y. Wu, Y.-G. Jiang, and X. Xue, ‚ÄúPose-normalized image generation for person reidentiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 650‚Äì667.
[118] Z. Zhong, L. Zheng, Z. Zheng, and et al., ‚ÄúCamera style adaptation for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 5157‚Äì5166.
[119] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, and J. Kautz, ‚ÄúJoint discriminative and generative learning for person reidentiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 2138‚Äì2147.
[120] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, ‚ÄúImageimage domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 994‚Äì1003.
[121] Y.-J. Li, C.-S. Lin, Y.-B. Lin, and Y.-C. F. Wang, ‚ÄúCross-dataset person re-identiÔ¨Åcation via unsupervised pose disentanglement and adaptation,‚Äù in ICCV, 2019, pp. 7919‚Äì7929.

[122] H. Luo, W. Jiang, Y. Gu, F. Liu, X. Liao, S. Lai, and J. Gu, ‚ÄúA strong baseline and batch normneuralization neck for deep person reidentiÔ¨Åcation,‚Äù arXiv preprint arXiv:1906.08332, 2019.
[123] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, ‚ÄúRandom erasing data augmentation,‚Äù arXiv preprint arXiv:1708.04896, 2017.
[124] Z. Dai, M. Chen, X. Gu, S. Zhu, and P. Tan, ‚ÄúBatch dropblock network for person re-identiÔ¨Åcation and beyond,‚Äù in ICCV, 2019, pp. 3691‚Äì3701.
[125] S. Bak, P. Carr, and J.-F. Lalonde, ‚ÄúDomain adaptation through synthesis for unsupervised person re-identiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 189‚Äì205.
[126] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, ‚ÄúPerson reidentiÔ¨Åcation by descriptive and discriminative classiÔ¨Åcation,‚Äù in Image Analysis, 2011, pp. 91‚Äì102.
[127] N. McLaughlin, J. Martinez del Rincon, and P. Miller, ‚ÄúRecurrent convolutional network for video-based person re-identiÔ¨Åcation,‚Äù in CVPR, 2016, pp. 1325‚Äì1334.
[128] D. Chung, K. Tahboub, and E. J. Delp, ‚ÄúA two stream siamese convolutional neural network for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 1983‚Äì1991.
[129] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, and X. Yang, ‚ÄúPerson reidentiÔ¨Åcation via recurrent feature aggregation,‚Äù in ECCV, 2016, pp. 701‚Äì716.
[130] Z. Zhou, Y. Huang, W. Wang, L. Wang, and T. Tan, ‚ÄúSee the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 4747‚Äì4756.
[131] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, ‚ÄúJointly attentive spatial-temporal pooling networks for video-based person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 4733‚Äì4742.
[132] A. Subramaniam, A. Nambiar, and A. Mittal, ‚ÄúCo-segmentation inspired attention networks for video-based person reidentiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 562‚Äì572.
[133] S. Li, S. Bak, P. Carr, and X. Wang, ‚ÄúDiversity regularized spatiotemporal attention for video-based person re-identiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 369‚Äì378.
[134] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, ‚ÄúVideo person re-identiÔ¨Åcation with competitive snippet-similarity aggregation and co-attentive snippet embedding,‚Äù in CVPR, 2018, pp. 1169‚Äì 1178.
[135] Y. Fu, X. Wang, Y. Wei, and T. Huang, ‚ÄúSta: Spatial-temporal attention for large-scale video-based person re-identiÔ¨Åcation,‚Äù in AAAI, 2019.
[136] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, ‚ÄúGlobal-local temporal representations for video person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 3958‚Äì3967.
[137] Y. Guo and N.-M. Cheung, ‚ÄúEfÔ¨Åcient and deep person reidentiÔ¨Åcation using multi-level similarity,‚Äù in CVPR, 2018, pp. 2335‚Äì2344.
[138] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, ‚ÄúOmni-scale feature learning for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 3702‚Äì 3712.
[139] R. Quan, X. Dong, Y. Wu, L. Zhu, and Y. Yang, ‚ÄúAuto-reid: Searching for a part-aware convnet for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 3750‚Äì3759.
[140] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, and X. Wang, ‚ÄúEliminating background-bias for robust person reidentiÔ¨Åcation,‚Äù in CVPR, 2018, pp. 5794‚Äì5803.
[141] Z. Zheng, L. Zheng, and Y. Yang, ‚ÄúA discriminatively learned cnn embedding for person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:1611.05666, 2016.
[142] M. Ye, X. Lan, Z. Wang, and P. C. Yuen, ‚ÄúBi-directional center-constrained top-ranking for visible thermal person reidentiÔ¨Åcation,‚Äù IEEE Transactions on Information Forensics and Security (TIFS), vol. 15, pp. 407‚Äì419, 2020.
[143] P. MoutaÔ¨Ås, M. Leng, and I. A. Kakadiaris, ‚ÄúAn overview and empirical comparison of distance metric learning methods,‚Äù IEEE Transactions on Cybernetics, vol. 47, no. 3, pp. 612‚Äì625, 2016.
[144] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang, ‚ÄúExploit the unknown gradually: One-shot video-based person reidentiÔ¨Åcation by stepwise learning,‚Äù in CVPR, 2018, pp. 5177‚Äì 5186.
[145] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, ‚ÄúUnsupervised embedding learning via invariant and spreading instance feature,‚Äù in CVPR, 2019, pp. 6210‚Äì6219.
[146] N. Wojke and A. Bewley, ‚ÄúDeep cosine metric learning for person re-identiÔ¨Åcation,‚Äù in WACV, 2018, pp. 748‚Äì756.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

19

[147] X. Fan, W. Jiang, H. Luo, and M. Fei, ‚ÄúSpherereid: Deep hypersphere manifold embedding for person re-identiÔ¨Åcation,‚Äù JVCIR, vol. 60, pp. 51‚Äì58, 2019.
[148] R. Mu¬® ller, S. Kornblith, and G. Hinton, ‚ÄúWhen does label smoothing help?‚Äù arXiv preprint arXiv:1906.02629, 2019.
[149] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, and S. Z. Li, ‚ÄúEmbedding deep metric for person re-identiÔ¨Åcation: A study against large variations,‚Äù in ECCV, 2016, pp. 732‚Äì748.
[150] S. Zhou, J. Wang, J. Wang, Y. Gong, and N. Zheng, ‚ÄúPoint to set similarity based deep feature learning for person reidentiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 3741‚Äì3750.
[151] R. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, and X. Bai, ‚ÄúHard-aware point-to-set deep metric for person re-identiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 188‚Äì204.
[152] W. Chen, X. Chen, J. Zhang, and K. Huang, ‚ÄúBeyond triplet loss: a deep quadruplet network for person re-identiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 403‚Äì412.
[153] Q. Yang, H.-X. Yu, A. Wu, and W.-S. Zheng, ‚ÄúPatch-based discriminative feature learning for unsupervised person reidentiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 3633‚Äì3642.
[154] Z. Liu, J. Wang, S. Gong, H. Lu, and D. Tao, ‚ÄúDeep reinforcement active learning for human-in-the-loop person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 6122‚Äì6131.
[155] J. Zhou, B. Su, and Y. Wu, ‚ÄúEasy identiÔ¨Åcation from better constraints: Multi-shot person re-identiÔ¨Åcation from reference constraints,‚Äù in CVPR, 2018, pp. 5373‚Äì5381.
[156] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, F. Huang, and R. Ji, ‚ÄúPyramidal person re-identiÔ¨Åcation via multi-loss dynamic training,‚Äù in CVPR, 2019, pp. 8514‚Äì8522.
[157] M. Ye, C. Liang, Z. Wang, Q. Leng, and J. Chen, ‚ÄúRanking optimization for person re-identiÔ¨Åcation via similarity and dissimilarity,‚Äù in ACM Multimedia (ACM MM), 2015, pp. 1239‚Äì1242.
[158] C. Liu, C. Change Loy, S. Gong, and G. Wang, ‚ÄúPop: Person reidentiÔ¨Åcation post-rank optimisation,‚Äù in ICCV, 2013, pp. 441‚Äì 448.
[159] H. Wang, S. Gong, X. Zhu, and T. Xiang, ‚ÄúHuman-in-the-loop person re-identiÔ¨Åcation,‚Äù in ECCV, 2016, pp. 405‚Äì422.
[160] S. Paisitkriangkrai, C. Shen, and A. Van Den Hengel, ‚ÄúLearning to rank in person re-identiÔ¨Åcation with metric ensembles,‚Äù in CVPR, 2015, pp. 1846‚Äì1855.
[161] S. Bai, P. Tang, P. H. Torr, and L. J. Latecki, ‚ÄúRe-ranking via metric fusion for object retrieval and person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 740‚Äì749.
[162] S. Bai, X. Bai, and Q. Tian, ‚ÄúScalable person re-identiÔ¨Åcation on supervised smoothed manifold,‚Äù in CVPR, 2017, pp. 2530‚Äì2539.
[163] A. J. Ma and P. Li, ‚ÄúQuery based adaptive re-ranking for person re-identiÔ¨Åcation,‚Äù in ACCV, 2014, pp. 397‚Äì412.
[164] J. Zhou, P. Yu, W. Tang, and Y. Wu, ‚ÄúEfÔ¨Åcient online local metric adaptation via negative samples for person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 2420‚Äì2428.
[165] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu, and Q. Tian, ‚ÄúQuery-adaptive late fusion for image search and person reidentiÔ¨Åcation,‚Äù in CVPR, 2015, pp. 1741‚Äì1750.
[166] A. Barman and S. K. Shah, ‚ÄúShape: A novel graph theoretic algorithm for making consensus-based decisions in person reidentiÔ¨Åcation systems,‚Äù in ICCV, 2017, pp. 1115‚Äì1124.
[167] W.-S. Zheng, S. Gong, and T. Xiang, ‚ÄúAssociating groups of people,‚Äù in BMVC, 2009, pp. 1‚Äì23.
[168] C. C. Loy, C. Liu, and S. Gong, ‚ÄúPerson re-identiÔ¨Åcation by manifold ranking,‚Äù in ICIP, 2013, pp. 3567‚Äì3571.
[169] M. Gou, Z. Wu, A. Rates-Borras, O. Camps, R. J. Radke et al., ‚ÄúA systematic evaluation and benchmark for person reidentiÔ¨Åcation: Features, metrics, and datasets,‚Äù IEEE TPAMI, vol. 41, no. 3, pp. 523‚Äì536, 2018.
[170] M. Li, X. Zhu, and S. Gong, ‚ÄúUnsupervised person reidentiÔ¨Åcation by deep learning tracklet association,‚Äù in ECCV, 2018, pp. 737‚Äì753.
[171] G. Song, B. Leng, Y. Liu, C. Hetang, and S. Cai, ‚ÄúRegionbased quality estimation network for large-scale person reidentiÔ¨Åcation,‚Äù in AAAI, 2018, pp. 7347‚Äì7354.
[172] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, ‚ÄúLearning discriminative features with multiple granularities for person reidentiÔ¨Åcation,‚Äù in ACM MM, 2018, pp. 274‚Äì282.
[173] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang, ‚ÄúAbd-net: Attentive but diverse person reidentiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8351‚Äì8361.

[174] B. Chen, W. Deng, and J. Hu, ‚ÄúMixed high-order attention network for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 371‚Äì381.
[175] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun, ‚ÄúAlignedreid: Surpassing humanlevel performance in person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:1711.08184, 2017.
[176] M. Ye, A. J. Ma, L. Zheng, J. Li, and P. C. Yuen, ‚ÄúDynamic label graph matching for unsupervised video re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 5142‚Äì5150.
[177] Z. Wang, S. Zheng, M. Song, Q. Wang, A. Rahimpour, and H. Qi, ‚Äúadvpattern: Physical-world attacks on deep person reidentiÔ¨Åcation via adversarially transformable patterns,‚Äù in ICCV, 2019, pp. 8341‚Äì8350.
[178] J. Zhang, N. Wang, and L. Zhang, ‚ÄúMulti-shot pedestrian reidentiÔ¨Åcation via sequential decision making,‚Äù in CVPR, 2018, pp. 6781‚Äì6789.
[179] A. Haque, A. Alahi, and L. Fei-Fei, ‚ÄúRecurrent attention models for depth-based person identiÔ¨Åcation,‚Äù in CVPR, 2016, pp. 1229‚Äì 1238.
[180] N. Karianakis, Z. Liu, Y. Chen, and S. Soatto, ‚ÄúReinforced temporal attention and split-rate transfer for depth-based person reidentiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 715‚Äì733.
[181] I. B. Barbosa, M. Cristani, A. Del Bue, L. Bazzani, and V. Murino, ‚ÄúRe-identiÔ¨Åcation with rgb-d sensors,‚Äù in ECCV Workshop, 2012, pp. 433‚Äì442.
[182] D. Chen, H. Li, X. Liu, Y. Shen, J. Shao, Z. Yuan, and X. Wang, ‚ÄúImproving deep visual representation for person reidentiÔ¨Åcation by global and local image-language association,‚Äù in ECCV, 2018, pp. 54‚Äì70.
[183] Y. Zhang and H. Lu, ‚ÄúDeep cross-modal projection learning for image-text matching,‚Äù in ECCV, 2018, pp. 686‚Äì701.
[184] J. Liu, Z.-J. Zha, R. Hong, M. Wang, and Y. Zhang, ‚ÄúDeep adversarial graph attention convolution network for text-based person search,‚Äù in ACM MM, 2019, pp. 665‚Äì673.
[185] M. Ye, Z. Wang, X. Lan, and P. C. Yuen, ‚ÄúVisible thermal person re-identiÔ¨Åcation via dual-constrained top-ranking,‚Äù in IJCAI, 2018, pp. 1092‚Äì1099.
[186] M. Ye, X. Lan, J. Li, and P. C. Yuen, ‚ÄúHierarchical discriminative learning for visible thermal person re-identiÔ¨Åcation,‚Äù in AAAI, 2018, pp. 7501‚Äì7508.
[187] Y. Hao, N. Wang, J. Li, and X. Gao, ‚ÄúHsme: Hypersphere manifold embedding for visible thermal person re-identiÔ¨Åcation,‚Äù in AAAI, 2019, pp. 8385‚Äì8392.
[188] M. Ye, J. Shen, and L. Shao, ‚ÄúVisible-infrared person reidentiÔ¨Åcation via homogeneous augmented tri-modal learning,‚Äù IEEE TIFS, 2020.
[189] Z. Wang, Z. Wang, Y. Zheng, Y.-Y. Chuang, and S. Satoh, ‚ÄúLearning to reduce dual-level discrepancy for infrared-visible person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 618‚Äì626.
[190] G. Wang, T. Zhang, J. Cheng, S. Liu, Y. Yang, and Z. Hou, ‚ÄúRgbinfrared cross-modality person re-identiÔ¨Åcation via joint pixel and feature alignment,‚Äù in ICCV, 2019, pp. 3623‚Äì3632.
[191] S. Choi, S. Lee, Y. Kim, T. Kim, and C. Kim, ‚ÄúHi-cmd: Hierarchical cross-modality disentanglement for visible-infrared person reidentiÔ¨Åcation,‚Äù in CVPR, 2020, pp. 10 257‚Äì10 266.
[192] M. Ye, J. Shen, D. J. Crandall, L. Shao, and J. Luo, ‚ÄúDynamic dual-attentive aggregation learning for visible-infrared person reidentiÔ¨Åcation,‚Äù in ECCV, 2020.
[193] Z. Wang, M. Ye, F. Yang, X. Bai, and S. Satoh, ‚ÄúCascaded sr-gan for scale-adaptive low resolution person re-identiÔ¨Åcation.‚Äù in IJCAI, 2018, pp. 3891‚Äì3897.
[194] Y.-J. Li, Y.-C. Chen, Y.-Y. Lin, X. Du, and Y.-C. F. Wang, ‚ÄúRecover and identify: A generative dual model for cross-resolution person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8090‚Äì8099.
[195] H. Liu, J. Feng, Z. Jie, K. Jayashree, B. Zhao, M. Qi, J. Jiang, and S. Yan, ‚ÄúNeural person search machines,‚Äù in ICCV, 2017, pp. 493‚Äì 501.
[196] Y. Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, and X. Yang, ‚ÄúLearning context graph for person search,‚Äù in CVPR, 2019, pp. 2158‚Äì2167.
[197] B. Munjal, S. Amin, F. Tombari, and F. Galasso, ‚ÄúQuery-guided end-to-end person search,‚Äù in CVPR, 2019, pp. 811‚Äì820.
[198] C. Han, J. Ye, Y. Zhong, X. Tan, C. Zhang, C. Gao, and N. Sang, ‚ÄúRe-id driven localization reÔ¨Ånement for person search,‚Äù in ICCV, 2019, pp. 9814‚Äì9823.
[199] X. Lan, H. Wang, S. Gong, and X. Zhu, ‚ÄúDeep reinforcement learning attention selection for person re-identiÔ¨Åcation,‚Äù in BMVC, 2017.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

20

[200] M. Yamaguchi, K. Saito, Y. Ushiku, and T. Harada, ‚ÄúSpatiotemporal person retrieval via natural language queries,‚Äù in ICCV, 2017, pp. 1453‚Äì1462.
[201] S. Tang, M. Andriluka, B. Andres, and B. Schiele, ‚ÄúMultiple people tracking by lifted multicut and person re-identiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 3539‚Äì3548.
[202] Y. Hou, L. Zheng, Z. Wang, and S. Wang, ‚ÄúLocality aware appearance metric for multi-target multi-camera tracking,‚Äù arXiv preprint arXiv:1911.12037, 2019.
[203] E. Kodirov, T. Xiang, Z. Fu, and S. Gong, ‚ÄúPerson re-identiÔ¨Åcation by unsupervised l1 graph learning,‚Äù in ECCV, 2016, pp. 178‚Äì195.
[204] Z. Liu, D. Wang, and H. Lu, ‚ÄúStepwise metric promotion for unsupervised video person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 2429‚Äì2438.
[205] H. Fan, L. Zheng, and Y. Yang, ‚ÄúUnsupervised person re-identiÔ¨Åcation: Clustering and Ô¨Åne-tuning,‚Äù arXiv preprint arXiv:1705.10444, 2017.
[206] M. Ye, J. Li, A. J. Ma, L. Zheng, and P. C. Yuen, ‚ÄúDynamic graph co-matching for unsupervised video-based person reidentiÔ¨Åcation,‚Äù IEEE TIP, vol. 28, no. 6, pp. 2976‚Äì2990, 2019.
[207] X. Wang, R. Panda, M. Liu, Y. Wang, and et al., ‚ÄúExploiting global camera network constraints for unsupervised video person reidentiÔ¨Åcation,‚Äù arXiv preprint arXiv:1908.10486, 2019.
[208] K. Zeng, M. Ning, Y. Wang, and Y. Guo, ‚ÄúHierarchical clustering with hard-batch triplet loss for person re-identiÔ¨Åcation,‚Äù in CVPR, 2020, pp. 13 657‚Äì13 665.
[209] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, and J.-H. Lai, ‚ÄúUnsupervised person re-identiÔ¨Åcation by soft multilabel learning,‚Äù in CVPR, 2019, pp. 2148‚Äì2157.
[210] A. Wu, W.-S. Zheng, and J.-H. Lai, ‚ÄúUnsupervised person reidentiÔ¨Åcation by camera-aware similarity consistency learning,‚Äù in ICCV, 2019, pp. 6922‚Äì6931.
[211] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, and S. Z. Li, ‚ÄúUnsupervised graph association for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8321‚Äì8330.
[212] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. S. Huang, ‚ÄúSelf-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 6112‚Äì6121.
[213] S. Bak and P. Carr, ‚ÄúOne-shot metric learning for person reidentiÔ¨Åcation,‚Äù in CVPR, 2017, pp. 2990‚Äì2999.
[214] X. Wang, S. Paul, D. S. Raychaudhuri, and at al., ‚ÄúLearning person re-identiÔ¨Åcation models from videos with weak supervision,‚Äù arXiv preprint arXiv:2007.10631, 2020.
[215] Z. Zhong, L. Zheng, S. Li, and Y. Yang, ‚ÄúGeneralizing a person retrieval model hetero-and homogeneously,‚Äù in ECCV, 2018, pp. 172‚Äì188.
[216] J. Liu, Z.-J. Zha, D. Chen, R. Hong, and M. Wang, ‚ÄúAdaptive transfer network for cross-domain person re-identiÔ¨Åcation,‚Äù in CVPR, 2019, pp. 7202‚Äì7211.
[217] Y. Huang, Q. Wu, J. Xu, and Y. Zhong, ‚ÄúSbsgan: Suppression of inter-domain background shift for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 9527‚Äì9536.
[218] Y. Chen, X. Zhu, and S. Gong, ‚ÄúInstance-guided context rendering for cross-domain person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 232‚Äì242.
[219] Y. Ge, D. Chen, and H. Li, ‚ÄúMutual mean-teaching: Pseudo label reÔ¨Ånery for unsupervised domain adaptation on person reidentiÔ¨Åcation,‚Äù in ICLR, 2020.
[220] Y. Wang, S. Liao, and L. Shao, ‚ÄúSurpassing real-world source training data: Random 3d characters for generalizable person reidentiÔ¨Åcation,‚Äù in ACM MM, 2020, pp. 3422‚Äì3430.
[221] L. Qi, L. Wang, J. Huo, L. Zhou, Y. Shi, and Y. Gao, ‚ÄúA novel unsupervised camera-aware domain adaptation framework for person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8080‚Äì8089.
[222] X. Zhang, J. Cao, C. Shen, and M. You, ‚ÄúSelf-training with progressive augmentation for unsupervised cross-domain person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8222‚Äì8231.
[223] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, ‚ÄúSelf-paced contrastive learning with hybrid memory for domain adaptive object re-id,‚Äù in NeurIPS, 2020.
[224] J. Lv, W. Chen, Q. Li, and C. Yang, ‚ÄúUnsupervised cross-dataset person re-identiÔ¨Åcation by transfer learning of spatial-temporal patterns,‚Äù in CVPR, 2018, pp. 7948‚Äì7956.
[225] S. Liao and L. Shao, ‚ÄúInterpretable and generalizable person re-identiÔ¨Åcation with query-adaptive convolution and temporal lifting,‚Äù in ECCV, 2020.

[226] H.-X. Yu, A. Wu, and W.-S. Zheng, ‚ÄúCross-view asymmetric metric learning for unsupervised person re-identiÔ¨Åcation,‚Äù in ICCV, 2017, pp. 994‚Äì1002.
[227] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, ‚ÄúStyle normalization and restitution for generalizable person re-identiÔ¨Åcation,‚Äù in CVPR, 2020, pp. 3143‚Äì3152.
[228] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y. Tian, ‚ÄúMultiple expert brainstorming for domain adaptive person re-identiÔ¨Åcation,‚Äù in ECCV, 2020.
[229] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, ‚ÄúMomentum contrast for unsupervised visual representation learning,‚Äù in CVPR, 2020.
[230] M. Ye, J. Shen, X. Zhang, P. C. Yuen, and S.-F. Chang, ‚ÄúAugmentation invariant and instance spreading feature for softmax embedding,‚Äù IEEE TPAMI, 2020.
[231] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, and S. Gong, ‚ÄúPartial person re-identiÔ¨Åcation,‚Äù in ICCV, 2015, pp. 4678‚Äì4686.
[232] L. He, J. Liang, H. Li, and Z. Sun, ‚ÄúDeep spatial feature reconstruction for partial person re-identiÔ¨Åcation: Alignment-free approach,‚Äù in CVPR, 2018, pp. 7073‚Äì7082.
[233] L. He, Y. Wang, W. Liu, X. Liao, H. Zhao, Z. Sun, and J. Feng, ‚ÄúForeground-aware pyramid reconstruction for alignment-free occluded person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 8450‚Äì8459.
[234] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, ‚ÄúPose-guided feature alignment for occluded person re-identiÔ¨Åcation,‚Äù in ICCV, 2019, pp. 542‚Äì551.
[235] T. Yu, D. Li, Y. Yang, T. Hospedales, and T. Xiang, ‚ÄúRobust person re-identiÔ¨Åcation by modelling feature uncertainty,‚Äù in ICCV, 2019, pp. 552‚Äì561.
[236] M. Ye and P. C. Yuen, ‚ÄúPurifynet: A robust person reidentiÔ¨Åcation model with noisy labels,‚Äù IEEE TIFS, 2020.
[237] X. Li, A. Wu, and W.-S. Zheng, ‚ÄúAdversarial open-world person re-identiÔ¨Åcation,‚Äù in ECCV, 2018, pp. 280‚Äì296.
[238] M. Golfarelli, D. Maio, and D. Malton, ‚ÄúOn the error-reject tradeoff in biometric veriÔ¨Åcation systems,‚Äù IEEE TPAMI, vol. 19, no. 7, pp. 786‚Äì796, 1997.
[239] G. Lisanti, N. Martinel, A. Del Bimbo, and G. Luca Foresti, ‚ÄúGroup re-identiÔ¨Åcation via unsupervised transfer of sparse features encoding,‚Äù in ICCV, 2017, pp. 2449‚Äì2458.
[240] Y. Cai, V. Takala, and M. Pietikainen, ‚ÄúMatching groups of people by covariance descriptor,‚Äù in ICPR, 2010, pp. 2744‚Äì2747.
[241] H. Xiao, W. Lin, B. Sheng, K. Lu, J. Yan, and et al., ‚ÄúGroup reidentiÔ¨Åcation: Leveraging and integrating multi-grain information,‚Äù in ACM MM, 2018, pp. 192‚Äì200.
[242] Z. Huang, Z. Wang, W. Hu, C.-W. Lin, and S. Satoh, ‚ÄúDotgnn: Domain-transferred graph neural network for group reidentiÔ¨Åcation,‚Äù in ACM MM, 2019, pp. 1888‚Äì1896.
[243] Y. Shen, H. Li, S. Yi, D. Chen, and X. Wang, ‚ÄúPerson reidentiÔ¨Åcation with deep similarity-guided graph neural network,‚Äù in ECCV, 2018, pp. 486‚Äì504.
[244] R. Panda, A. Bhuiyan, V. Murino, and A. K. Roy-Chowdhury, ‚ÄúUnsupervised adaptive re-identiÔ¨Åcation in open world dynamic camera networks,‚Äù in CVPR, 2017, pp. 7054‚Äì7063.
[245] S. M. Assari, H. Idrees, and M. Shah, ‚ÄúHuman re-identiÔ¨Åcation in crowd videos using personal, social and environmental constraints,‚Äù in ECCV, 2016, pp. 119‚Äì136.
[246] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural networks,‚Äù in CVPR, 2018, pp. 7794‚Äì7803.
[247] F. Radenovic¬¥, G. Tolias, and O. Chum, ‚ÄúFine-tuning cnn image retrieval with no human annotation,‚Äù IEEE TPAMI, vol. 41, no. 7, pp. 1655‚Äì1668, 2018.
[248] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott, ‚ÄúMultisimilarity loss with general pair weighting for deep metric learning,‚Äù in CVPR, 2019, pp. 5022‚Äì5030.
[249] L. He, Z. Sun, Y. Zhu, and Y. Wang, ‚ÄúRecognizing partial biometric patterns.‚Äù arXiv preprint arXiv:1810.07399, 2018.
[250] J. Xue, Z. Meng, K. Katipally, H. Wang, and K. van Zon, ‚ÄúClothing change aware person identiÔ¨Åcation,‚Äù in CVPR Workshops, 2018, pp. 2112‚Äì2120.
[251] F. Wan, Y. Wu, X. Qian, and Y. Fu, ‚ÄúWhen person re-identiÔ¨Åcation meets changing clothes,‚Äù arXiv preprint arXiv:2003.04070, 2020.
[252] S. Roy, S. Paul, N. E. Young, and A. K. Roy-Chowdhury, ‚ÄúExploiting transitivity for learning person re-identiÔ¨Åcation models on a budget,‚Äù in CVPR, 2018, pp. 7064‚Äì7072.
[253] Z. Zheng and Y. Yang, ‚ÄúPerson re-identiÔ¨Åcation in the 3d space,‚Äù arXiv preprint arXiv:2006.04569, 2020.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

1

[254] Y. Hu, D. Yi, S. Liao, Z. Lei, and S. Z. Li, ‚ÄúCross dataset person re-identiÔ¨Åcation,‚Äù in ACCV, 2014, pp. 650‚Äì664.
[255] G. Wu and S. Gong, ‚ÄúDecentralised learning from independent multi-domain labels for person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:2006.04150, 2020.
[256] P. Bhargava, ‚ÄúIncremental learning in person re-identiÔ¨Åcation,‚Äù arXiv preprint arXiv:1808.06281, 2018.
[257] F. Zhu, X. Kong, L. Zheng, H. Fu, and Q. Tian, ‚ÄúPart-based deep hashing for large-scale person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing (TIP), vol. 26, no. 10, pp. 4806‚Äì4817, 2017.
[258] J. Chen, Y. Wang, J. Qin, L. Liu, and L. Shao, ‚ÄúFast person reidentiÔ¨Åcation via cross-camera semantic binary transformation,‚Äù in CVPR, 2017, pp. 3873‚Äì3882.
[259] G. Wang, S. Gong, J. Cheng, and Z. Hou, ‚ÄúFaster person reidentiÔ¨Åcation,‚Äù in ECCV, 2020, pp. 275‚Äì292.
[260] A. Wu, W.-S. Zheng, X. Guo, and J.-H. Lai, ‚ÄúDistilled person reidentiÔ¨Åcation: Towards a more scalable system,‚Äù in CVPR, 2019, pp. 1187‚Äì1196.
[261] M. Ye, X. Lan, and Q. Leng, ‚ÄúModality-aware collaborative learning for visible thermal person re-identiÔ¨Åcation,‚Äù in ACM MM, 2019, pp. 347‚Äì355.
[262] Z. Feng, J. Lai, and X. Xie, ‚ÄúLearning modality-speciÔ¨Åc representations for visible-infrared person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Image Processing (TIP), vol. 29, pp. 579‚Äì590, 2020.
[263] D. Li, X. Wei, X. Hong, and Y. Gong, ‚ÄúInfrared-visible cross-modal person re-identiÔ¨Åcation with an x modality.‚Äù in AAAI, 2020, pp. 4610‚Äì4617.
[264] Y. Lu, Y. Wu, B. Liu, T. Zhang, B. Li, Q. Chu, and N. Yu, ‚ÄúCrossmodality person re-identiÔ¨Åcation with shared-speciÔ¨Åc feature transfer,‚Äù in CVPR, 2020, pp. 13 379‚Äì13 389.
Supplemental Materials:
This supplementary material accompanies our main
manuscript with the implementation details and more com-
prehensive experiments. We Ô¨Årst present the experiments
on two single-modality closed-world Re-ID tasks, including
image-based Re-ID on four datasets in Section A and video-
based Re-ID on four datasets in Section B. Then we intro-
duce the comprehensive comparison on two open-world
Re-ID tasks, including visible-infrared cross-modality Re-
ID on two datasets in Section C and partial Re-ID on two
datasets in Section D. In addition, a structure overview for
our survey is Ô¨Ånally summarized.

Non-local Attention. The ResNet contains 4 residual stages, i.e. conv2 x, conv3 x, conv4 x and conv5 x, each containing stacks of bottleneck residual blocks. We inserted Ô¨Åve non-local blocks after conv3 3, conv3 4, conv4 4, conv4 5 and conv4 6 respectively. We adopt the Dot Product version of non-local block with a bottleneck of 512 channels in our experiment. For each non-local block, a BatchNorm layer is added right after the last linear layer that represents Wz. The afÔ¨Åne parameter of this BatchNorm layer is initialized as zeros to ensure that the non-local block can be inserted into any pre-trained networks while maintaining its initial behavior.
Training Strategy. In the training stage, we randomly sample 16 identities and 4 images for each identity to form a mini-batch of size 64. Each image is resized into 256 √ó 128 pixels, padding 10 pixels with zero values, and then randomly cropped into 256 √ó 128 pixels. Random horizontally Ô¨Çipping and random erasing with 0.5 probability respectively are also adopted for data augmentation. SpeciÔ¨Åcally, random erasing augmentation [123] randomly selects a rectangle region with area ratio re to the whole image, and erase its pixels with the mean value of the image. Besides, the aspect ratio of this region is randomly initialized between r1 and r2. In our method, we set the above hyper-parameter as 0.02 < re < 0.4, r1 = 0.3 and r2 = 3.33. At last, we normalize the RGB channels of each image with mean 0.485, 0.456, 0.406 and stand deviation 0.229, 0.224, 0.225, respectively, which are the same with settings in [122].
Training Loss. In the training stage, three types of loss are combined for optimization, including identity classiÔ¨Åcation loss (Lid), center loss (Lct) and our proposed weighted regularization triplet loss (Lwrt).

Ltotal = Lid + Œ≤1Lct + Œ≤2Lwrt.

(R1)

A. Experiments on Single-modality Image-based Re-ID
Architecture Design. The overall structure4 of our proposed AGW baseline for single-modality Re-ID is illustrated in ¬ß 4 (Fig. R1). We adopt ResNet50 pre-trained on ImageNet as our backbone network and change the dimension of the fully connected layer to be consistent with the number of identities in the training dataset. The stride of the last spatial down-sampling operation in the backbone network is changed from 2 to 1. Consequently, the spatial size of the output feature map is changed from 8 √ó 4 to 16 √ó 8, when feeding an image of resolution 256 √ó 128 as input. In our method, we replace the Global Average Pooling in the original ResNet50 with the Generalized-mean (GeM) pooling. The pooling hyper parameter pk for generalized-mean pooling is initialized as 3.0. A BatchNorm layer, named BNNeck is plugged between the GeM pooling layer and the fully connected layer. The output of the GeM pooling layer is adopted for computing center loss and triplet loss in the training stage, while the feature after BNNeck is used for computing distance between pedestrian images during testing inference stage.
4. https://github.com/mangye16/ReID-Survey

The balanced weight of the center loss (Œ≤1) is set to 0.0005 and the one (Œ≤2) of the weighted regularized triplet loss is set to 1.0. Label smoothing is adopted to improve the original identity classiÔ¨Åcation loss, which encourages the model to be less conÔ¨Ådent during training and prevent overÔ¨Åtting for classiÔ¨Åcation task. Concretely, it changes the one-hot label as follow:

qi =

1

‚àí

N ‚àí1 N

Œµ

if i = y

Œµ/N

otherwise ,

(R2)

where N is the total number of identities, is a small constant to reduce the conÔ¨Ådence for the true identity label y and qi is treated as a new classiÔ¨Åcation target for training. In our method, we set to be 0.1.
Optimizer Setting. Adam optimizer with a weight decay 0.0005 is adopted to train our model. The initial learning rate is set as 0.00035 and is decreased by 0.1 at the 40th epoch and 70th epoch, respectively. The model is trained for 120 epochs in total. Besides, a warm-up learning rate scheme is also employed to improve the stability of training process and bootstrap the network for better performance. SpeciÔ¨Åcally, in the Ô¨Årst 10 epochs, the learning rate is linearly

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

2
Feature

1 √ó 1 ùëêùëúùëõùë£

ResNet50 Backbone

Generalized Mean Pooling
(2) GeM
ùë§1ùëù

BN
ID loss
ùë§2ùëù

1 √ó 1 ùëêùëúùëõùë£

1 √ó 1 ùëêùëúùëõùë£

Z

X

1 √ó 1 ùëêùëúùëõùë£

(1) Non-local Attention

(3) Weighted Regularization Triplet

Fig. R1: The framework of the proposed AGW baseline for single-modality image-based Re-ID.

TABLE R1: Comparison with the state-of-the-arts on four video-based Re-ID datasets, including MARS, DukeVideo, PRID2011 and iLIDS-VID. Rank-1, -5, -10 accuracy (%), mAP (%) and mINP (%) are reported.

Method

Venue

MARS

DukeVideo

PRID2011

iLIDS-VID

R1 R5 mAP mINP R1 R5 mAP mINP R1 R5 R20 mINP R1 R5 R20 mINP

ETAP [144]

CVPR18 80.7 92.0 67.3 - 83.6 94.5 78.3 -

-

-

-

-

-

-

-

-

DRSA [133]

CVPR18 82.3 - 65.8 -

-

-

-

- 93.2 -

-

- 80.2 -

-

-

Snippet [134] CVPR18 86.3 94.7 76.1 -

-

-

-

- 93.0 99.3 -

- 85.4 96.7 -

-

STA [135]

AAAI18 86.3 95.7 80.8 - 96.2 99.3 94.9 -

-

-

-

-

-

-

-

-

VRSTC [20]

CVPR19 88.5 96.5 82.3 - 95.0 99.1 93.5 -

-

-

-

- 83.4 95.5 99.5 -

ADFD [110]

CVPR19 87.0 95.4 78.2

-

-

-

-

- 93.9 99.5 100 - 86.3 97.4 99.7 -

GLTR [136]

ICCV19

87.0 95.7 78.4

-

96.2 99.3 93.7

-

95.5 100.0 -

- 86.0 98.0 -

-

CoSeg [132]

ICCV19

84.9 95.5 79.9 57.8 95.4 99.3 94.1 89.8 -

-

-

- 79.6 95.3 99.3 -

BagTricks [122] CVPR19W 85.8 95.2 81.6 62.0 92.6 98.9 92.4 88.3 84.3 93.3 98.0 88.5 74.0 93.3 99.1 82.2

AGW

-

87.0 95.7 82.2 62.8 94.6 99.1 93.4 89.2 87.8 96.6 98.9 91.7 78.0 97.0 99.5 85.5

AGW+

-

87.6 85.8 83.0 63.9 95.4 99.3 94.9 91.9 94.4 98.4 100 95.4 83.2 98.3 99.7 89.0

increased from 3.5 √ó 10‚àí5 to 3.5 √ó 10‚àí4. The learning rate lr(t) at epoch t can be computed as:

Ô£±
Ô£¥ Ô£¥ Ô£≤ lr(t) =

3.5 √ó 10‚àí5 √ó 3.5 √ó 10‚àí4 3.5 √ó 10‚àí5

t 10

if t ‚â§ 10 if 10 < t ‚â§ 40 if 40 < t ‚â§ 70

(R3)

Ô£¥

Ô£¥ Ô£≥

3.5 √ó 10‚àí6

if 70 < t ‚â§ 120.

B. Experiments on Video-based Re-ID
Implementation Details. We extend our proposed AGW baseline to a video-based Re-ID model by several minor changes to the backbone structure and training strategy of single-modality image-based Re-ID model. The videobased AGW baseline takes a video sequence as input and extracts the frame-level feature vectors, which are then averaged to be a video-level feature vector before the BNNeck layer. Besides, the video-based AGW baseline is trained for 400 epochs totally to better Ô¨Åt the video person Re-ID datasets. The learning rate is decayed by 10 times every 100 epochs. To form an input video sequence, we adopt the constraint random sampling strategy [133] to sample 4 frames as a summary for the original pedestrian tracklet. The BagTricks [122] baseline is extended to a video-based Re-ID model in the same way as AGW baseline for fair comparison. In addition, we also develop a variant of AGW baseline, termed as AGW+, to model more abundant temporal information in a pedestrian tracklet. AGW+ baseline adopts the dense sampling strategy to form an input video sequence in the testing stage. Dense sampling strategy takes all the frames in a pedestrian tracklet to form input video

sequence, resulting better performance but higher computational cost. To further improve the performance of AGW+ baseline on video re-ID datasets, we also remove the warmup learning rate strategy and add dropout operation before the linear classiÔ¨Åcation layer.
Detailed Comparison. In this section, we conduct the performance comparison between AGW baseline and other state-of-the-art video-based person Re-ID methods, including ETAP [144], DRSA [133], STA [135] Snippet [134], VRSTC [20], ADFD [110], GLTR [136] and CoSeg [132]. The comparison results on four video person Re-ID datasets (MARS, DukeVideo, PRID2011 and iLIDS-VID) are listed in Table R1. As we can see, by simply taking video sequence as input and adopting average pooling to aggregate framelevel feature, our AGW baseline achieves competitive results on two large-scale video Re-ID dataset, MARS and DukeVideo. Besides, AGW baseline also performs signiÔ¨Åcantly better than BagTricks [122] baseline under multiple evaluation metrics. By further modeling more temporal information and adjusting training strategy, AGW+ baseline gains huge improvement and also achieves competitive results on both PRID2011 and iLIDS-VID datasets. AGW+ baseline outperforms most state-of-the-art methods on MARS, DukeVideo and PRID2011 datasets. Most of these video-based person Re-ID methods achieve state-of-the-art performance by designing complicate temporal attention mechanism to exploit temporal dependency in pedestrian video. We believe that our AGW baseline can help video Re-ID model achieve higher performance with properly designed mechanism to further exploit spatial and temporal dependency.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Non-Local Non-Local

3 ID loss

Generalized Mean

BN

Pooling

Specific

ResNet50 Sharing Blocks

Weighted Regularization Triplet loss

Generalized Mean

BN

Pooling

Non-Local Non-Local ResNet50

ID loss

Fig. R2: The framework of the proposed AGW baseline for cross-modality visible-infrared Re-ID.

TABLE R2: Comparison with the state-of-the-arts on SYSU-MM01 dataset. Rank at r accuracy (%), mAP (%) and mINP (%) are reported. (Single-shot query setting [21] for experiments). ‚Äú*‚Äù represents methods published after the paper submission.

Settings
Method
One-stream [21] Two-stream [21] Zero-Pad [21] TONE [186] HCML [186] BDTR [142] eBDTR [142] HSME [187] D2RL [189] MAC [261] MSR [262] AlignGAN [190] X-Modal‚àó [263] Hi-CMD‚àó [191] cm-SSFT‚àó [264] DDAG‚àó [192] HAT‚àó [188]
AGW

Venue
ICCV17 ICCV17 ICCV17 AAAI18 AAAI18 IJCAI18 TIFS19 AAAI19
CVPR19 MM19 TIP19 ICCV19
AAAI-20
CVPR20
CVPR20
ECCV20
TIFS20
-

r=1
12.04 11.65 14.80 12.52 14.32 27.32 27.82 20.68
28.9 33.26 37.35 42.4 49.9 34.9 47.7 54.75 55.29 47.50

r = 10
49.68 47.99 54.12 50.72 53.16 66.96 67.34 32.74
70.6 79.04 83.40 85.0 89.8 77.6
90.39 92.14 84.39

All Search
r = 20
66.74 65.50 71.33 68.60 69.17 81.07 81.34 77.95
82.4 90.09 93.34 93.7 96.0
95.81 97.36 92.14

mAP
13.67 12.85 15.95 14.42 16.16 27.32 28.42 23.12
29.2 36.22 38.11 40.7 50.7 35.9 54.1 53.02 53.89 47.65

mINP
-
39.62 35.30

r=1
16.94 15.60 20.58 20.82 24.52 31.92 32.46
-
36.43 39.64 45.9
61.02 62.10 54.17

Indoor Search

r = 10 r = 20

63.55

82.10

61.18

81.02

68.38

85.79

68.86

84.46

73.25

86.73

77.18

89.28

77.42

89.62

-

-

-

-

62.36

71.63

89.29

97.66

87.6

94.4

-

-

-

-

-

-

94.06

98.41

95.75

99.20

91.14

95.98

mAP
22.95 21.49 26.92 26.38 30.08 41.86 42.46
-
37.03 50.88 54.3
67.98 69.37 62.97

mINP
-
62.61 59.23

C. Experiments on Cross-modality Re-ID
Architecture Design. We adopt a two-stream network structure as the backbone for cross-modality visible-infrared ReID5. Compared to the one-stream architecture in singlemodality person Re-ID (Fig. 8), the major difference is that, i.e., the Ô¨Årst block is speciÔ¨Åc for two modalities in order to capture modality-speciÔ¨Åc information, while the remaining blocks are shared to learn modality sharable features. Compared to the two-stream structure widely used in [142], [261], which only has one shared embedding layer, our design captures more sharable components. An illustration for cross-modality visible-infrared Re-ID is shown in Fig. R2.
Training Strategy. At each training step, we random sample 8 identities from the whole dataset. Then 4 visible and 4 infrared images are randomly selected for each identity. Totally, each training batch contains 32 visible and 32 infrared images. This guarantees the informative hard triplet mining from both modalities, i.e., we directly select the hard positive and negative from both intra- and intermodalities. This approximates the idea of bi-directional center-constrained top-ranking loss, handling the inter- and intra-modality variations simultaneously.
For fair comparison, we follow the settings in [142] exactly to conduct the image processing and data aug-
5. https://github.com/mangye16/Cross-Modal-Re-ID-baseline

mentation. For infrared images, we keep the original three channels, just like the visible RGB images. All the input images from both modalities are Ô¨Årst resized to 288 √ó 144, and random crop with zero-padding together with random horizontal Ô¨Çipping are adopted for data argumentation. The cropped image sizes are 256 √ó 128 for both modality. The image normalization are exactly following the singlemodality setting.
Training Loss. In the training stage, we combine with the identity classiÔ¨Åcation loss (Lid) and our proposed weighted regularization triplet loss (Lwrt). The weight of combining the identity loss and weighted regularized triplet loss is set to 1, the same as the single-modality setting. The pooling parameter pk is set to 3. For stable training, we adopt the same identity classiÔ¨Åer for two heterogeneous modalities, mining sharable information.
Optimizer Setting. We set the initial learning rate as 0.1 on both datasets, and decay it by 0.1 and 0.01 at 20 and 50 epochs, respectively. The total number of training epoch is 60. We also adopt a warm-up learning rate scheme. We adopt the stochastic gradient descent (SGD) optimizer for optimization, and the momentum parameter is set to 0.9. We have tried the same Adam optimizer (used in single-modality Re-ID) on cross-modality Re-ID task, but the performance is much lower than that of SGD optimizer by using a large learning rate. This is crucial since ImageNet

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

4

TABLE R3: Comparison with the state-of-the-arts on RegDB dataset on both query settings. Rank at r accuracy (%), mAP (%) and mINP (%) are reported. (Both the visible to thermal and thermal to visible query settings are evaluated.) ‚Äú*‚Äù represents methods published after the paper submission.

Settings

Method

Venue

HCML [186]

AAAI18

Zero-Pad [21]

ICCV17

BDTR [142]

IJCAI18

eBDTR [142]

TIFS19

HSME [187]

AAAI19

D2RL [189]

CVPR19

MAC [261]

MM19

MSR [262]

TIP19

AlignGAN [190] XModal‚àó [263] Hi-CMD‚àó [191] cm-SSFT‚àó [264] DDAG‚àó [192] HAT‚àó [188]

ICCV19 AAAI20 CVPR20 CVPR20 ECCV20 TIFS20

AGW

-

r=1
24.44 17.75 33.56 34.62 50.85
43.4 36.43 48.43 57.9 62.21 70.93 72.3 69.34 71.83 70.05

Visible to Thermal

r = 10 r = 20

mAP

47.53

56.78

20.08

34.21

44.35

18.90

58.61

67.43

32.76

58.96

68.72

33.46

73.36

81.66

47.00

66.1

76.3

44.1

62.36

71.63

37.03

70.32

79.95

48.67

-

-

53.6

83.13

91.72

60.18

86.39

-

66.04

-

-

72.9

86.19

91.49

63.46

87.16

92.16

67.56

86.21

91.55

66.37

mINP -
49.24 50.19

r=1
21.70 16.63 32.92 34.21 50.15
36.20
56.3
71.0 68.06 70.02 70.49

Thermal to Visible

r = 10 r = 20

mAP

45.02

55.58

22.24

34.68

44.25

17.82

58.46

68.43

31.96

58.74

68.64

32.49

72.40

81.07

46.16

-

-

-

61.68

70.99

36.63

-

-

-

-

-

53.4

-

-

-

-

-

-

-

-

71.7

85.15

90.31

61.80

86.45

91.61

66.30

87.12

91.84

65.90

mINP -
48.62 51.24

initialization is adopted for the infrared images. Detailed Comparison This section conducts the compar-
ison with the state-of-the-art cross-modality VI-ReID methods, including eBDTR [142], HSME [187], D2RL [189], MAC [261], MSR [262] and AlignGAN [190]. These methods are published in the past two years. AlignGAN [190], published in ICCV 2019, achieves the state-of-the-art performance by aligning the cross-modality representation at both the feature level and pixel level with GAN generated images. The results on two datasets are shown in Tables R2 and R3. We observe that the proposed AGW consistently outperforms the current state-of-the-art, without the time-consuming image generation process. For different query settings on RegDB dataset, our proposed baseline generally keeps the same performance. Our proposed baseline has been widely used in many recently developed methods. We believe our new baseline will provide a good guidance to boost the cross-modality Re-ID.
D. Experiments on Partial Re-ID
Implementation Details. We also evaluate the performance of our proposed AGW baseline on two commonly-used partial Re-ID datasets, Partial-REID and Partial-iLIDS. The overall backbone structure and training strategy for partial Re-ID AGW baseline model are the same as the one for single-modality image-based Re-ID model. Both PartialREID and Partial-iLIDS datasets offer only query image set and gallery image set. So we train AGW baseline model on the training set of Market-1501 dataset, then evaluate its performance on the testing set of two partial Re-ID datasets. We adopt the same way to evaluate the performance of BagTricks [122] baseline on these two partial Re-ID datasets for better comparison and analysis.
Detailed Comparison. We compare the performance of AGW baseline with other state-of-the-art partial Re-ID methods, including DSR [232], SFR [249] and VPM [67]. All these methods are published in recent years. The comparison results on both Partial-REID and Partial-iLIDS datasets are shown in Table R4. The VPM [67] achieves a very high performance by perceiving the visibility of regions through self-supervision and extracting region-level features. Considering only global features, our proposed AGW baseline still achieves competitive results compared to the current

TABLE R4: Comparison with the state-of-the-arts on two partial Re-ID datasets, including Partial-REID and PartialiLIDS. Rank-1, -3 accuracy (%) and mINP (%) are reported.

Method
DSR [232] CVPR18 SFR [249] ArXiv18 VPM [67] CVPR19 BagTricks [122] CVPR19W AGW

Partial-REID

R1 R3 mINP

50.7 70.0

-

56.9 78.5

-

67.7 81.9

-

62.0 74.0 45.4

69.7 80.0 56.7

Partial-iLIDS R1 R3 mINP 58.8 67.2 63.9 74.8 67.2 76.5 58.8 73.9 68.7 64.7 79.8 73.3

state-of-the-arts on both datasets. Besides, AGW baseline brings signiÔ¨Åcant improvement comparing to BagTricks [122] under multiple evaluation metrics, demonstrating its effectiveness for partial Re-ID problem.
E. Overview of This Survey
The overview Ô¨Ågure of this survey is shown in Fig. R3. According to the Ô¨Åve steps in developing a person Re-ID system, we conduct the survey from both closed-world and open-world settings. The closed-world setting is detailed in three different aspects: feature representation learning, deep metric learning and ranking optimization. We then summarize the datasets and SOTAs from both image- and video-based perspectives. For open-world person Re-ID, we summarize it into Ô¨Åve aspects: including heterogeneous data, Re-ID from raw images/videos, unavailable/limited labels, noisy annotation and open-set Re-ID.
Following the summary, we present an outlook for future person Re-ID. We design a new evaluation metric (mINP) to evaluate the difÔ¨Åculty to Ô¨Ånd all the correct matches. By analyzing the advantages of existing Re-ID methods, we develop a strong AGW baseline for future developments, which achieves competitive performance on four Re-ID tasks. Finally, some under-investigated open issues are discussed. Our survey provides a comprehensive summarization of existing state-of-the-art in different subtasks. Meanwhile, the analysis of future directions is also presented for further development guidance.
Acknowledgement. The authors would like to thank the anonymous reviewers for providing valuable feedbacks to improve the quality of this survey. The authors also would like to thank the pioneer researchers in person reidentiÔ¨Åcation and other related Ô¨Åelds. This work is sponsored by CAAI-Huawei MindSpore Open Fund.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE

5

Closed-World Setting

Feature Representation Learning
Deep Metric Learning
Ranking Optimization

Global Feature Learning Local Feature Learning Auxiliary Feature Learning Video Feature Learning
Identity Loss Verification Loss
Triplet Loss
Re-Ranking Rank Fusion

‚úì Single-modality Data ‚úì Bounding Boxes Generation ‚úì Sufficient Annotated Data ‚úì Correct Annotation ‚úì Query Exists in Gallery
Survey

Datasets and Evaluation
Heterogeneous Data

Person Survey Open-World

Re-ID

Setting

Outlook

Re-ID from Raw Images/Videos
Unavailable /Limited Labels
Noisy Annotation
Open-set Re-ID and Beyond
New Evaluation Metric

Datasets & Metrics State-of-the-Arts
Depth-based Re-ID Text-Image Re-ID Visible-Infrared Re-ID Cross-Resolution Re-ID End-to-end Person Search Multi-Camera Tracking
Unsupervised Re-ID Unsupervised Domain Adaptation
Partial Re-ID Re-ID with Sample Noise Re-ID with Label Noise
Open-set Re-ID Group Re-ID
Dynamic Multi-Camera Network mINP: Measure the ability to retrieve the hardest match

Re-ID in Next Era

New AGW Baseline

AGW: Achieve state-of-theart/comparable performance on
four Re-ID tasks

Under-investigated Open Issues

Discuss five open issues from different aspects

Fig. R3: An overview of this survey. It contains three main components, including Closed-World Setting in Section 2, Open-World Setting in Section 3 and an outlook of Re-ID in Next Era in Section 4.

